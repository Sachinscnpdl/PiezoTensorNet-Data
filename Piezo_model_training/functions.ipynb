{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b20ead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elements_occurance(df):\n",
    "    df = df.loc[:, (df != 0).any(axis=0)]\n",
    "\n",
    "    cols = list(df.columns.values)    #Make a list of all of the columns in the df\n",
    "    set = df.astype(bool).sum(axis=0) # Extract the occurance of each element in the alloys\n",
    "\n",
    "    element_df = set.to_frame()      # Convert extracted the occurance of each element in dataframe\n",
    "\n",
    "    element_occurancy = element_df[7:]\n",
    "    element_occurancy.columns =['Occurance']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f57783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_element_number(df):\n",
    "    # Add a column of \"Number of component\" & \"component\" in each alloy system\n",
    "    prop = []\n",
    "    for number in range(len(df['formula_pretty'])):\n",
    "        mpea = df['composition'][number]\n",
    "        element = list(Composition(mpea).as_dict().keys()) # List element present in Alloys ['Al', 'Cr', 'Fe', 'Ni', 'Mo']\n",
    "        prop.append([len(element), \" \".join(element)])\n",
    "\n",
    "        prop_data = pd.DataFrame(prop, columns=['No of Components', 'Component'])\n",
    "    df = pd.concat([df, prop_data], axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bdc5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def element_number(df, fig_title='Elements Number', fig_name='element_number'):\n",
    "    import os\n",
    "    import matplotlib\n",
    "    import matplotlib.ticker as tck\n",
    "    import seaborn as sns\n",
    "    #matplotlib.use('Agg')\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(5,5))\n",
    "    ax = sns.countplot(x='No of Components', data=df)\n",
    "    \n",
    "    plt.rcParams.update({'font.size': 20})\n",
    "    \n",
    "    ax.set_title('Number of elements', fontdict={'size': 24, 'color': 'blue'})\n",
    "    #ax.bar_label(ax.containers[0], fontproperties={'size': 18})\n",
    "        \n",
    "    ax.set_xlabel('Element Numbers', fontdict={'size': 20, 'color': 'r'})\n",
    "    ax.set_ylabel('Count', fontdict={'size': 20, 'color': 'r'})\n",
    "    \n",
    "    plt.ylim(0, 820,400)\n",
    "    \n",
    "    plt.tick_params(axis='both', which='both', length=5, width=1.5,color='black')\n",
    "    \n",
    "    ax.yaxis.set_minor_locator(tck.AutoMinorLocator(2))\n",
    "    \n",
    "    plt.savefig(\"plots//element_number\",dpi=1200, bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab653c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def element_occurrence(df,limit_value=8, fig_title='Hardness/ Elongation', fig_name='element_occurrence'):\n",
    "    \n",
    "    df = df.loc[:, (df != 0).any(axis=0)]\n",
    "\n",
    "    cols = list(df.columns.values)    #Make a list of all of the columns in the df\n",
    "    set = df.astype(bool).sum(axis=0) # Extract the occurance of each element in the alloys\n",
    "\n",
    "    element_df = set.to_frame()      # Convert extracted the occurance of each element in dataframe\n",
    "\n",
    "    element_occurancy = element_df[limit_value:-150]\n",
    "    element_occurancy.columns =['Occurrence']\n",
    "    element_occurancy = element_occurancy.sort_values('Occurrence')\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(36,20))\n",
    "    \n",
    "    df = element_occurancy\n",
    "    print(df)\n",
    "    \n",
    "    mask = df['Occurrence'] <= 29\n",
    "    df1 = df[mask]\n",
    "    df2_3 = df[~mask]\n",
    "    \n",
    "    mask2 = df2_3['Occurrence'] < 75\n",
    "    df2 = df2_3[mask2]\n",
    "    df3 = df2_3[~mask2]\n",
    "    \n",
    "    import matplotlib\n",
    "    #matplotlib.use('agg')\n",
    "    def plot_hor_bar(subplot, data, title = 'title', xlabel = 'Occurrence'):\n",
    "        print('lenght:  ',len(data))\n",
    "        plt.subplot(1,3,subplot)\n",
    "        ax = sns.barplot(x=data['Occurrence'],y=data.index, data=data)\n",
    "        #ax.bar_label(ax.containers[0], fontproperties={'size': 24})\n",
    "        plt.title(title,\n",
    "                  fontsize=36, color='b')\n",
    "        plt.xlabel(xlabel, fontsize=42, color='b')\n",
    "        plt.xticks(fontsize=36)\n",
    "        ax.xaxis.set_major_locator(plt.MaxNLocator(6))\n",
    "        \n",
    "        plt.ylabel(None)\n",
    "        plt.yticks(fontsize=40,color='black')\n",
    "        plt.tick_params(axis='both', which='both', length=15, width=5,color='black')\n",
    "        #plt.pause(.01)\n",
    "        sns.despine(left=True)\n",
    "        ax.grid(False)\n",
    "        ax.tick_params(bottom=True, left=False)\n",
    "\n",
    "        return None\n",
    " \n",
    "    \n",
    "    plot_hor_bar(1, df1, title = 'Elements Occurring Less than 25', xlabel = ' ')\n",
    "    plot_hor_bar(2, df2, title = 'Elements Occurring from 25 to 75', xlabel = 'Occurrence counts')\n",
    "    plot_hor_bar(3, df3, title = 'Elements Occurring more than 75', xlabel = ' ')\n",
    "    #plt.title('Elements Occurrence counts')\n",
    "    \n",
    "    plt.savefig(\"plots//element_occurrence\",dpi=200, bbox_inches='tight')\n",
    "\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7a95a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def properties_calculation(dataframe):\n",
    "    \n",
    "    # Import csv files \"Midema\" to calculte input features\n",
    "    elem_prop_data = pd.read_csv('csv/Miedema.csv')\n",
    "    VEC_elements = elem_prop_data.set_index('element')['valence_electrons'].to_dict()\n",
    "    shear_modulus_g = elem_prop_data.set_index('element')['shear_modulus'].to_dict()\n",
    "    bulk_modulus_b = elem_prop_data.set_index('element')['compressibility'].to_dict()\n",
    "    \n",
    "    # Input featurs calculation\n",
    "    df = dataframe\n",
    "    properties = []\n",
    "    for number in range(len(df['formula_pretty'])):\n",
    "\n",
    "        mpea = df['composition'][number]\n",
    "        #print(mpea)\n",
    "        #print(Composition(mpea).as_dict().keys())\n",
    "        element = list(Composition(mpea).as_dict().keys()) # List element present in Alloys ['Al', 'Cr', 'Fe', 'Ni', 'Mo']\n",
    "        #print(element)\n",
    "        fraction_composition = list(Composition(mpea).as_dict().values()) # List Fraction composition of corresponding element in an Alloy eg. [1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "        #print(fraction_composition)\n",
    "        total_mole = sum(fraction_composition) # Sum of elemental composition\n",
    "        #print(total_mole)\n",
    "\n",
    "        atomic_number = []\n",
    "        bulk_modulus = []\n",
    "        shear_modulus = []\n",
    "        molar_heat = []\n",
    "        thermal_conductivity = []\n",
    "        mole_fraction = []\n",
    "        X_i = []\n",
    "        r_i = []\n",
    "        Tm_i = []\n",
    "        VEC_i= []\n",
    "        R = 8.314\n",
    "\n",
    "        for i in element:\n",
    "\n",
    "            atomic_number.append(Element(i).Z)\n",
    "            #molar_heat.append(Cp_dict[i])\n",
    "\n",
    "            bulk_b =Element(i).bulk_modulus\n",
    "\n",
    "            if type(bulk_b) == type(None):\n",
    "                for j in bulk_modulus_b: bulk_b = (bulk_modulus_b.get(j))       \n",
    "\n",
    "            bulk_modulus.append(bulk_b)\n",
    "\n",
    "            #print(bulk_modulus)\n",
    "\n",
    "            shear_g = (Element(i).rigidity_modulus)\n",
    "            if type(shear_g) == type(None):\n",
    "                for s in shear_modulus_g: shear_g = ((shear_modulus_g.get(s)))\n",
    "            shear_modulus.append(shear_g)\n",
    "\n",
    "            thermal_conductivity.append(Element(i).thermal_conductivity)\n",
    "            mole_fraction.append(Composition(mpea).get_atomic_fraction(i)) # Calculates mole fraction of mpea using \"Composition\" functions\n",
    "\n",
    "            X_i.append(Element(i).X) # Calculate individual electronegativity using \"Element\" function\n",
    "\n",
    "            r_i.append(Element(i).atomic_radius) if Element(i).atomic_radius_calculated == None else r_i.append(Element(i).atomic_radius_calculated) # There are two functions present in Element␣class of pymatgen, so here checking using if conditional in both functions␣to not miss any value\n",
    "            Tm_i.append(Element(i).melting_point) # Calculating melting point of every element using \"Element\" class and function\n",
    "            \n",
    "            try: VEC_i.append(DemlData().get_elemental_property(Element(i),\"valence\")) # VEC is also present in 2 locations in matminer, first is the␣function \"DemlData()\"\n",
    "            except KeyError:\n",
    "                if i in VEC_elements: VEC_i.append(float(VEC_elements.get(i))) #In case data is not present in \"DemlData()\" function, there is a csv file␣inside matminer opened earlier as \"elem_prop_data\" in the very first cell\n",
    "                if i=='Xe': VEC_i.append(float(2))\n",
    "            #print(number, VEC_i)\n",
    "            #print('VEC: ',i, VEC_elements.get('Xe'))\n",
    "        \n",
    "        # Average Atomic Number\n",
    "        AN = sum(np.multiply(mole_fraction, atomic_number))\n",
    "        #print(AN)\n",
    "\n",
    "        # Average Molar Heat coefficient\n",
    "        #Cp_bar = sum(np.multiply(mole_fraction, molar_heat))    \n",
    "        #print(Cp_bar)\n",
    "\n",
    "        #term_Cp = (1-np.divide(molar_heat, Cp_bar))**2\n",
    "        #del_Cp = sum(np.multiply(mole_fraction, term_Cp))**0.5 \n",
    "\n",
    "        # Thermal Conductivity\n",
    "        k = sum(np.multiply(mole_fraction, thermal_conductivity))\n",
    "\n",
    "        # Bulk Modolus\n",
    "        bulk = sum(np.multiply(mole_fraction, bulk_modulus)) # Bulk modulus of 'Zr' not present\n",
    "        \n",
    "        # Bulk modolus asymmetry\n",
    "        term_bulk = (1-np.divide(bulk_modulus, bulk))**2\n",
    "        del_bulk = sum(np.multiply(mole_fraction, term_bulk))**0.5         \n",
    "\n",
    "        # Shear Modolus\n",
    "        shear= sum(np.multiply(mole_fraction, shear_modulus))\n",
    "        \n",
    "        # Shear modolus asymmetry\n",
    "        term_shear = (1-np.divide(shear_modulus, shear))**2\n",
    "        del_shear = sum(np.multiply(mole_fraction, term_shear))**0.5         \n",
    "\n",
    "        # Calculation of Atomic Radius Difference (del)\n",
    "\n",
    "        r_bar = sum(np.multiply(mole_fraction, r_i))\n",
    "        term = (1-np.divide(r_i, r_bar))**2\n",
    "        atomic_size_difference = sum(np.multiply(mole_fraction, term))**0.5 \n",
    "        #print(number,element,mole_fraction,r_i,r_bar,term,atomic_size_difference)\n",
    "\n",
    "\n",
    "        # Electronegativity (del_X)\n",
    "        X_bar = sum(np.multiply(mole_fraction, X_i))\n",
    "        del_Chi = (sum(np.multiply(mole_fraction, (np.subtract(X_i,X_bar))**2)))**0.5\n",
    "        #term_X = (1-np.divide(X_i, X_bar))**2\n",
    "        #del_Chi = (sum(np.multiply(mole_fraction, term_X)))**0.5 \n",
    "\n",
    "        # Difference Melting Temperature\n",
    "        T_bar = sum(np.multiply(mole_fraction, Tm_i))\n",
    "        del_Tm =(sum(np.multiply(mole_fraction, (np.subtract(Tm_i,T_bar))**2)))**0.5\n",
    "\n",
    "        # Average Melting Temperature\n",
    "        Tm = sum(np.multiply(mole_fraction, Tm_i))    \n",
    "\n",
    "        # Valence Electron Concentration\n",
    "        VEC = sum(np.multiply(mole_fraction, VEC_i))\n",
    "        #print(VEC)\n",
    "\n",
    "        # Entropy of mixing\n",
    "        #del_Smix = -WenAlloys().compute_configuration_entropy(mole_fraction)*1000 #WenAlloys class imported from matminer library\n",
    "        del_Smix = -R*sum(np.multiply(mole_fraction, np.log(mole_fraction)))\n",
    "\n",
    "\n",
    "        # Geometrical parameters\n",
    "        if atomic_size_difference == 0:  atomic_size_difference = 1e-9\n",
    "        lemda = np.divide(del_Smix, (atomic_size_difference)**2)\n",
    "        #print(number,del_Smix,atomic_size_difference, lemda)\n",
    "\n",
    "        #parameter = Tm*del_Smix/abs(del_Hmix) \n",
    "        #print(number,\"lemda, parameter\", lemda, parameter)\n",
    "\n",
    "\n",
    "        #properties.append([len(element), \" \".join(element), \" \".join(list(map(str, fraction_composition))),total_mole, round(sum(mole_fraction),1), atomic_size_difference, round(del_Chi, 4),del_Tm, Tm, VEC, AN, k, bulk,del_bulk,shear,del_shear, round(del_Smix, 4),round(lemda,4), round(del_Hmix, 4),round(parameter,4)])\n",
    "        properties.append([len(element), \" \".join(element), \" \".join(list(map(str, fraction_composition))),total_mole, round(sum(mole_fraction),1), atomic_size_difference, round(del_Chi, 4),del_Tm, Tm, VEC, AN, k, bulk,del_bulk,shear,del_shear, round(del_Smix, 4),round(lemda,4)])\n",
    "    #prop_data = pd.DataFrame(properties, columns=['No of Components','Component','Moles of individual Components', 'Total Moles', 'Sum of individual MoleFractions', '$\\delta$', 'Δ$\\chi$', 'ΔTm','Tm(K)', 'VEC', 'AN', 'K','B', 'ΔB','G', 'ΔG','ΔSmix','$\\lambda$', 'ΔHmix','$\\Omega$'])\n",
    "    prop_data = pd.DataFrame(properties, columns=['No of Components','Component','Moles of individual Components', 'Total Moles', 'Sum of individual MoleFractions', '$\\delta$', 'Δ$\\chi$', 'ΔTm','Tm(K)', 'VEC', 'AN', 'K','B', 'ΔB','G', 'ΔG', 'ΔSmix','$\\lambda$',])\n",
    "\n",
    "    df = pd.concat([df, prop_data], axis = 1)\n",
    "    \n",
    "    df_input_target = df.iloc[:,[-18,-13,-12,-11,-10, -9, -8, -7, -6,-5, -4, -3, -2, -1,4,5,2,6,7]]\n",
    "    \n",
    "    return(df,df_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af214d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def properties_calculation_old(dataframe):\n",
    "    \n",
    "    # Import csv files \"Midema\" to calculte input features\n",
    "    elem_prop_data = pd.read_csv('csv/Miedema.csv')\n",
    "    VEC_elements = elem_prop_data.set_index('element')['valence_electrons'].to_dict()\n",
    "    shear_modulus_g = elem_prop_data.set_index('element')['shear_modulus'].to_dict()\n",
    "    bulk_modulus_b = elem_prop_data.set_index('element')['compressibility'].to_dict()\n",
    "    \n",
    "    # Input featurs calculation\n",
    "    df = dataframe\n",
    "    properties = []\n",
    "    for number in range(len(df['formula_pretty'])):\n",
    "\n",
    "        mpea = df['composition'][number]\n",
    "        #print(mpea)\n",
    "        #print(Composition(mpea).as_dict().keys())\n",
    "        element = list(Composition(mpea).as_dict().keys()) # List element present in Alloys ['Al', 'Cr', 'Fe', 'Ni', 'Mo']\n",
    "        #print(element)\n",
    "        fraction_composition = list(Composition(mpea).as_dict().values()) # List Fraction composition of corresponding element in an Alloy eg. [1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "        #print(fraction_composition)\n",
    "        total_mole = sum(fraction_composition) # Sum of elemental composition\n",
    "        #print(total_mole)\n",
    "\n",
    "        atomic_number = []\n",
    "        bulk_modulus = []\n",
    "        shear_modulus = []\n",
    "        molar_heat = []\n",
    "        thermal_conductivity = []\n",
    "        mole_fraction = []\n",
    "        X_i = []\n",
    "        r_i = []\n",
    "        Tm_i = []\n",
    "        VEC_i= []\n",
    "        R = 8.314\n",
    "\n",
    "        for i in element:\n",
    "\n",
    "            atomic_number.append(Element(i).Z)\n",
    "            #molar_heat.append(Cp_dict[i])\n",
    "\n",
    "            bulk_b =Element(i).bulk_modulus\n",
    "\n",
    "            if type(bulk_b) == type(None):\n",
    "                for j in bulk_modulus_b: bulk_b = (bulk_modulus_b.get(j))       \n",
    "\n",
    "            bulk_modulus.append(bulk_b)\n",
    "\n",
    "            #print(bulk_modulus)\n",
    "\n",
    "            shear_g = (Element(i).rigidity_modulus)\n",
    "            if type(shear_g) == type(None):\n",
    "                for s in shear_modulus_g: shear_g = ((shear_modulus_g.get(s)))\n",
    "            shear_modulus.append(shear_g)\n",
    "\n",
    "            thermal_conductivity.append(Element(i).thermal_conductivity)\n",
    "            mole_fraction.append(Composition(mpea).get_atomic_fraction(i)) # Calculates mole fraction of mpea using \"Composition\" functions\n",
    "\n",
    "            X_i.append(Element(i).X) # Calculate individual electronegativity using \"Element\" function\n",
    "\n",
    "            r_i.append(Element(i).atomic_radius) if Element(i).atomic_radius_calculated == None else r_i.append(Element(i).atomic_radius_calculated) # There are two functions present in Element␣class of pymatgen, so here checking using if conditional in both functions␣to not miss any value\n",
    "            Tm_i.append(Element(i).melting_point) # Calculating melting point of every element using \"Element\" class and function\n",
    "            \n",
    "            try: VEC_i.append(DemlData().get_elemental_property(Element(i),\"valence\")) # VEC is also present in 2 locations in matminer, first is the␣function \"DemlData()\"\n",
    "            except KeyError:\n",
    "                if i in VEC_elements: VEC_i.append(float(VEC_elements.get(i))) #In case data is not present in \"DemlData()\" function, there is a csv file␣inside matminer opened earlier as \"elem_prop_data\" in the very first cell\n",
    "                if i=='Xe': VEC_i.append(float(2))\n",
    "            #print(number, VEC_i)\n",
    "            #print('VEC: ',i, VEC_elements.get('Xe'))\n",
    "        \n",
    "        # Average Atomic Number\n",
    "        AN = sum(np.multiply(mole_fraction, atomic_number))\n",
    "        #print(AN)\n",
    "\n",
    "        # Average Molar Heat coefficient\n",
    "        #Cp_bar = sum(np.multiply(mole_fraction, molar_heat))    \n",
    "        #print(Cp_bar)\n",
    "\n",
    "        #term_Cp = (1-np.divide(molar_heat, Cp_bar))**2\n",
    "        #del_Cp = sum(np.multiply(mole_fraction, term_Cp))**0.5 \n",
    "\n",
    "        # Thermal Conductivity\n",
    "        k = sum(np.multiply(mole_fraction, thermal_conductivity))\n",
    "\n",
    "        # Bulk Modolus\n",
    "        bulk = sum(np.multiply(mole_fraction, bulk_modulus)) # Bulk modulus of 'Zr' not present\n",
    "        \n",
    "        # Bulk modolus asymmetry\n",
    "        term_bulk = (1-np.divide(bulk_modulus, bulk))**2\n",
    "        del_bulk = sum(np.multiply(mole_fraction, term_bulk))**0.5         \n",
    "\n",
    "        # Shear Modolus\n",
    "        shear= sum(np.multiply(mole_fraction, shear_modulus))\n",
    "        \n",
    "        # Shear modolus asymmetry\n",
    "        term_shear = (1-np.divide(shear_modulus, shear))**2\n",
    "        del_shear = sum(np.multiply(mole_fraction, term_shear))**0.5         \n",
    "\n",
    "        # Calculation of Atomic Radius Difference (del)\n",
    "\n",
    "        r_bar = sum(np.multiply(mole_fraction, r_i))\n",
    "        term = (1-np.divide(r_i, r_bar))**2\n",
    "        atomic_size_difference = sum(np.multiply(mole_fraction, term))**0.5 \n",
    "        #print(number,element,mole_fraction,r_i,r_bar,term,atomic_size_difference)\n",
    "\n",
    "\n",
    "        # Electronegativity (del_X)\n",
    "        X_bar = sum(np.multiply(mole_fraction, X_i))\n",
    "        del_Chi = (sum(np.multiply(mole_fraction, (np.subtract(X_i,X_bar))**2)))**0.5\n",
    "        #term_X = (1-np.divide(X_i, X_bar))**2\n",
    "        #del_Chi = (sum(np.multiply(mole_fraction, term_X)))**0.5 \n",
    "\n",
    "        # Difference Melting Temperature\n",
    "        T_bar = sum(np.multiply(mole_fraction, Tm_i))\n",
    "        del_Tm =(sum(np.multiply(mole_fraction, (np.subtract(Tm_i,T_bar))**2)))**0.5\n",
    "\n",
    "        # Average Melting Temperature\n",
    "        Tm = sum(np.multiply(mole_fraction, Tm_i))    \n",
    "\n",
    "        # Valence Electron Concentration\n",
    "        #print(mole_fraction.shape, VEC_i.shape)\n",
    "        #print(number)\n",
    "        #print(mole_fraction)\n",
    "        #print(VEC_i)\n",
    "        VEC = sum(np.multiply(mole_fraction, VEC_i))\n",
    "        #print(VEC)\n",
    "\n",
    "        # Entropy of mixing\n",
    "        #del_Smix = -WenAlloys().compute_configuration_entropy(mole_fraction)*1000 #WenAlloys class imported from matminer library\n",
    "        del_Smix = -R*sum(np.multiply(mole_fraction, np.log(mole_fraction)))\n",
    "\n",
    "\n",
    "        HEA = element\n",
    "        #print(len(mole_fraction), len(HEA))\n",
    "\n",
    "\n",
    "        # Enthalpy of mixing\n",
    "        AB = []\n",
    "        C_i_C_j = []\n",
    "        del_Hab = []\n",
    "        for item in range(len(HEA)):\n",
    "            for jitem in range(item, len(HEA)-1):\n",
    "                AB.append(HEA[item] + HEA[jitem+1])\n",
    "                C_i_C_j.append(mole_fraction[item]*mole_fraction[jitem+1])\n",
    "                #del_Hab.append(round(Miedema().deltaH_chem([HEA[item], HEA[jitem+1]], [0.5, 0.5], 'ss'),3)) # Calculating binary entropy of mixing at 0.5-0.5␣ (equal) composition using Miedema class of \"matminer\" library\n",
    "                del_Hab.append(MixingEnthalpy().get_mixing_enthalpy(Element(HEA[item]), Element(HEA[jitem+1]))) # Matminer MixingOfEnthalpy\n",
    "                #print(HEA)\n",
    "                #print(del_Hab)\n",
    "                #print(\" \")\n",
    "\n",
    "        omega = np.multiply(del_Hab, 4)\n",
    "        del_Hmix = sum(np.multiply(omega, C_i_C_j))\n",
    "\n",
    "        # Geometrical parameters\n",
    "        if atomic_size_difference == 0:  atomic_size_difference = 1e-9\n",
    "        lemda = np.divide(del_Smix, (atomic_size_difference)**2)\n",
    "        #print(number,del_Smix,atomic_size_difference, lemda)\n",
    "\n",
    "        #parameter = Tm*del_Smix/abs(del_Hmix) \n",
    "        #print(number,\"lemda, parameter\", lemda, parameter)\n",
    "\n",
    "\n",
    "        #properties.append([len(element), \" \".join(element), \" \".join(list(map(str, fraction_composition))),total_mole, round(sum(mole_fraction),1), atomic_size_difference, round(del_Chi, 4),del_Tm, Tm, VEC, AN, k, bulk,del_bulk,shear,del_shear, round(del_Smix, 4),round(lemda,4), round(del_Hmix, 4),round(parameter,4)])\n",
    "        properties.append([len(element), \" \".join(element), \" \".join(list(map(str, fraction_composition))),total_mole, round(sum(mole_fraction),1), atomic_size_difference, round(del_Chi, 4),del_Tm, Tm, VEC, AN, k, bulk,del_bulk,shear,del_shear, round(del_Smix, 4),round(lemda,4)])\n",
    "    #prop_data = pd.DataFrame(properties, columns=['No of Components','Component','Moles of individual Components', 'Total Moles', 'Sum of individual MoleFractions', '$\\delta$', 'Δ$\\chi$', 'ΔTm','Tm(K)', 'VEC', 'AN', 'K','B', 'ΔB','G', 'ΔG','ΔSmix','$\\lambda$', 'ΔHmix','$\\Omega$'])\n",
    "    prop_data = pd.DataFrame(properties, columns=['No of Components','Component','Moles of individual Components', 'Total Moles', 'Sum of individual MoleFractions', '$\\delta$', 'Δ$\\chi$', 'ΔTm','Tm(K)', 'VEC', 'AN', 'K','B', 'ΔB','G', 'ΔG', 'ΔSmix','$\\lambda$',])\n",
    "\n",
    "    df = pd.concat([df, prop_data], axis = 1)\n",
    "    \n",
    "    df_input_target = df.iloc[:,[-18,-13,-12,-11,-10, -9, -8, -7, -6,-5, -4, -3, -2, -1,4,5,2,6,7]]\n",
    "    \n",
    "    return(df,df_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ab5b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b0489b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_target(datasets, input_name):\n",
    "    inputs = datasets\n",
    "    inputs = inputs.astype(float)\n",
    "\n",
    "#     print(inputs.head(2))\n",
    "#     print(\"..................................\")\n",
    "    #df_all = pd.DataFrame(inputs, columns = input_name+[\"e_ij_max\"])\n",
    "    #print(df_all.head())\n",
    "\n",
    "    df_inputs = df_all.drop(['e_ij_max'], axis=1)\n",
    "    df_targets = df_all['e_ij_max']\n",
    "    return (df_all, df_inputs, df_targets)\n",
    "    \n",
    "def train_test_split(datasets, input_name):\n",
    "    \n",
    "    #df_all = datasets.astype(float)\n",
    "    df_all = datasets\n",
    "    df_inputs = df_all.drop(['e_ij_max','total'], axis=1)\n",
    "    df_targets = df_all['total']\n",
    "    \n",
    "\n",
    "\n",
    "    # Split dataset in train-test\n",
    "#     print(\"..................................\")\n",
    "#     print(df_all.head(2))\n",
    "#     print(\"..................................\")\n",
    "#     print(df_inputs.head(2))\n",
    "#     print(\"..................................\")\n",
    "#     print(df_targets.head())\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_inputs, df_targets, test_size=0.1, random_state=33)\n",
    "\n",
    "    #X_train, X_test,  = train_test_split(df_inputs, test_size=0.1, random_state=0)\n",
    "\n",
    "    #X_train_no_fab = X_train.drop(['Fab_1', 'Fab_2', 'Fab_3', 'Fab_4','No of Components'], axis=1)\n",
    "    #X_train_fab = X_train.loc[:,['Fab_1', 'Fab_2', 'Fab_3', 'Fab_4']]\n",
    "\n",
    "    #X_test_no_fab = X_test.drop(['Fab_1', 'Fab_2', 'Fab_3', 'Fab_4','No of Components'], axis=1)\n",
    "    #X_test_fab = X_test.loc[:,['Fab_1', 'Fab_2', 'Fab_3', 'Fab_4']]\n",
    "    \n",
    "    ##n_component = X_train.loc[:,['No of Components']]\n",
    "\n",
    "    input_df = pd.concat([X_train, y_train], axis=1)\n",
    "    \n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "    \n",
    "        \n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    \n",
    "    return(X_train, X_test,y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb69d4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_distrubition(df,title=\"123\"):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    from matplotlib.ticker import MaxNLocator\n",
    "    from matplotlib.cm import ScalarMappable\n",
    "    import matplotlib.ticker as tck\n",
    "\n",
    "    df= df.to_frame()\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "    plt.rc('font', size=18)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    df[\"total\"] = [np.array(m).astype('float64') for m in df[\"total\"]]\n",
    "\n",
    "    # create plot\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # create colormap\n",
    "    norm = plt.Normalize(np.array(df[\"total\"].values.tolist()).min(), np.array(df[\"total\"].values.tolist()).max())\n",
    "    cmap = plt.cm.plasma\n",
    "\n",
    "    mappable = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "\n",
    "    # plot surface\n",
    "    for i, row in df.iterrows():\n",
    "        x, y = np.meshgrid(range(6), range(3))\n",
    "        z = row[\"total\"]\n",
    "\n",
    "        #ax.plot_surface(x+i*6, y, z, cmap=cmap, alpha=0.9,linewidth=0, rstride=1, cstride=1)\n",
    "        ax.plot_surface(x, y+i*3, z, cmap=cmap, alpha=0.9,linewidth=0, rstride=1, cstride=1)\n",
    "\n",
    "    # add colorbar\n",
    "    clb = fig.colorbar(mappable, ax=ax,  shrink=0.45, pad = -0.05)\n",
    "    clb.ax.tick_params(labelsize=16) \n",
    "    clb.set_label(r'$\\alpha$', rotation=0)\n",
    "    # Label and coordinate\n",
    "    #ax.text(5, 10,10 , r\"$\\alpha$\", color='red', fontsize=12)\n",
    "    ax.text(4, max(ax.get_ybound()),1.1*max(ax.get_zbound()), r\"$\\alpha ^{  n}_{ij}$\", color='red', fontsize=18)\n",
    "    \n",
    "\n",
    "    # set labels and title\n",
    "    ax.set_xlabel('Columns', labelpad=12, fontsize=20,color='r')\n",
    "    ax.set_ylabel('Rows x N', labelpad=20, fontsize=20,color='r')\n",
    "    #ax.set_zlabel('Frequency', labelpad=8, fontsize=20,color='r')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    ax.set_xticks(np.linspace(0, 6, 4))\n",
    "    #ax.set_xticklabels(['1', '2', '6','2'])\n",
    "\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(4))\n",
    "    ax.xaxis.set_minor_locator(tck.AutoMinorLocator(2))\n",
    "\n",
    "    \n",
    "    ax.zaxis.set_major_locator(MaxNLocator(1))\n",
    "    ax.get_zaxis().set_visible(False)\n",
    "    \n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontsize=18)\n",
    "    #ax.set_zticklabels(ax.get_zticklabels(), fontsize=18)\n",
    "\n",
    "    ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "    # set camera angle and distance\n",
    "    #ax.view_init(elev=20, azim=-40)\n",
    "    ax.view_init(elev=20, azim=-40)\n",
    "    #ax.dist=12\n",
    "\n",
    "    # set background color and grid\n",
    "    ax.xaxis.pane.fill = False\n",
    "    ax.yaxis.pane.fill = False\n",
    "    ax.zaxis.pane.fill = False\n",
    "    ax.xaxis.pane.set_edgecolor('white')\n",
    "    ax.yaxis.pane.set_edgecolor('white')\n",
    "    ax.zaxis.pane.set_edgecolor('white')\n",
    "    ax.grid(False)\n",
    "    \n",
    "    ax.set_zticks([]) # Remove the tick labels on the z-axis\n",
    "    ax.set_zticklabels([])\n",
    "\n",
    "    ax.zaxis.line.set_color((1.0, 1.0, 1.0, 0.0)) # Set the z-axis line color to transparent\n",
    "\n",
    "    # show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7479b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_split(df,title=\"123\"):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    from matplotlib.ticker import MaxNLocator\n",
    "    from matplotlib.cm import ScalarMappable\n",
    "\n",
    "    df= df.to_frame()\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "    plt.rc('font', size=18)\n",
    "\n",
    "    df[\"total\"] = [np.array(m).astype('float64') for m in df[\"total\"]]\n",
    "\n",
    "    # create plot\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # create colormap\n",
    "    norm = plt.Normalize(np.array(df[\"total\"].values.tolist()).min(), np.array(df[\"total\"].values.tolist()).max())\n",
    "    cmap = plt.cm.plasma\n",
    "\n",
    "    # create ScalarMappable object based on frequency values\n",
    "    sm = ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array(df[\"total\"])\n",
    "\n",
    "    # plot surface with colors based on frequency values\n",
    "    for i, row in df.iterrows():\n",
    "        x, y = np.meshgrid(range(6), range(3))\n",
    "        z = row[\"total\"]\n",
    "\n",
    "        ax.plot_surface(x, y+i*3, z, cmap=cmap, alpha=0.9, linewidth=0, rstride=1, cstride=1)\n",
    "\n",
    "    # add colorbar\n",
    "    fig.colorbar(sm, ax=ax, shrink=0.4, pad=0.04)\n",
    "\n",
    "    # set labels and title\n",
    "    ax.set_xlabel('j x n', labelpad=12, fontsize=20,color='r')\n",
    "    ax.set_ylabel('Rows', labelpad=20, fontsize=20,color='r')\n",
    "    ax.set_zlabel('Frequency', labelpad=8, fontsize=20,color='r')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(6))\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(3))\n",
    "    ax.zaxis.set_major_locator(MaxNLocator(3))\n",
    "\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontsize=18)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "    # set camera angle and distance\n",
    "    ax.view_init(elev=15, azim=-30)\n",
    "\n",
    "    # set background color and grid\n",
    "    ax.xaxis.pane.fill = False\n",
    "    ax.yaxis.pane.fill = False\n",
    "    ax.zaxis.pane.fill = False\n",
    "    ax.xaxis.pane.set_edgecolor('white')\n",
    "    ax.yaxis.pane.set_edgecolor('white')\n",
    "    ax.zaxis.pane.set_edgecolor('white')\n",
    "    ax.grid(False)\n",
    "\n",
    "    # set plot size and scale\n",
    "    fig.set_size_inches(10, 8)\n",
    "    x_scale = 0.8\n",
    "    y_scale = 1.2\n",
    "    z_scale = 0.8\n",
    "    scale = np.diag([x_scale, y_scale, z_scale, 1.0])\n",
    "    scale = scale * (1.0/scale.max())\n",
    "    scale[3,3] = 1.0\n",
    "\n",
    "    def short_proj():\n",
    "        return np.dot(Axes3D.get_proj(ax), scale)\n",
    "\n",
    "    ax.get_proj = short_proj\n",
    "\n",
    "    # show plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1e1200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n18_split(df,title=\"123\"):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    from matplotlib.ticker import MaxNLocator\n",
    "    from matplotlib.cm import ScalarMappable\n",
    "\n",
    "    df= df.to_frame()\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "    plt.rc('font', size=18)\n",
    "\n",
    "    df[\"total\"] = [np.array(m).astype('float64') for m in df[\"total\"]]\n",
    "\n",
    "    # create plot\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # create colormap\n",
    "    norm = plt.Normalize(np.array(df[\"total\"].values.tolist()).min(), np.array(df[\"total\"].values.tolist()).max())\n",
    "    cmap = plt.cm.plasma\n",
    "\n",
    "    # create ScalarMappable object based on frequency values\n",
    "    sm = ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array(df[\"total\"])\n",
    "\n",
    "    # plot surface with colors based on frequency values\n",
    "    for i, row in df.iterrows():\n",
    "        x, y = np.meshgrid(range(18), range(2520))\n",
    "        z = row[\"total\"]\n",
    "\n",
    "        ax.plot_surface(x, y, z, cmap=cmap, alpha=0.9, linewidth=0, rstride=1, cstride=1)\n",
    "\n",
    "    # add colorbar\n",
    "    fig.colorbar(sm, ax=ax, shrink=0.4, pad=0.04)\n",
    "\n",
    "    # set labels and title\n",
    "    ax.set_xlabel('j x n', labelpad=12, fontsize=20,color='r')\n",
    "    ax.set_ylabel('Rows', labelpad=20, fontsize=20,color='r')\n",
    "    ax.set_zlabel('Frequency', labelpad=8, fontsize=20,color='r')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(6))\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(3))\n",
    "    ax.zaxis.set_major_locator(MaxNLocator(3))\n",
    "\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontsize=18)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "    # set camera angle and distance\n",
    "    ax.view_init(elev=15, azim=-30)\n",
    "\n",
    "    # set background color and grid\n",
    "    ax.xaxis.pane.fill = False\n",
    "    ax.yaxis.pane.fill = False\n",
    "    ax.zaxis.pane.fill = False\n",
    "    ax.xaxis.pane.set_edgecolor('white')\n",
    "    ax.yaxis.pane.set_edgecolor('white')\n",
    "    ax.zaxis.pane.set_edgecolor('white')\n",
    "    ax.grid(False)\n",
    "\n",
    "    # set plot size and scale\n",
    "    fig.set_size_inches(10, 8)\n",
    "    x_scale = 0.8\n",
    "    y_scale = 1.2\n",
    "    z_scale = 0.8\n",
    "    scale = np.diag([x_scale, y_scale, z_scale, 1.0])\n",
    "    scale = scale * (1.0/scale.max())\n",
    "    scale[3,3] = 1.0\n",
    "\n",
    "    def short_proj():\n",
    "        return np.dot(Axes3D.get_proj(ax), scale)\n",
    "\n",
    "    ax.get_proj = short_proj\n",
    "\n",
    "    # show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8e9bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_distribution(data,text,test_annotate=0,test_annotate2=0,limit=1200,distance=0.15,ylabel = 'Hardness (HV)', title=\"Hardness Distribution\",plot_path=\"plots\\\\hardness\\\\\"):\n",
    "    \n",
    "    import seaborn as sns\n",
    "    import matplotlib.ticker as tck\n",
    "    \n",
    "    mean=data.mean()\n",
    "    median=np.median(data)\n",
    "    ten_per = np.percentile(data, 10)\n",
    "    ninety_per = np.percentile(data, 90)\n",
    "    print(text,'\\n Mean: ',mean,'\\n Median: ',median, '\\n 10 Percentile:',ten_per, '\\n 90 Percentile:', ninety_per)\n",
    "    print(\"Number of \",text, data.shape[0])\n",
    "    print(\"----------------------\")\n",
    "    sns.set(style=\"ticks\", color_codes=True,font_scale=1)\n",
    "    \n",
    "    fig , ax = plt.subplots(figsize=(4,4), dpi=400)\n",
    "    sns.kdeplot( y=data, color=\"g\",lw=0.5, shade=True, bw_adjust=1)\n",
    "    \n",
    "    # Plot Mean and Median\n",
    "    plt.plot(distance,mean, marker=\"o\", markersize=6, markeredgecolor=\"red\", markerfacecolor=\"red\", label=\"Mean\",linestyle = 'None')\n",
    "    plt.plot(distance,median, marker=\"^\", markersize=6, markeredgecolor=\"blue\", markerfacecolor=\"blue\", label=\"Median\",linestyle = 'None')\n",
    "    \n",
    "    # Plot 10 and 90 percentile\n",
    "    plt.plot(distance,ten_per, marker=\"o\", markersize=5, markeredgecolor=\"red\", label=\"10% Percentile\",linestyle = 'None')\n",
    "    plt.plot(distance,ninety_per, marker=\"o\", markersize=5, markeredgecolor=\"blue\", label=\"90% Percentile\",linestyle = 'None')\n",
    "\n",
    "    x_values = [distance, distance]\n",
    "    y_values = [ten_per, ninety_per]\n",
    "    plt.plot(x_values, y_values, 'green', linestyle=\"-\")\n",
    "    \n",
    "    # Annotations\n",
    "    plt.text(distance*1.06, ten_per+test_annotate, str(round(ten_per,3))+ \"(10%)\", horizontalalignment='left', size='medium', color='b')\n",
    "    plt.text(distance*1.06, ninety_per+test_annotate2, str(round(ninety_per,3))+ \"(90%)\", horizontalalignment='left', size='medium', color='b')\n",
    "\n",
    "    #plt.gca().axes.get_xaxis().set_visible(False) # Remove x-axis lable\n",
    "    \n",
    "    plt.ylim(0, limit)\n",
    "    ax.yaxis.set_minor_locator(tck.AutoMinorLocator(2))\n",
    "    \n",
    "    plt.title(title+str(text)+\" (\"+str(data.shape[0])+\" data)\")\n",
    "    plt.ylabel(ylabel)\n",
    "    \n",
    "    # Crop shaded region above max and below min value\n",
    "    plt.axhspan(0,min(data), color='white')\n",
    "    plt.axhspan(max(data),limit, color='white')\n",
    "\n",
    "    plt.legend(frameon=False,loc='upper right')\n",
    "    plt.savefig(plot_path+str(text)+ylabel+'_split.png',dpi=1200, bbox_inches='tight')\n",
    "    #plt.legend(frameon=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b37d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_data(X_train):\n",
    "    # Standarize the input features\n",
    "    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "    std_X_train = scaler.fit_transform(X_train)\n",
    "    input_name = list(X_train.columns.values)\n",
    "\n",
    "    std_df = pd.DataFrame(data=std_X_train, columns=input_name)\n",
    "    #std_df['Hardness (HV)'] = y_train\n",
    "    \n",
    "    return(scaler, std_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06a9fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(std_train_df,name,prop='e_ij_max'):\n",
    "    import seaborn as sns\n",
    "    sns.set(style=\"ticks\", color_codes=True,font_scale=2.2)\n",
    "    plt.figure(figsize=(22,12))\n",
    "    cmap = sns.diverging_palette(133,10,s=80, l=55, n=9, as_cmap=True)\n",
    "    cor_train = std_train_df.corr()\n",
    "    sns.heatmap(cor_train, annot=True, fmt='.2f',cmap=cmap) #\n",
    "\n",
    "    plt.savefig(name+'_pcc_all.pdf',dpi=1200)\n",
    "    plt.show()\n",
    "\n",
    "def pcc_fs(std_df,y_train,input_pcc,name,prop='HV'):\n",
    "    \n",
    "    std_all_feature = np.column_stack((std_df,y_train))\n",
    "    std_train_df=pd.DataFrame(data=std_all_feature, columns=input_name+[prop])\n",
    "    heatmap(std_train_df,name)\n",
    "    \n",
    "    X_train_pcc = std_train_df.loc[:,input_pcc+[prop]]\n",
    "    import seaborn as sns\n",
    "    plt.figure(figsize=(13,6))\n",
    "    cor_mid = X_train_pcc.corr()\n",
    "    sns.heatmap(cor_mid, annot=True, cmap= plt.cm.CMRmap_r,fmt='.2f')\n",
    "    plt.savefig(name+'_pcc_fs.png',dpi=1200,bbox_inches='tight')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3d84bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vif_value(datasets):\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    vif = pd.DataFrame()\n",
    "    vif['VIF Factor'] = [variance_inflation_factor(datasets.values,i) for i in range(datasets.shape[1])]\n",
    "\n",
    "    vif['features'] = datasets.columns\n",
    "\n",
    "    return(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0460a5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_fs(std_df,name, title=\"a) Hardness PCA-1\"):\n",
    "    \n",
    "    import seaborn as sns\n",
    "    import matplotlib.ticker as tck\n",
    "    \n",
    "    # Plot PCA graph\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA()\n",
    "    sns.set(style=\"ticks\", color_codes=True,font_scale=3)\n",
    "    principalComponents = pca.fit_transform(std_df)\n",
    "    plt.figure(figsize=(8,7))\n",
    "    #plt.figure()\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_),color='purple', linewidth=3)\n",
    "    plt.xlabel('No. of Principal Components')\n",
    "    plt.ylabel('Cumulative EV')\n",
    "    plt.title(title+ ' : Explained Variance ',color='blue', pad=20)\n",
    "    \n",
    "    plt.grid(alpha=0.75)\n",
    "    plt.grid(True, which='minor')\n",
    "\n",
    "    #x=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "    #x=[2,4,6,8,10,12,14]\n",
    "    plt.xlim(0, 150)\n",
    "    #values = range(len(x))\n",
    "    #plt.xticks(values, x)\n",
    "    #plt.xticks()\n",
    "    \n",
    "    # Number of x ticks\n",
    "    plt.xticks(range(0,152,50),rotation=0)\n",
    "    plt.gca().xaxis.set_minor_locator(tck.AutoMinorLocator(2))\n",
    "    #plt.gca().xaxis.set_minor_locator(MultipleLocator(12))\n",
    "    \n",
    "\n",
    "    plt.rcParams.update({'font.size': 30})\n",
    "    plt.tick_params(axis='both', which='both', length=3, width=1,color='black')\n",
    "    \n",
    "    import numpy\n",
    "    plt.yticks(numpy.linspace(0.4, 1.0, num=4))\n",
    "    \n",
    "    plt.ylim(0.2, 1.05)\n",
    "    #plt.yticks(range(0.3,0.9,0.3),rotation=0)\n",
    "    plt.gca().yaxis.set_minor_locator(MultipleLocator(0.1))\n",
    "    sns.set(style=\"ticks\", color_codes=True,font_scale=2)\n",
    "\n",
    "    plt.savefig(name+'_fs.png',dpi=1200,bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Principal components to capture 0.9 variance in data\n",
    "    pca_1 = PCA(0.96)\n",
    "    df_pca = pca_1.fit_transform(std_df)\n",
    "    \n",
    "    comp = pca_1.n_components_\n",
    "    print('No. of components for PCA:' ,comp)\n",
    "    \n",
    "    pca_new = PCA(n_components = comp)\n",
    "    df_pca_new = pca_new.fit_transform(std_df)\n",
    "    \n",
    "    print('Explained variance for 96% ', comp, 'components: ',pca_new.explained_variance_ratio_)\n",
    "    print('Cumulative:', np.cumsum(pca_new.explained_variance_ratio_))\n",
    "    \n",
    "    return(pca_1,df_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c963b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bca3c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e38ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2667a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Model\n",
    "\n",
    "\n",
    "\n",
    "# The value used in the function plays no role as the different hyperparameter value will be used while calling \"create_model\" function\n",
    "def create_model(lyrs=6, neuron_size=64, act='selu', opt='Adam', dr=0.0, learning_rate=0.001,init_weights= 'he_uniform', weight_constraint = 3):\n",
    "    import tensorflow as tf\n",
    "\n",
    "    import keras\n",
    "    from keras.layers import Dense\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dropout\n",
    "    from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # clear model\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    # create first hidden layer\n",
    "    model.add(Dense(neuron_size,input_dim=input_dim, activation=act))\n",
    "    model.add(Dropout(dr))\n",
    "    #tf.keras.layers.BatchNormalization(),\n",
    "    \n",
    "    # create additional hidden layers\n",
    "    for i in range(1,lyrs):\n",
    "        model.add(Dense(neuron_size, activation=act))\n",
    "        model.add(Dropout(dr))\n",
    "        \n",
    "    model.add(Dense(neuron_size, activation='softmax'))    \n",
    "        #tf.keras.layers.BatchNormalization(),\n",
    "    # add dropout, default is none\n",
    "    #model.add(Dropout(dr))\n",
    "    \n",
    "    # create output layer\n",
    "    model.add(Dense(18, activation='sigmoid'))  # output layer\n",
    "    opt = Adam(learning_rate=learning_rate)\n",
    "    huber = tf.keras.losses.Huber(delta=1.5)\n",
    "    model.compile(loss='mse', optimizer=opt, metrics=['mse', 'mape','mae',tf.keras.metrics.RootMeanSquaredError()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f10652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrics_plot(history,train_matrics,val_matrics,lable_name,model_name, data_of,plot_path):\n",
    "    import matplotlib.ticker as tck\n",
    "    all_train_mae_histories = []\n",
    "    train_mae_history = train_matrics\n",
    "    all_train_mae_histories.append(train_mae_history)\n",
    "    average_train_mae_history = [\n",
    "        np.mean([x[i] for x in all_train_mae_histories]) for i in range(max_epochs)]\n",
    "\n",
    "    all_val_mae_histories = []\n",
    "    val_mae_history = val_matrics\n",
    "    all_val_mae_histories.append(val_mae_history)\n",
    "    average_val_mae_history = [\n",
    "        np.mean([x[i] for x in all_val_mae_histories]) for i in range(max_epochs)]\n",
    "    \n",
    "    loss = train_matrics\n",
    "    val_loss = val_matrics\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    plt.plot(epochs, loss, 'b', linewidth=2, label='Training '+lable_name)\n",
    "    plt.plot(epochs, val_loss, '--r',  linewidth=1, label='Validation '+lable_name)\n",
    "    plt.title('Training and Validation '+lable_name)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(lable_name)\n",
    "    plt.legend()\n",
    "    #plt.savefig('mae_hardness.pdf',dpi=1200)\n",
    "    plt.show()\n",
    "    \n",
    "    def smooth_curve(points, factor=0.9):\n",
    "      smoothed_points = []\n",
    "      for point in points:\n",
    "        if smoothed_points:\n",
    "          previous = smoothed_points[-1]\n",
    "          smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "          smoothed_points.append(point)\n",
    "      return smoothed_points\n",
    "\n",
    "    smooth_train_mae_history = smooth_curve(average_train_mae_history[5:])\n",
    "    smooth_val_mae_history = smooth_curve(average_val_mae_history[5:])\n",
    "    \n",
    "    sns.set(style=\"ticks\", color_codes=True,font_scale=2.25)\n",
    "    fig, ax = plt.subplots(figsize=(6,5.5),dpi=600)\n",
    "    #plt.ylim(20, 120)    # y-label range\n",
    "    plt.gca().yaxis.set_minor_locator(tck.AutoMinorLocator(2))\n",
    "    plt.plot(range(1, len(smooth_train_mae_history) + 1), smooth_train_mae_history, 'b', label = 'Training '+lable_name)\n",
    "    plt.plot(range(1, len(smooth_val_mae_history) + 1),smooth_val_mae_history, '--r', label = 'Validation '+lable_name)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(''+lable_name)\n",
    "    #plt.title('Smooth Training and Validation '+lable_name)\n",
    "    plt.title(data_of+': '+model_name)\n",
    "    plt.legend()\n",
    "    plt.savefig(plot_path+data_of+'_'+lable_name+'.pdf',dpi=1200, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5c06ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict on test data\n",
    "\n",
    "def r2_plot(model,input_datasets,target_datasets,name,model_name,plot_path=\"plots\\\\hardness\\\\_\"):\n",
    "    sns.set(style=\"ticks\", color_codes=True,font_scale=1.25)\n",
    "    a=0.2 # Percentage error range\n",
    "    predictions_datasets = model.predict(input_datasets)\n",
    "\n",
    "    import sklearn.metrics\n",
    "    from sklearn.metrics import r2_score\n",
    "    r2_test = r2_score(target_datasets, predictions_datasets)\n",
    "    plt.figure(figsize=(4,4),dpi=200)\n",
    "\n",
    "    # plot x=y line \n",
    "    x_line = np.linspace(0, 50, 50)\n",
    "    \n",
    "    sns.lineplot(x=x_line, y=x_line,color='black',lw=0.75)\n",
    "\n",
    "    print('Test R2 score: ', r2_test)\n",
    "    \n",
    "\n",
    "\n",
    "    test_r2 = sns.regplot(x=target_datasets,y=predictions_datasets,ci=None,scatter_kws=dict(s=8,color='r'),fit_reg=False)\n",
    "    test_r2.set(title=str(model_name)+'Performance on Test data,'+' $R^2$ = ' +str(round(r2_test,3)))\n",
    "    test_r2.set_xlabel(\"Real Targets\"+\"(\"+name+\")\", fontsize = 16)\n",
    "    test_r2.set_ylabel(\"Predicted Value\"+\"(\"+name+\")\", fontsize = 16)\n",
    "\n",
    "    Y1 = x_line*(1+a)\n",
    "    Y2 = x_line*(1-a)\n",
    "\n",
    "    sns.lineplot(x=x_line,y=Y1,lw=0.5,color='b',alpha=.2)\n",
    "    sns.lineplot(x=x_line,y=Y2,lw=0.5,color='b',alpha=.2)\n",
    "\n",
    "    test_r2.fill_between(x_line, Y1,x_line,color='b',alpha=.2)\n",
    "    test_r2.fill_between(x_line, Y2,x_line,color='b',alpha=.2)\n",
    "    \n",
    "    # x and y ticks\n",
    "    listOf_Yticks = np.arange(0, 40, 5)\n",
    "    plt.yticks(listOf_Yticks)\n",
    "    plt.xticks(listOf_Yticks)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #ax.yaxis.set_minor_locator(tck.AutoMinorLocator(2))\n",
    "\n",
    "\n",
    "    test_r2.figure.savefig('r2_hardness_'+str(name)+'.png',dpi=1200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a51a554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47ee28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The value used in the function plays no role as the different hyperparameter value will be used while calling \"create_model\" function\n",
    "def create_model_class(opt='Adam', dr=0.0, learning_rate=0.001,init_weights= 'he_uniform', weight_constraint = 3):\n",
    "    import tensorflow as tf\n",
    "\n",
    "    import keras\n",
    "    from keras.layers import Dense\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dropout\n",
    "    from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # clear model\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    # create first hidden layer\n",
    "    model.add(Dense(32,input_dim=input_dim, activation='selu', kernel_initializer = init_weights, kernel_constraint = max_norm(weight_constraint),kernel_regularizer='l2'))\n",
    "    model.add(Dropout(dr))\n",
    "    #tf.keras.layers.BatchNormalization(),\n",
    "    \n",
    "    # create additional hidden layers\n",
    "    model.add(Dense(24, activation = 'relu', kernel_initializer = init_weights, kernel_constraint = max_norm(weight_constraint),kernel_regularizer='l2'))\n",
    "    model.add(Dropout(dr))\n",
    "        \n",
    "    model.add(Dense(16, activation = 'LeakyReLU', kernel_initializer = init_weights, kernel_constraint = max_norm(weight_constraint),kernel_regularizer='l2'))\n",
    "    model.add(Dropout(dr))    \n",
    "    \n",
    "    model.add(Dense(12, activation = 'PReLU', kernel_initializer = init_weights, kernel_constraint = max_norm(weight_constraint),kernel_regularizer='l2'))\n",
    "    model.add(Dropout(dr))\n",
    "    \n",
    "    model.add(Dense(8, activation = 'relu', kernel_initializer = init_weights, kernel_constraint = max_norm(weight_constraint),kernel_regularizer='l2'))\n",
    "    model.add(Dropout(dr))\n",
    "    \n",
    "    #tf.keras.layers.BatchNormalization(),\n",
    "    # add dropout, default is none\n",
    "    #model.add(Dropout(dr))\n",
    "    \n",
    "    # create output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))  # output layer\n",
    "\n",
    "    model.compile(loss= 'binary_crossentropy', optimizer=opt, metrics=['mse','accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddc6ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_model(models, test_datasets,y_test):    \n",
    "    import sklearn.metrics\n",
    "    from sklearn.metrics import r2_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    preds_array=[]\n",
    "    #type(preds_df)\n",
    "    for i in range(len(models)):\n",
    "        preds = models[i].predict(test_datasets[i])\n",
    "        accuracy_mean = accuracy_score(y_test,preds)\n",
    "        preds_array.append(preds)\n",
    "\n",
    "        print(\"R sq. for Model\",i,accuracy_mean)\n",
    "        #print(preds)\n",
    "    preds_array=np.array(preds_array)    \n",
    "    summed = np.sum(preds_array, axis=0)\n",
    "    ensemble_prediction = np.argmax(summed, axis=1)\n",
    "    mean_preds = np.mean(preds_array, axis=0)\n",
    "\n",
    "    accuracy_mean = accuracy_score(y_test,mean_preds)\n",
    "    print(\"Average accuracy:\", accuracy_mean)\n",
    "\n",
    "    # Weight calculations\n",
    "    df = pd.DataFrame([])\n",
    "\n",
    "    for w1 in range(0, 4):\n",
    "        for w2 in range(0,4):\n",
    "            for w3 in range(0,4):\n",
    "                for w4 in range(0,4):\n",
    "                    wts = [w1/10.,w2/10.,w3/10.,w4/10.]\n",
    "                    wted_preds1 = np.tensordot(preds_array, wts, axes=((0),(0)))\n",
    "                    wted_ensemble_pred = np.mean(wted_preds1, axis=1)\n",
    "                    weighted_r2 = accuracy_score(y_test, wted_ensemble_pred)\n",
    "                    df = pd.concat([df,pd.DataFrame({'acc':weighted_r2,'wt1':wts[0],'wt2':wts[1], \n",
    "                                                 'wt3':wts[2],'wt4':wts[3] }, index=[0])], ignore_index=True)\n",
    "\n",
    "    max_r2_row = df.iloc[df['acc'].idxmax()]\n",
    "    print(\"Max $R^2$ of \", max_r2_row[0], \" obained with w1=\", max_r2_row[1],\" w2=\", max_r2_row[2], \" w3=\", max_r2_row[3], \" and w4=\", max_r2_row[4])  \n",
    "    return(preds_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f280ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_model(df_piezo, cat = 'B', point=''):   \n",
    "##############################################\n",
    "    import multiprocessing\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    import keras\n",
    "\n",
    "    # Define the number of processes to use\n",
    "    num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "    # Create a shared dictionary to cache models\n",
    "    manager = multiprocessing.Manager()\n",
    "    model_cache = manager.dict()\n",
    "\n",
    "    def ensemble_modelt(df_pred, model_path= 'model_files/nn_model/cubic/'):\n",
    "\n",
    "        # Assuming your data is stored in 'data' variable\n",
    "        df_pred = df_pred.reshape(1, -1)\n",
    "        # Check if the model is already in the cache\n",
    "        if model_path in model_cache:\n",
    "            return model_cache[model_path]\n",
    "\n",
    "        scaler = pickle.load(open(model_path+'scaler_reg.pkl', 'rb'))\n",
    "        df_std = scaler.transform(df_pred)\n",
    "\n",
    "        pca_1 = pickle.load(open(model_path+'pca_reg.pkl', 'rb'))\n",
    "        df_pca = pca_1.transform(df_std)\n",
    "\n",
    "        model1 = keras.models.load_model(model_path+'model_1.h5')\n",
    "        model2 = keras.models.load_model(model_path+'model_2.h5')\n",
    "        model3 = keras.models.load_model(model_path+'model_3.h5')\n",
    "        model4 = keras.models.load_model(model_path+'model_4.h5')\n",
    "        model5 = keras.models.load_model(model_path+'model_5.h5')\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "#         for model in [model1, model2, model3, model4, model5]:\n",
    "        for model in [model2, model3]:\n",
    "            pred = model.predict(df_pca)  # Assuming the models have a predict() method\n",
    "#             print(\"pred\", pred)\n",
    "            predictions.append(pred)\n",
    "\n",
    "        ensemble_prediction = np.mean(predictions, axis=0)  # Average the predicted probabilities across models\n",
    "        ensemble_prediction = ensemble_prediction.tolist()\n",
    "        ensemble_prediction = ensemble_prediction[0]\n",
    "\n",
    "        # Store the prediction in the cache\n",
    "    #     model_cache[model_path] = ensemble_prediction\n",
    "        return ensemble_prediction\n",
    "        \n",
    "\n",
    "##############################################################################################\n",
    "################################################################################################\n",
    "\n",
    "    def input_features(df_piezo):\n",
    "        import numpy as np\n",
    "\n",
    "        df_piezo = stc.featurize_dataframe(df_piezo, \"formula_pretty\",ignore_errors=True,return_errors=True)\n",
    "        df_piezo = ef.featurize_dataframe(df_piezo, \"composition\",ignore_errors=True,return_errors=True)\n",
    "\n",
    "        from matminer.featurizers.composition import ElementProperty\n",
    "        featurizer = ElementProperty.from_preset('magpie')\n",
    "        df_piezo = featurizer.featurize_dataframe(df_piezo, col_id='composition')\n",
    "        #y = bg_data_featurized['gap expt']\n",
    "\n",
    "        %run functions.ipynb\n",
    "        df, df_input_target = properties_calculation(df_piezo)\n",
    "\n",
    "        magpie_list = ['MagpieData minimum Number',\n",
    "         'MagpieData maximum Number',\n",
    "         'MagpieData range Number',\n",
    "         'MagpieData mean Number',\n",
    "         'MagpieData avg_dev Number',\n",
    "         'MagpieData mode Number',\n",
    "         'MagpieData minimum MendeleevNumber',\n",
    "         'MagpieData maximum MendeleevNumber',\n",
    "         'MagpieData range MendeleevNumber',\n",
    "         'MagpieData mean MendeleevNumber',\n",
    "         'MagpieData avg_dev MendeleevNumber',\n",
    "         'MagpieData mode MendeleevNumber',\n",
    "         'MagpieData minimum AtomicWeight',\n",
    "         'MagpieData maximum AtomicWeight',\n",
    "         'MagpieData range AtomicWeight',\n",
    "         'MagpieData mean AtomicWeight',\n",
    "         'MagpieData avg_dev AtomicWeight',\n",
    "         'MagpieData mode AtomicWeight',\n",
    "         'MagpieData minimum MeltingT',\n",
    "         'MagpieData maximum MeltingT',\n",
    "         'MagpieData range MeltingT',\n",
    "         'MagpieData mean MeltingT',\n",
    "         'MagpieData avg_dev MeltingT',\n",
    "         'MagpieData mode MeltingT',\n",
    "         'MagpieData minimum Column',\n",
    "         'MagpieData maximum Column',\n",
    "         'MagpieData range Column',\n",
    "         'MagpieData mean Column',\n",
    "         'MagpieData avg_dev Column',\n",
    "         'MagpieData mode Column',\n",
    "         'MagpieData minimum Row',\n",
    "         'MagpieData maximum Row',\n",
    "         'MagpieData range Row',\n",
    "         'MagpieData mean Row',\n",
    "         'MagpieData avg_dev Row',\n",
    "         'MagpieData mode Row',\n",
    "         'MagpieData minimum CovalentRadius',\n",
    "         'MagpieData maximum CovalentRadius',\n",
    "         'MagpieData range CovalentRadius',\n",
    "         'MagpieData mean CovalentRadius',\n",
    "         'MagpieData avg_dev CovalentRadius',\n",
    "         'MagpieData mode CovalentRadius',\n",
    "         'MagpieData minimum Electronegativity',\n",
    "         'MagpieData maximum Electronegativity',\n",
    "         'MagpieData range Electronegativity',\n",
    "         'MagpieData mean Electronegativity',\n",
    "         'MagpieData avg_dev Electronegativity',\n",
    "         'MagpieData mode Electronegativity',\n",
    "         'MagpieData minimum NsValence',\n",
    "         'MagpieData maximum NsValence',\n",
    "         'MagpieData range NsValence',\n",
    "         'MagpieData mean NsValence',\n",
    "         'MagpieData avg_dev NsValence',\n",
    "         'MagpieData mode NsValence',\n",
    "         'MagpieData minimum NpValence',\n",
    "         'MagpieData maximum NpValence',\n",
    "         'MagpieData range NpValence',\n",
    "         'MagpieData mean NpValence',\n",
    "         'MagpieData avg_dev NpValence',\n",
    "         'MagpieData mode NpValence',\n",
    "         'MagpieData minimum NdValence',\n",
    "         'MagpieData maximum NdValence',\n",
    "         'MagpieData range NdValence',\n",
    "         'MagpieData mean NdValence',\n",
    "         'MagpieData avg_dev NdValence',\n",
    "         'MagpieData mode NdValence',\n",
    "         'MagpieData minimum NfValence',\n",
    "         'MagpieData maximum NfValence',\n",
    "         'MagpieData range NfValence',\n",
    "         'MagpieData mean NfValence',\n",
    "         'MagpieData avg_dev NfValence',\n",
    "         'MagpieData mode NfValence',\n",
    "         'MagpieData minimum NValence',\n",
    "         'MagpieData maximum NValence',\n",
    "         'MagpieData range NValence',\n",
    "         'MagpieData mean NValence',\n",
    "         'MagpieData avg_dev NValence',\n",
    "         'MagpieData mode NValence',\n",
    "         'MagpieData minimum NsUnfilled',\n",
    "         'MagpieData maximum NsUnfilled',\n",
    "         'MagpieData range NsUnfilled',\n",
    "         'MagpieData mean NsUnfilled',\n",
    "         'MagpieData avg_dev NsUnfilled',\n",
    "         'MagpieData mode NsUnfilled',\n",
    "         'MagpieData minimum NpUnfilled',\n",
    "         'MagpieData maximum NpUnfilled',\n",
    "         'MagpieData range NpUnfilled',\n",
    "         'MagpieData mean NpUnfilled',\n",
    "         'MagpieData avg_dev NpUnfilled',\n",
    "         'MagpieData mode NpUnfilled',\n",
    "         'MagpieData maximum NdUnfilled',\n",
    "         'MagpieData range NdUnfilled',\n",
    "         'MagpieData mean NdUnfilled',\n",
    "         'MagpieData avg_dev NdUnfilled',\n",
    "         'MagpieData mode NdUnfilled',\n",
    "         'MagpieData maximum NfUnfilled',\n",
    "         'MagpieData range NfUnfilled',\n",
    "         'MagpieData mean NfUnfilled',\n",
    "         'MagpieData avg_dev NfUnfilled',\n",
    "         'MagpieData minimum NUnfilled',\n",
    "         'MagpieData maximum NUnfilled',\n",
    "         'MagpieData range NUnfilled',\n",
    "         'MagpieData mean NUnfilled',\n",
    "         'MagpieData avg_dev NUnfilled',\n",
    "         'MagpieData mode NUnfilled',\n",
    "         'MagpieData minimum GSvolume_pa',\n",
    "         'MagpieData maximum GSvolume_pa',\n",
    "         'MagpieData range GSvolume_pa',\n",
    "         'MagpieData mean GSvolume_pa',\n",
    "         'MagpieData avg_dev GSvolume_pa',\n",
    "         'MagpieData mode GSvolume_pa',\n",
    "         'MagpieData minimum GSbandgap',\n",
    "         'MagpieData maximum GSbandgap',\n",
    "         'MagpieData range GSbandgap',\n",
    "         'MagpieData mean GSbandgap',\n",
    "         'MagpieData avg_dev GSbandgap',\n",
    "         'MagpieData mode GSbandgap',\n",
    "         'MagpieData maximum GSmagmom',\n",
    "         'MagpieData range GSmagmom',\n",
    "         'MagpieData mean GSmagmom',\n",
    "         'MagpieData avg_dev GSmagmom',\n",
    "         'MagpieData mode GSmagmom',\n",
    "         'MagpieData minimum SpaceGroupNumber',\n",
    "         'MagpieData maximum SpaceGroupNumber',\n",
    "         'MagpieData range SpaceGroupNumber',\n",
    "         'MagpieData mean SpaceGroupNumber',\n",
    "         'MagpieData avg_dev SpaceGroupNumber',\n",
    "         'MagpieData mode SpaceGroupNumber']\n",
    "\n",
    "        df_fs_magpie =df.loc[:, magpie_list]\n",
    "        df_input_target = df_input_target.drop(['No of Components'], axis=1)\n",
    "        df_input_target= df_input_target.iloc[:,list(range(0,13))]\n",
    "        df_features = pd.concat([df_fs_magpie,df_input_target], axis=1)\n",
    "        return df_features\n",
    "\n",
    "##############################################################################################\n",
    "################################################################################################\n",
    "\n",
    "    df_features= input_features(df_piezo)\n",
    "    path='model_files//nn_model//classification//'\n",
    "\n",
    "    import pickle\n",
    "    scaler = pickle.load(open(path+'scaler.pkl','rb'))\n",
    "    df_std = scaler.transform(df_features)\n",
    "    pca_1 = pickle.load(open(path+'pca_1.pkl','rb'))\n",
    "    df_pca =  pca_1.transform(df_std)\n",
    "\n",
    "    from tensorflow import keras\n",
    "    model_cat = keras.models.load_model('model_files/nn_model/classification/model_cat.h5')\n",
    "    model_cata = keras.models.load_model('model_files/nn_model/classification/model_cata.h5')\n",
    "    model_catb = keras.models.load_model('model_files/nn_model/classification/model_catb.h5')\n",
    "\n",
    "    y_cat = model_cat.predict(df_pca)\n",
    "    if cat=='NA':\n",
    "        category = np.where(y_cat[:, 0] > 0.5, 'A', 'B')\n",
    "    else: \n",
    "        category = np.full(y_cat.shape[0], cat)\n",
    "        \n",
    "        \n",
    "    subcategories = []\n",
    "    y_tensor = []\n",
    "    if np.any(category == 'A'):\n",
    "        y_subcat = model_cata.predict(df_pca)\n",
    "\n",
    "        for subcat in y_subcat:\n",
    "            subcategory = []\n",
    "            y_target = []\n",
    "            if subcat[0] > 0.33:\n",
    "                subcategory.append('cubic')\n",
    "            elif subcat[1] > 0.33:\n",
    "                subcategory.append('tetra42m')\n",
    "            elif subcat[2] > 0.33:\n",
    "                subcategory.append('ortho222')\n",
    "\n",
    "            subcategories.append(subcategory)\n",
    "\n",
    "    else:\n",
    "        y_subcat = model_catb.predict(df_pca)\n",
    "        for subcat in y_subcat:\n",
    "            subcategory = []\n",
    "            y_target = []\n",
    "\n",
    "            if subcat[0] > 0.5:\n",
    "                subcategory.append('orthomm2')\n",
    "            elif subcat[1] > 0.5:\n",
    "                subcategory.append('hextetramm')\n",
    "\n",
    "            subcategories.append(subcategory)\n",
    "\n",
    "#     print(subcategories)\n",
    "    if point!= '':\n",
    "        # Replace all values in `subcategory` with \"new\"\n",
    "        subcategories = np.where(subcategories != '', point, subcategories)\n",
    "\n",
    "#     print(\"* * * \")\n",
    "#     print(subcategories)\n",
    "    df_predict = df_features.values\n",
    "    y_tensor = []\n",
    "    y_value = []\n",
    "    for item in range(df_pca.shape[0]):\n",
    "        if category[item]=='A' and subcategories[item] == ['cubic']:\n",
    "            y = ensemble_modelt(df_predict[item], model_path='model_files/nn_model/cubic/')\n",
    "            y_value = [[0, 0, 0, y[0], 0, 0], [0, 0, 0, 0, y[0], 0], [0, 0, 0, 0, 0, y[0]]]\n",
    "\n",
    "        elif category[item]=='A' and subcategories[item] == ['tetra42m']:\n",
    "            y = ensemble_modelt(df_predict[item], model_path='model_files/nn_model/tetra42m/') \n",
    "            y_value = [[0, 0, 0, y[0], 0, 0], [0, 0, 0, 0, y[0], 0], [0, 0, 0, 0, 0, y[1]]]\n",
    "\n",
    "        elif category[item]=='A' and subcategories[item] == ['ortho222']:\n",
    "            y = ensemble_modelt(df_predict[item], model_path='model_files/nn_model/ortho222/') \n",
    "            y_value = [[0, 0, 0, y[0], 0, 0], [0, 0, 0, 0, y[1], 0], [0, 0, 0, 0, 0, y[2]]]\n",
    "\n",
    "        elif category[item]=='B' and subcategories[item] == ['orthomm2']:\n",
    "            y = ensemble_modelt(df_predict[item], model_path='model_files/nn_model/orthomm2/')\n",
    "            y_value = [[0, 0, 0, 0, y[0], 0], [0, 0, 0, y[1], 0, 0], [y[2], y[3], y[4], 0, 0, 0]]\n",
    "\n",
    "        elif category[item]=='B' and subcategories[item] == ['hextetramm']:\n",
    "            y = ensemble_modelt(df_predict[item], model_path='model_files/nn_model/hextetramm/') \n",
    "            y_value = [[0, 0, 0, 0, y[0], 0], [0, 0, 0, y[0], 0, 0], [y[1], y[1], y[2], 0, 0, 0]]\n",
    "       \n",
    "        y_tensor.append(y_value)\n",
    "    return category, subcategories, y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eeb32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def two_dopants_ternary(base_composition = \"(AlN)\", element1=\"Mg\", element2=\"Hg\", cat='B', point = 'hextetramm', order=[2,2]):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # x_array = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.3, 0.3, 0.3, 0.3, 0.4, 0.5, 0.6, 0.4, 0.4, 0.5])\n",
    "    # y_array = np.array([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.2, 0.2, 0.2, 0.2, 0.2, 0.3, 0.4, 0.5, 0.6, 0.3, 0.3, 0.3, 0.4, 0.5, 0.4])\n",
    "\n",
    "    x_array = np.array([0, 0, 0, 0, 0, 0, 0, 0,0, \n",
    "                        0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8,  \n",
    "                        0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
    "                        0.2, 0.3, 0.4, 0.5, 0.6, 0.7,\n",
    "                        0.2, 0.2, 0.2, 0.2, 0.2,\n",
    "                        0.3, 0.4, 0.5, 0.6,\n",
    "                        0.3, 0.3, 0.3,\n",
    "                        0.4, 0.5,\n",
    "                       0.4])\n",
    "    y_array = np.array([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, \n",
    "                        0, 0, 0, 0, 0, 0, 0,0,\n",
    "                        0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,\n",
    "                        0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
    "                        0.2, 0.3, 0.4, 0.5, 0.6,\n",
    "                        0.2, 0.2, 0.2, 0.2,\n",
    "                        0.3, 0.4, 0.5,\n",
    "                        0.3, 0.3,\n",
    "                       0.4])\n",
    "\n",
    "    compositions = []\n",
    "    for x, y in zip(x_array, y_array):\n",
    "        composition = element1 + str(x) + element2 + str(y) + base_composition #\"Al\"+str(1-x-y)+\"N\"\n",
    "        compositions.append(composition)\n",
    "\n",
    "    df_composition = pd.DataFrame(compositions, columns=[\"formula_pretty\"])\n",
    "    \n",
    "    ######################################################################################################\n",
    "    \n",
    "    # Do the required predictions\n",
    "    cat, sub, tensor_eo = prediction_model(df_composition, cat= cat, point = point)\n",
    "    ##########################################################################################################\n",
    "    ##########################################################################################################\n",
    "    \n",
    "    # Ternary Plots\n",
    "    import plotly.figure_factory as ff\n",
    "    import numpy as np\n",
    "    \n",
    "#     Take the 3x3 of tensor\n",
    "\n",
    "    target = []\n",
    "    for itm in range(len(tensor_eo)):\n",
    "#         trgt = tensor_eo[itm][order[0]][order[1]]\n",
    "        trgt = np.sqrt(np.square(tensor_eo[itm][2][0]) + np.square(tensor_eo[itm][2][1]) +np.square(tensor_eo[itm][2][2]))\n",
    "        target.append(trgt)\n",
    "\n",
    "    target = np.array(target)\n",
    "\n",
    "    # Calculate the remaining composition z\n",
    "    comp = 1 - x_array - y_array\n",
    "    pole_labels = [base_composition, element1, str(\" .  \")+element2]\n",
    "    colorscale = 'Rainbow' # Picnic Rainbow\n",
    "\n",
    "    min_comp = np.min(comp)\n",
    "    min_x = np.min(x_array)\n",
    "    min_y = np.min(y_array)\n",
    "\n",
    "    fig = ff.create_ternary_contour(np.array([comp, x_array, y_array]), target,\n",
    "                                    pole_labels=pole_labels,\n",
    "                                    interp_mode='cartesian',\n",
    "                                    ncontours=40,\n",
    "                                    colorscale=colorscale,\n",
    "                                    showscale=True,\n",
    "                                    width=1400, height=1050)\n",
    "\n",
    "    fig.update_ternaries(baxis_nticks=5)\n",
    "    fig.update_ternaries(aaxis_nticks=5)\n",
    "    fig.update_ternaries(caxis_nticks=5)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_font_size=22,\n",
    "#         xaxis_title=\"X Axis Title\",\n",
    "#         yaxis_title=\"Y Axis Title\",\n",
    "#         legend_title=\"Legend Title\",\n",
    "        font=dict(\n",
    "            size=90,\n",
    "            color=\"black\",\n",
    "            family=\"Gravitas One\"\n",
    "        ),\n",
    "        margin=dict(l=200, r=200, t=200, b=220),  # Adjust the margins\n",
    "        autosize=False,\n",
    "        paper_bgcolor='white',  # Set the background color\n",
    "        plot_bgcolor='white',  # Set the plot area color\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        ternary={\n",
    "            'aaxis': {'ticklen': 18, 'tickwidth': 4},\n",
    "            'baxis': {'ticklen': 18, 'tickwidth': 4},\n",
    "            'caxis': {'ticklen': 18, 'tickwidth': 4}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Adjust color bar padding\n",
    "    fig.update_coloraxes(colorbar=dict(\n",
    "        tickmode='auto',\n",
    "        thickness=50,\n",
    "        dtick=1,\n",
    "\n",
    "\n",
    "    ))\n",
    "\n",
    "     #Increase linewidth of contour lines\n",
    "    for contour_trace in fig['data']:\n",
    "        if isinstance(contour_trace, go.Contour):\n",
    "            contour_trace['line']['width'] = 10  # Increase linewidth for contour lines\n",
    "\n",
    "\n",
    "    \n",
    "    fig.update_ternaries(sum=1, aaxis_min=0.1, baxis_min=0.2, caxis_min=0)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "    # Locate the maximum value\n",
    "    max_index = np.argmax(target)\n",
    "    max_x = x_array[max_index]\n",
    "    max_y = y_array[max_index]\n",
    "\n",
    "    print(f\"The maximum value is {round(max(target),2)} located at {element1}={max_x}, {element2}={max_y} and the index is {max_index}\")\n",
    "    \n",
    "    return cat, sub, tensor_eo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6361c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_dopants(base_composition = \"(AlN)\", element1=\"Mg\", element2=\"Hg\", cat='B', point = 'hextetramm', order=[2,2]):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # x_array = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.3, 0.3, 0.3, 0.3, 0.4, 0.5, 0.6, 0.4, 0.4, 0.5])\n",
    "    # y_array = np.array([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.2, 0.2, 0.2, 0.2, 0.2, 0.3, 0.4, 0.5, 0.6, 0.3, 0.3, 0.3, 0.4, 0.5, 0.4])\n",
    "\n",
    "    x_array = np.array([0, 0, 0, 0, 0, 0, 0, 0,0, \n",
    "                        0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8,  \n",
    "                        0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
    "                        0.2, 0.3, 0.4, 0.5, 0.6, 0.7,\n",
    "                        0.2, 0.2, 0.2, 0.2, 0.2,\n",
    "                        0.3, 0.4, 0.5, 0.6,\n",
    "                        0.3, 0.3, 0.3,\n",
    "                        0.4, 0.5,\n",
    "                       0.4])\n",
    "    y_array = np.array([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, \n",
    "                        0, 0, 0, 0, 0, 0, 0,0,\n",
    "                        0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,\n",
    "                        0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
    "                        0.2, 0.3, 0.4, 0.5, 0.6,\n",
    "                        0.2, 0.2, 0.2, 0.2,\n",
    "                        0.3, 0.4, 0.5,\n",
    "                        0.3, 0.3,\n",
    "                       0.4])\n",
    "\n",
    "    compositions = []\n",
    "    for x, y in zip(x_array, y_array):\n",
    "        composition = element1 + str(x) + element2 + str(y) + base_composition #\"Al\"+str(1-x-y)+\"N\"\n",
    "        compositions.append(composition)\n",
    "\n",
    "    df_composition = pd.DataFrame(compositions, columns=[\"formula_pretty\"])\n",
    "    \n",
    "    ######################################################################################################\n",
    "    \n",
    "    # Do the required predictions\n",
    "    cat, sub, tensor_eo = prediction_model(df_composition, cat= cat, point = point)\n",
    "    ##########################################################################################################\n",
    "    ##########################################################################################################\n",
    "    \n",
    "    # Ternary Plots\n",
    "    import plotly.figure_factory as ff\n",
    "    import numpy as np\n",
    "    \n",
    "#     Take the 3x3 of tensor\n",
    "\n",
    "    target = []\n",
    "    target_33 = []\n",
    "    target_31 = []\n",
    "\n",
    "    for itm in range(len(tensor_eo)):\n",
    "        trgt = np.sqrt(np.square(tensor_eo[itm][2][0]) + np.square(tensor_eo[itm][2][1]) +np.square(tensor_eo[itm][2][2]))\n",
    "        target.append(trgt)\n",
    "        trgt_33 = tensor_eo[itm][2][2]\n",
    "        target_33.append(trgt_33)\n",
    "        trgt_31 = tensor_eo[itm][2][0]\n",
    "        target_31.append(trgt_31)        \n",
    "\n",
    "\n",
    "    target = np.array(target)\n",
    "\n",
    "\n",
    "    # Locate the maximum value\n",
    "    max_index = np.argmax(target)\n",
    "    max_x = x_array[max_index]\n",
    "    max_y = y_array[max_index]\n",
    "    \n",
    "    max_index_33 = np.argmax(target_33)\n",
    "    max_x_33 = x_array[max_index_33]\n",
    "    max_y_33 = y_array[max_index_33]\n",
    "\n",
    "    print(f\"The max. value is {round(max(target),2)} and {round(target_33[max_index], 2)} {round(target_31[max_index], 2)}  at {element1}={max_x}, {element2}={max_y} and the index is {max_index}\")\n",
    "    print(f\"The max. 3x3 value is {round(max(target_33),2)} and {round(target[max_index_33], 2)} {round(target_31[max_index_33], 2)}  at {element1}={max_x_33}, {element2}={max_y_33} and the index is {max_index_33}\")\n",
    "\n",
    "    return cat, sub, tensor_eo, target, target_33, target_31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80386add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_dopants_plots(base_composition = \"(AlN)\", element1=\"Mg\", cat='B', point = 'hextetramm', order=[2,2]):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # x_array = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.3, 0.3, 0.3, 0.3, 0.4, 0.5, 0.6, 0.4, 0.4, 0.5])\n",
    "    # y_array = np.array([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.2, 0.2, 0.2, 0.2, 0.2, 0.3, 0.4, 0.5, 0.6, 0.3, 0.3, 0.3, 0.4, 0.5, 0.4])\n",
    "\n",
    "    x_array = np.arange(0, 0.75, 0.025)\n",
    "    y_array = np.arange(0, 0.75, 0.025)\n",
    "\n",
    "\n",
    "    compositions = []\n",
    "    for x, y in zip(x_array, y_array):\n",
    "        composition = element1 + str(x) + base_composition #\"Al\"+str(1-x-y)+\"N\"\n",
    "        compositions.append(composition)\n",
    "\n",
    "    df_composition = pd.DataFrame(compositions, columns=[\"formula_pretty\"])\n",
    "    \n",
    "    ######################################################################################################\n",
    "    \n",
    "    # Do the required predictions\n",
    "    cat, sub, tensor_eo = prediction_model(df_composition, cat=cat, point = point)\n",
    "    ##########################################################################################################\n",
    "    ##########################################################################################################\n",
    "    \n",
    "#     target = []\n",
    "#     for itm in range(len(tensor_eo)):\n",
    "#         trgt = tensor_eo[itm][order[0]][order[1]]\n",
    "#         target.append(trgt)\n",
    "    \n",
    "    target = []\n",
    "    for itm in range(len(tensor_eo)):\n",
    "        trgt = tensor_eo[itm][order[0]][order[1]]\n",
    "        target.append(trgt)\n",
    "        \n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Generate some sample data\n",
    "    x = x_array\n",
    "    y = target\n",
    "\n",
    "    # Create a figure and axis objects\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Customize the line plot\n",
    "    line_width = 2.5\n",
    "    line_color = 'indigo'\n",
    "\n",
    "    # Plot the data\n",
    "    ax.plot(x, y, linewidth=line_width, color=line_color)\n",
    "\n",
    "    # Customize the tick labels and font size\n",
    "    tick_font_size = 18\n",
    "    ax.tick_params(axis='both', labelsize=tick_font_size)\n",
    "\n",
    "    # Customize other font sizes\n",
    "    title_font_size = 20\n",
    "    x_label_font_size = 18\n",
    "    y_label_font_size = 18\n",
    "\n",
    "    ax.set_title('Doping '+str(element1)+ \" on \"+str(base_composition), fontsize=title_font_size)\n",
    "    ax.set_xlabel('Dopants composition (x)', fontsize=x_label_font_size)\n",
    "\n",
    "    ax.set_ylabel(\"e' \"+f\"$_{order[0]}_{order[1]}$ \"+r\"$C/m^2$\", fontsize=y_label_font_size)\n",
    "    \n",
    "    # Locate the maximum value\n",
    "    max_index = np.argmax(target)\n",
    "    max_x = x_array[max_index]\n",
    "    max_y = y_array[max_index]\n",
    "    \n",
    "     # Add dashed vertical line from maximum value to y_max\n",
    "    plt.axvline(x=max_x, ymin=0, ymax=max_y, color='gray',alpha=0.25, linestyle='--')\n",
    "    plt.axhline(y=max_y, xmin=0, xmax=max_x, color='gray', alpha=0.25, linestyle='--')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    print(f\"The maximum value is {round(max(target),2)} located at {element1}={max_x} and the index is {max_index}\")\n",
    "    \n",
    "    return cat, sub, tensor_eo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcce0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_dopants_new(base_composition = \"(AlN)\", element1=\"Mg\", cat='B', point = 'hextetramm', order=[2,2]):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # x_array = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.3, 0.3, 0.3, 0.3, 0.4, 0.5, 0.6, 0.4, 0.4, 0.5])\n",
    "    # y_array = np.array([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.2, 0.2, 0.2, 0.2, 0.2, 0.3, 0.4, 0.5, 0.6, 0.3, 0.3, 0.3, 0.4, 0.5, 0.4])\n",
    "\n",
    "    x_array = np.arange(0, 0.75, 0.025)\n",
    "    y_array = np.arange(0, 0.75, 0.025)\n",
    "\n",
    "\n",
    "    compositions = []\n",
    "    for x, y in zip(x_array, y_array):\n",
    "        composition = element1 + str(x) + base_composition # \"Al\"+str(1-x)+\"N\" #base_composition #\n",
    "        compositions.append(composition)\n",
    "\n",
    "    df_composition = pd.DataFrame(compositions, columns=[\"formula_pretty\"])\n",
    "    \n",
    "    ######################################################################################################\n",
    "    \n",
    "    # Do the required predictions\n",
    "    cat, sub, tensor_eo = prediction_model(df_composition, cat=cat, point = point)\n",
    "    ##########################################################################################################\n",
    "    ##########################################################################################################\n",
    "    \n",
    "#     target = []\n",
    "#     for itm in range(len(tensor_eo)):\n",
    "#         trgt = tensor_eo[itm][order[0]][order[1]]\n",
    "#         target.append(trgt)\n",
    "    \n",
    "    target = []\n",
    "    target_33 = []\n",
    "    target_31 = []\n",
    "\n",
    "    for itm in range(len(tensor_eo)):\n",
    "        trgt = np.sqrt(np.square(tensor_eo[itm][2][0]) + np.square(tensor_eo[itm][2][1]) +np.square(tensor_eo[itm][2][2]))\n",
    "        target.append(trgt)\n",
    "        trgt_33 = tensor_eo[itm][2][2]\n",
    "        target_33.append(trgt_33)\n",
    "        trgt_31 = tensor_eo[itm][2][0]\n",
    "        target_31.append(trgt_31)        \n",
    "\n",
    "  \n",
    "    # Locate the maximum value\n",
    "    max_index = np.argmax(target)\n",
    "    max_x = x_array[max_index]\n",
    "    max_y = y_array[max_index]\n",
    "    \n",
    "    max_index_33 = np.argmax(target_33)\n",
    "    max_x_33 = x_array[max_index_33]\n",
    "    max_y_33 = y_array[max_index_33]\n",
    "\n",
    "    print(f\"The maximum value is {round(max(target),2)} and {round(target_33[max_index], 2)} {round(target_31[max_index], 2)} located at {element1}={max_x} and the index is {max_index}\")\n",
    "    print(f\"The maximum 3x3 value is {round(max(target_33),2)} and {round(target[max_index_33], 2)} {round(target_31[max_index_33], 2)} located at {element1}={max_x_33} and the index is {max_index_33}\")\n",
    "    \n",
    "    return tensor_eo, target, target_33, target_31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2416cef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
