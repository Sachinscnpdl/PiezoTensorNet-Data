{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5648bb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "from pymatgen.core.composition import Composition, Element\n",
    "from pymatgen.core.structure import SiteCollection\n",
    "from matminer.featurizers.composition.alloy import Miedema, WenAlloys,YangSolidSolution\n",
    "from matminer.featurizers.composition import ElementFraction\n",
    "from matminer.featurizers.conversions import StrToComposition\n",
    "from matminer.utils.data import MixingEnthalpy, DemlData\n",
    "from matminer.utils import data_files #for importing \"Miedema.csv\" present inside package of Matminer library\n",
    "from matplotlib.ticker import MultipleLocator # for minor tick lines\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ef= ElementFraction()\n",
    "stc = StrToComposition()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3df988",
   "metadata": {},
   "source": [
    "## Train-Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "630a9fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.read_csv('csv/piezo(Featurized)_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be0a8771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MagpieData minimum Number</th>\n",
       "      <th>MagpieData maximum Number</th>\n",
       "      <th>MagpieData range Number</th>\n",
       "      <th>MagpieData mean Number</th>\n",
       "      <th>MagpieData avg_dev Number</th>\n",
       "      <th>MagpieData mode Number</th>\n",
       "      <th>MagpieData minimum MendeleevNumber</th>\n",
       "      <th>MagpieData maximum MendeleevNumber</th>\n",
       "      <th>MagpieData range MendeleevNumber</th>\n",
       "      <th>MagpieData mean MendeleevNumber</th>\n",
       "      <th>MagpieData avg_dev MendeleevNumber</th>\n",
       "      <th>MagpieData mode MendeleevNumber</th>\n",
       "      <th>MagpieData minimum AtomicWeight</th>\n",
       "      <th>MagpieData maximum AtomicWeight</th>\n",
       "      <th>MagpieData range AtomicWeight</th>\n",
       "      <th>MagpieData mean AtomicWeight</th>\n",
       "      <th>MagpieData avg_dev AtomicWeight</th>\n",
       "      <th>MagpieData mode AtomicWeight</th>\n",
       "      <th>MagpieData minimum MeltingT</th>\n",
       "      <th>MagpieData maximum MeltingT</th>\n",
       "      <th>MagpieData range MeltingT</th>\n",
       "      <th>MagpieData mean MeltingT</th>\n",
       "      <th>MagpieData avg_dev MeltingT</th>\n",
       "      <th>MagpieData mode MeltingT</th>\n",
       "      <th>MagpieData minimum Column</th>\n",
       "      <th>MagpieData maximum Column</th>\n",
       "      <th>MagpieData range Column</th>\n",
       "      <th>MagpieData mean Column</th>\n",
       "      <th>MagpieData avg_dev Column</th>\n",
       "      <th>MagpieData mode Column</th>\n",
       "      <th>MagpieData minimum Row</th>\n",
       "      <th>MagpieData maximum Row</th>\n",
       "      <th>MagpieData range Row</th>\n",
       "      <th>MagpieData mean Row</th>\n",
       "      <th>MagpieData avg_dev Row</th>\n",
       "      <th>MagpieData mode Row</th>\n",
       "      <th>MagpieData minimum CovalentRadius</th>\n",
       "      <th>MagpieData maximum CovalentRadius</th>\n",
       "      <th>MagpieData range CovalentRadius</th>\n",
       "      <th>MagpieData mean CovalentRadius</th>\n",
       "      <th>MagpieData avg_dev CovalentRadius</th>\n",
       "      <th>MagpieData mode CovalentRadius</th>\n",
       "      <th>MagpieData minimum Electronegativity</th>\n",
       "      <th>MagpieData maximum Electronegativity</th>\n",
       "      <th>MagpieData range Electronegativity</th>\n",
       "      <th>MagpieData mean Electronegativity</th>\n",
       "      <th>MagpieData avg_dev Electronegativity</th>\n",
       "      <th>MagpieData mode Electronegativity</th>\n",
       "      <th>MagpieData minimum NsValence</th>\n",
       "      <th>MagpieData maximum NsValence</th>\n",
       "      <th>MagpieData range NsValence</th>\n",
       "      <th>MagpieData mean NsValence</th>\n",
       "      <th>MagpieData avg_dev NsValence</th>\n",
       "      <th>MagpieData mode NsValence</th>\n",
       "      <th>MagpieData minimum NpValence</th>\n",
       "      <th>MagpieData maximum NpValence</th>\n",
       "      <th>MagpieData range NpValence</th>\n",
       "      <th>MagpieData mean NpValence</th>\n",
       "      <th>MagpieData avg_dev NpValence</th>\n",
       "      <th>MagpieData mode NpValence</th>\n",
       "      <th>MagpieData minimum NdValence</th>\n",
       "      <th>MagpieData maximum NdValence</th>\n",
       "      <th>MagpieData range NdValence</th>\n",
       "      <th>MagpieData mean NdValence</th>\n",
       "      <th>MagpieData avg_dev NdValence</th>\n",
       "      <th>MagpieData mode NdValence</th>\n",
       "      <th>MagpieData minimum NfValence</th>\n",
       "      <th>MagpieData maximum NfValence</th>\n",
       "      <th>MagpieData range NfValence</th>\n",
       "      <th>MagpieData mean NfValence</th>\n",
       "      <th>MagpieData avg_dev NfValence</th>\n",
       "      <th>MagpieData mode NfValence</th>\n",
       "      <th>MagpieData minimum NValence</th>\n",
       "      <th>MagpieData maximum NValence</th>\n",
       "      <th>MagpieData range NValence</th>\n",
       "      <th>MagpieData mean NValence</th>\n",
       "      <th>MagpieData avg_dev NValence</th>\n",
       "      <th>MagpieData mode NValence</th>\n",
       "      <th>MagpieData minimum NsUnfilled</th>\n",
       "      <th>MagpieData maximum NsUnfilled</th>\n",
       "      <th>MagpieData range NsUnfilled</th>\n",
       "      <th>MagpieData mean NsUnfilled</th>\n",
       "      <th>MagpieData avg_dev NsUnfilled</th>\n",
       "      <th>MagpieData mode NsUnfilled</th>\n",
       "      <th>MagpieData minimum NpUnfilled</th>\n",
       "      <th>MagpieData maximum NpUnfilled</th>\n",
       "      <th>MagpieData range NpUnfilled</th>\n",
       "      <th>MagpieData mean NpUnfilled</th>\n",
       "      <th>MagpieData avg_dev NpUnfilled</th>\n",
       "      <th>MagpieData mode NpUnfilled</th>\n",
       "      <th>MagpieData maximum NdUnfilled</th>\n",
       "      <th>MagpieData range NdUnfilled</th>\n",
       "      <th>MagpieData mean NdUnfilled</th>\n",
       "      <th>MagpieData avg_dev NdUnfilled</th>\n",
       "      <th>MagpieData mode NdUnfilled</th>\n",
       "      <th>MagpieData maximum NfUnfilled</th>\n",
       "      <th>MagpieData range NfUnfilled</th>\n",
       "      <th>MagpieData mean NfUnfilled</th>\n",
       "      <th>MagpieData avg_dev NfUnfilled</th>\n",
       "      <th>MagpieData minimum NUnfilled</th>\n",
       "      <th>MagpieData maximum NUnfilled</th>\n",
       "      <th>MagpieData range NUnfilled</th>\n",
       "      <th>MagpieData mean NUnfilled</th>\n",
       "      <th>MagpieData avg_dev NUnfilled</th>\n",
       "      <th>MagpieData mode NUnfilled</th>\n",
       "      <th>MagpieData minimum GSvolume_pa</th>\n",
       "      <th>MagpieData maximum GSvolume_pa</th>\n",
       "      <th>MagpieData range GSvolume_pa</th>\n",
       "      <th>MagpieData mean GSvolume_pa</th>\n",
       "      <th>MagpieData avg_dev GSvolume_pa</th>\n",
       "      <th>MagpieData mode GSvolume_pa</th>\n",
       "      <th>MagpieData minimum GSbandgap</th>\n",
       "      <th>MagpieData maximum GSbandgap</th>\n",
       "      <th>MagpieData range GSbandgap</th>\n",
       "      <th>MagpieData mean GSbandgap</th>\n",
       "      <th>MagpieData avg_dev GSbandgap</th>\n",
       "      <th>MagpieData mode GSbandgap</th>\n",
       "      <th>MagpieData maximum GSmagmom</th>\n",
       "      <th>MagpieData range GSmagmom</th>\n",
       "      <th>MagpieData mean GSmagmom</th>\n",
       "      <th>MagpieData avg_dev GSmagmom</th>\n",
       "      <th>MagpieData mode GSmagmom</th>\n",
       "      <th>MagpieData minimum SpaceGroupNumber</th>\n",
       "      <th>MagpieData maximum SpaceGroupNumber</th>\n",
       "      <th>MagpieData range SpaceGroupNumber</th>\n",
       "      <th>MagpieData mean SpaceGroupNumber</th>\n",
       "      <th>MagpieData avg_dev SpaceGroupNumber</th>\n",
       "      <th>MagpieData mode SpaceGroupNumber</th>\n",
       "      <th>$\\delta$</th>\n",
       "      <th>Δ$\\chi$</th>\n",
       "      <th>ΔTm</th>\n",
       "      <th>Tm(K)</th>\n",
       "      <th>VEC</th>\n",
       "      <th>AN</th>\n",
       "      <th>K</th>\n",
       "      <th>B</th>\n",
       "      <th>ΔB</th>\n",
       "      <th>G</th>\n",
       "      <th>ΔG</th>\n",
       "      <th>ΔSmix</th>\n",
       "      <th>$\\lambda$</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>category</th>\n",
       "      <th>total</th>\n",
       "      <th>e_ij_max</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>87.0</td>\n",
       "      <td>15.999400</td>\n",
       "      <td>50.941500</td>\n",
       "      <td>34.942100</td>\n",
       "      <td>22.988551</td>\n",
       "      <td>6.989151</td>\n",
       "      <td>15.9994</td>\n",
       "      <td>54.80</td>\n",
       "      <td>2183.00</td>\n",
       "      <td>2128.20</td>\n",
       "      <td>439.351250</td>\n",
       "      <td>435.912188</td>\n",
       "      <td>54.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>114.375000</td>\n",
       "      <td>48.375000</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.93</td>\n",
       "      <td>3.44</td>\n",
       "      <td>2.51</td>\n",
       "      <td>2.272500</td>\n",
       "      <td>1.167500</td>\n",
       "      <td>3.44</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.105000</td>\n",
       "      <td>29.243333</td>\n",
       "      <td>20.138333</td>\n",
       "      <td>17.145000</td>\n",
       "      <td>9.073750</td>\n",
       "      <td>9.105000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>120.500000</td>\n",
       "      <td>108.500000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.590535</td>\n",
       "      <td>1.1870</td>\n",
       "      <td>675.083204</td>\n",
       "      <td>439.351250</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>56.388290</td>\n",
       "      <td>49.142500</td>\n",
       "      <td>0.961850</td>\n",
       "      <td>28.987500</td>\n",
       "      <td>0.687330</td>\n",
       "      <td>8.1005</td>\n",
       "      <td>23.2283</td>\n",
       "      <td>orthomm2</td>\n",
       "      <td>CAT B</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.383, 0.0], [0.0, 0.0, ...</td>\n",
       "      <td>1.190230</td>\n",
       "      <td>[0.383, 0.415, 0.5, 0.493, -0.961]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>19.222222</td>\n",
       "      <td>14.962963</td>\n",
       "      <td>8.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>81.777778</td>\n",
       "      <td>7.679012</td>\n",
       "      <td>87.0</td>\n",
       "      <td>15.999400</td>\n",
       "      <td>121.760000</td>\n",
       "      <td>105.760600</td>\n",
       "      <td>43.162500</td>\n",
       "      <td>36.217467</td>\n",
       "      <td>15.9994</td>\n",
       "      <td>54.80</td>\n",
       "      <td>2180.00</td>\n",
       "      <td>2125.20</td>\n",
       "      <td>435.295556</td>\n",
       "      <td>507.327407</td>\n",
       "      <td>54.80</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.555556</td>\n",
       "      <td>2.024691</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.888889</td>\n",
       "      <td>1.185185</td>\n",
       "      <td>2.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>90.333333</td>\n",
       "      <td>32.444444</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.44</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.923333</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>3.44</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.888889</td>\n",
       "      <td>0.197531</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.222222</td>\n",
       "      <td>1.037037</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>3.703704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.888889</td>\n",
       "      <td>2.938272</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.197531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.111111</td>\n",
       "      <td>0.617284</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.987654</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>1.037037</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.105000</td>\n",
       "      <td>33.285000</td>\n",
       "      <td>24.180000</td>\n",
       "      <td>14.518333</td>\n",
       "      <td>7.957407</td>\n",
       "      <td>9.105000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>67.555556</td>\n",
       "      <td>74.074074</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.587625</td>\n",
       "      <td>0.7370</td>\n",
       "      <td>677.987410</td>\n",
       "      <td>435.295556</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>19.222222</td>\n",
       "      <td>20.573276</td>\n",
       "      <td>64.595556</td>\n",
       "      <td>0.525916</td>\n",
       "      <td>46.166667</td>\n",
       "      <td>0.570860</td>\n",
       "      <td>8.3366</td>\n",
       "      <td>24.1428</td>\n",
       "      <td>hextetramm</td>\n",
       "      <td>CAT B</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, -0.047, 0.0], [0.0, 0.0,...</td>\n",
       "      <td>0.084143</td>\n",
       "      <td>[-0.047, -0.047, 0.051]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>19.714286</td>\n",
       "      <td>20.653061</td>\n",
       "      <td>8.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>87.0</td>\n",
       "      <td>12.010700</td>\n",
       "      <td>238.028910</td>\n",
       "      <td>226.018210</td>\n",
       "      <td>47.148087</td>\n",
       "      <td>54.537378</td>\n",
       "      <td>15.9994</td>\n",
       "      <td>54.80</td>\n",
       "      <td>3823.00</td>\n",
       "      <td>3768.20</td>\n",
       "      <td>786.428571</td>\n",
       "      <td>1045.183673</td>\n",
       "      <td>54.80</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.857143</td>\n",
       "      <td>3.102041</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.714286</td>\n",
       "      <td>1.224490</td>\n",
       "      <td>2.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>31.428571</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1.38</td>\n",
       "      <td>3.44</td>\n",
       "      <td>2.06</td>\n",
       "      <td>3.018571</td>\n",
       "      <td>0.602041</td>\n",
       "      <td>3.44</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.142857</td>\n",
       "      <td>1.224490</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.714286</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>2.204082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>2.693878</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.857143</td>\n",
       "      <td>4.326531</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.640000</td>\n",
       "      <td>20.025000</td>\n",
       "      <td>14.385000</td>\n",
       "      <td>10.170000</td>\n",
       "      <td>2.815714</td>\n",
       "      <td>9.105000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.496</td>\n",
       "      <td>4.496</td>\n",
       "      <td>0.642286</td>\n",
       "      <td>1.101061</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>45.285714</td>\n",
       "      <td>47.551020</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.636473</td>\n",
       "      <td>0.7361</td>\n",
       "      <td>1316.972832</td>\n",
       "      <td>782.757143</td>\n",
       "      <td>5.285714</td>\n",
       "      <td>19.714286</td>\n",
       "      <td>23.876129</td>\n",
       "      <td>57.257143</td>\n",
       "      <td>0.328981</td>\n",
       "      <td>53.357143</td>\n",
       "      <td>0.441039</td>\n",
       "      <td>6.6205</td>\n",
       "      <td>16.3430</td>\n",
       "      <td>orthomm2</td>\n",
       "      <td>CAT B</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, -0.09, 0.0], [0.0, 0.0, ...</td>\n",
       "      <td>0.517998</td>\n",
       "      <td>[-0.09, 0.022, 0.381, 0.013, -0.35]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.347826</td>\n",
       "      <td>2.449905</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>69.521739</td>\n",
       "      <td>23.485822</td>\n",
       "      <td>87.0</td>\n",
       "      <td>15.999400</td>\n",
       "      <td>35.453000</td>\n",
       "      <td>19.453600</td>\n",
       "      <td>21.069826</td>\n",
       "      <td>5.290879</td>\n",
       "      <td>15.9994</td>\n",
       "      <td>54.80</td>\n",
       "      <td>1687.00</td>\n",
       "      <td>1632.20</td>\n",
       "      <td>442.351739</td>\n",
       "      <td>452.808658</td>\n",
       "      <td>54.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.782609</td>\n",
       "      <td>4.098299</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.478261</td>\n",
       "      <td>0.499055</td>\n",
       "      <td>2.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>33.391304</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.93</td>\n",
       "      <td>3.44</td>\n",
       "      <td>2.51</td>\n",
       "      <td>2.551739</td>\n",
       "      <td>0.979773</td>\n",
       "      <td>3.44</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.826087</td>\n",
       "      <td>0.287335</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.695652</td>\n",
       "      <td>1.561437</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.521739</td>\n",
       "      <td>1.758034</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.287335</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.260870</td>\n",
       "      <td>1.168242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.434783</td>\n",
       "      <td>1.077505</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.105000</td>\n",
       "      <td>29.243333</td>\n",
       "      <td>20.138333</td>\n",
       "      <td>15.716993</td>\n",
       "      <td>6.899471</td>\n",
       "      <td>9.105000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.493</td>\n",
       "      <td>2.493</td>\n",
       "      <td>0.209217</td>\n",
       "      <td>0.345664</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>107.826087</td>\n",
       "      <td>103.803403</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.581325</td>\n",
       "      <td>1.0277</td>\n",
       "      <td>562.105536</td>\n",
       "      <td>442.351739</td>\n",
       "      <td>4.521739</td>\n",
       "      <td>10.347826</td>\n",
       "      <td>74.579472</td>\n",
       "      <td>52.044348</td>\n",
       "      <td>0.561106</td>\n",
       "      <td>34.400000</td>\n",
       "      <td>0.448683</td>\n",
       "      <td>10.9024</td>\n",
       "      <td>32.2616</td>\n",
       "      <td>cubic</td>\n",
       "      <td>CAT A</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.201, 0.0, 0.0], [0.0, 0.0, ...</td>\n",
       "      <td>0.200670</td>\n",
       "      <td>0.201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>39.428571</td>\n",
       "      <td>26.775510</td>\n",
       "      <td>16.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>70.142857</td>\n",
       "      <td>23.510204</td>\n",
       "      <td>88.0</td>\n",
       "      <td>32.065000</td>\n",
       "      <td>207.200000</td>\n",
       "      <td>175.135000</td>\n",
       "      <td>93.330100</td>\n",
       "      <td>70.017257</td>\n",
       "      <td>32.0650</td>\n",
       "      <td>388.36</td>\n",
       "      <td>1629.00</td>\n",
       "      <td>1240.64</td>\n",
       "      <td>773.150000</td>\n",
       "      <td>489.057143</td>\n",
       "      <td>388.36</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>5.142857</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.285714</td>\n",
       "      <td>1.469388</td>\n",
       "      <td>3.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>136.285714</td>\n",
       "      <td>35.755102</td>\n",
       "      <td>105.0</td>\n",
       "      <td>1.21</td>\n",
       "      <td>2.58</td>\n",
       "      <td>1.37</td>\n",
       "      <td>2.152857</td>\n",
       "      <td>0.538776</td>\n",
       "      <td>2.58</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.571429</td>\n",
       "      <td>1.632653</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>2.448980</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>5.224490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>10.571429</td>\n",
       "      <td>5.224490</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>2.040816</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.142857</td>\n",
       "      <td>1.306122</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25.786875</td>\n",
       "      <td>31.736667</td>\n",
       "      <td>5.949792</td>\n",
       "      <td>27.818690</td>\n",
       "      <td>2.322075</td>\n",
       "      <td>25.786875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.202</td>\n",
       "      <td>2.202</td>\n",
       "      <td>1.258286</td>\n",
       "      <td>1.078531</td>\n",
       "      <td>2.202</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>127.571429</td>\n",
       "      <td>65.795918</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.440894</td>\n",
       "      <td>0.6515</td>\n",
       "      <td>546.022223</td>\n",
       "      <td>773.150000</td>\n",
       "      <td>4.857143</td>\n",
       "      <td>39.428571</td>\n",
       "      <td>8.260000</td>\n",
       "      <td>22.028571</td>\n",
       "      <td>0.758010</td>\n",
       "      <td>32.085714</td>\n",
       "      <td>0.448433</td>\n",
       "      <td>7.9457</td>\n",
       "      <td>40.8755</td>\n",
       "      <td>ortho222</td>\n",
       "      <td>CAT A</td>\n",
       "      <td>[[0.0, 0.0, 0.0, -0.656, 0.0, 0.0], [0.0, 0.0,...</td>\n",
       "      <td>0.656000</td>\n",
       "      <td>[-0.656, -0.562, -0.562]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>9.750000</td>\n",
       "      <td>4.291667</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>69.083333</td>\n",
       "      <td>25.541667</td>\n",
       "      <td>87.0</td>\n",
       "      <td>6.941000</td>\n",
       "      <td>54.938045</td>\n",
       "      <td>47.997045</td>\n",
       "      <td>20.230281</td>\n",
       "      <td>9.365788</td>\n",
       "      <td>15.9994</td>\n",
       "      <td>54.80</td>\n",
       "      <td>1519.00</td>\n",
       "      <td>1464.20</td>\n",
       "      <td>287.048333</td>\n",
       "      <td>270.956389</td>\n",
       "      <td>54.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.583333</td>\n",
       "      <td>4.791667</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>89.250000</td>\n",
       "      <td>27.125000</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>3.44</td>\n",
       "      <td>2.46</td>\n",
       "      <td>2.664167</td>\n",
       "      <td>0.905139</td>\n",
       "      <td>3.44</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>1.416667</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.083333</td>\n",
       "      <td>1.388889</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.105000</td>\n",
       "      <td>22.570238</td>\n",
       "      <td>13.465238</td>\n",
       "      <td>12.712477</td>\n",
       "      <td>4.579539</td>\n",
       "      <td>9.105000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.625</td>\n",
       "      <td>1.625</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.451389</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>63.583333</td>\n",
       "      <td>80.708333</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.575800</td>\n",
       "      <td>0.9822</td>\n",
       "      <td>403.386616</td>\n",
       "      <td>287.048333</td>\n",
       "      <td>5.083333</td>\n",
       "      <td>9.750000</td>\n",
       "      <td>14.871505</td>\n",
       "      <td>44.910000</td>\n",
       "      <td>0.666821</td>\n",
       "      <td>37.158333</td>\n",
       "      <td>0.396665</td>\n",
       "      <td>9.3012</td>\n",
       "      <td>28.0541</td>\n",
       "      <td>ortho222</td>\n",
       "      <td>CAT A</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.248, 0.0, 0.0], [0.0, 0.0, ...</td>\n",
       "      <td>0.248230</td>\n",
       "      <td>[0.248, 0.184, 0.184]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>11.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>21.750000</td>\n",
       "      <td>8.437500</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>66.125000</td>\n",
       "      <td>21.875000</td>\n",
       "      <td>88.0</td>\n",
       "      <td>22.989769</td>\n",
       "      <td>92.906380</td>\n",
       "      <td>69.916611</td>\n",
       "      <td>46.406019</td>\n",
       "      <td>20.195081</td>\n",
       "      <td>32.0650</td>\n",
       "      <td>370.87</td>\n",
       "      <td>2750.00</td>\n",
       "      <td>2379.13</td>\n",
       "      <td>923.731250</td>\n",
       "      <td>673.586563</td>\n",
       "      <td>388.36</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>126.750000</td>\n",
       "      <td>21.750000</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2.58</td>\n",
       "      <td>1.65</td>\n",
       "      <td>2.081250</td>\n",
       "      <td>0.498750</td>\n",
       "      <td>2.58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.070000</td>\n",
       "      <td>29.243333</td>\n",
       "      <td>18.173333</td>\n",
       "      <td>21.588854</td>\n",
       "      <td>6.111641</td>\n",
       "      <td>25.786875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.202</td>\n",
       "      <td>2.202</td>\n",
       "      <td>1.101000</td>\n",
       "      <td>1.101000</td>\n",
       "      <td>2.202</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>148.500000</td>\n",
       "      <td>78.500000</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.344270</td>\n",
       "      <td>0.5720</td>\n",
       "      <td>803.444295</td>\n",
       "      <td>923.731250</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>21.750000</td>\n",
       "      <td>124.352500</td>\n",
       "      <td>60.887500</td>\n",
       "      <td>1.142579</td>\n",
       "      <td>39.037500</td>\n",
       "      <td>0.353994</td>\n",
       "      <td>10.0849</td>\n",
       "      <td>85.0892</td>\n",
       "      <td>orthomm2</td>\n",
       "      <td>CAT B</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, -11.854, 0.0], [0.0, 0.0...</td>\n",
       "      <td>11.854210</td>\n",
       "      <td>[-11.854, -0.641, -0.262, -0.42, -0.343]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>8.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>25.142857</td>\n",
       "      <td>19.591837</td>\n",
       "      <td>8.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>82.428571</td>\n",
       "      <td>5.224490</td>\n",
       "      <td>87.0</td>\n",
       "      <td>15.999400</td>\n",
       "      <td>207.200000</td>\n",
       "      <td>191.200600</td>\n",
       "      <td>58.663371</td>\n",
       "      <td>48.758824</td>\n",
       "      <td>15.9994</td>\n",
       "      <td>54.80</td>\n",
       "      <td>600.61</td>\n",
       "      <td>545.81</td>\n",
       "      <td>203.661429</td>\n",
       "      <td>170.127347</td>\n",
       "      <td>54.80</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.857143</td>\n",
       "      <td>1.306122</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.142857</td>\n",
       "      <td>1.306122</td>\n",
       "      <td>2.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>93.428571</td>\n",
       "      <td>31.346939</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1.81</td>\n",
       "      <td>3.44</td>\n",
       "      <td>1.63</td>\n",
       "      <td>2.815714</td>\n",
       "      <td>0.713469</td>\n",
       "      <td>3.44</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>1.306122</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.285714</td>\n",
       "      <td>4.897959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.428571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>11.142857</td>\n",
       "      <td>5.877551</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.142857</td>\n",
       "      <td>1.306122</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.142857</td>\n",
       "      <td>1.306122</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.105000</td>\n",
       "      <td>28.110000</td>\n",
       "      <td>19.005000</td>\n",
       "      <td>14.606429</td>\n",
       "      <td>6.287347</td>\n",
       "      <td>9.105000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>57.285714</td>\n",
       "      <td>51.755102</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.530646</td>\n",
       "      <td>0.7385</td>\n",
       "      <td>194.902036</td>\n",
       "      <td>203.661429</td>\n",
       "      <td>4.857143</td>\n",
       "      <td>25.142857</td>\n",
       "      <td>13.300903</td>\n",
       "      <td>52.480000</td>\n",
       "      <td>0.050409</td>\n",
       "      <td>38.300000</td>\n",
       "      <td>0.348557</td>\n",
       "      <td>7.9457</td>\n",
       "      <td>28.2177</td>\n",
       "      <td>orthomm2</td>\n",
       "      <td>CAT B</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.285, 0.0], [0.0, 0.0, ...</td>\n",
       "      <td>1.073001</td>\n",
       "      <td>[0.285, 0.634, 0.631, 0.49, -0.716]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>8.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>7.480000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>59.800000</td>\n",
       "      <td>36.240000</td>\n",
       "      <td>87.0</td>\n",
       "      <td>15.999400</td>\n",
       "      <td>95.960000</td>\n",
       "      <td>79.960600</td>\n",
       "      <td>30.213978</td>\n",
       "      <td>16.702933</td>\n",
       "      <td>15.9994</td>\n",
       "      <td>53.50</td>\n",
       "      <td>2896.00</td>\n",
       "      <td>2842.50</td>\n",
       "      <td>426.483000</td>\n",
       "      <td>493.903400</td>\n",
       "      <td>53.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>6.840000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>109.500000</td>\n",
       "      <td>57.600000</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.82</td>\n",
       "      <td>3.98</td>\n",
       "      <td>3.16</td>\n",
       "      <td>2.699000</td>\n",
       "      <td>1.213200</td>\n",
       "      <td>3.44</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>2.160000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>2.280000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.105000</td>\n",
       "      <td>73.106667</td>\n",
       "      <td>64.001667</td>\n",
       "      <td>24.758417</td>\n",
       "      <td>20.236283</td>\n",
       "      <td>9.105000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.970</td>\n",
       "      <td>1.970</td>\n",
       "      <td>0.591000</td>\n",
       "      <td>0.827400</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>99.700000</td>\n",
       "      <td>103.440000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.754440</td>\n",
       "      <td>1.3058</td>\n",
       "      <td>833.637612</td>\n",
       "      <td>426.492000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>47.916284</td>\n",
       "      <td>56.386000</td>\n",
       "      <td>1.098664</td>\n",
       "      <td>28.840000</td>\n",
       "      <td>0.656173</td>\n",
       "      <td>12.5108</td>\n",
       "      <td>21.9804</td>\n",
       "      <td>hextetramm</td>\n",
       "      <td>CAT B</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.042, 0.0], [0.0, 0.0, ...</td>\n",
       "      <td>0.768228</td>\n",
       "      <td>[0.042, -0.113, 0.752]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>3.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>8.875000</td>\n",
       "      <td>4.781250</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>60.375000</td>\n",
       "      <td>29.687500</td>\n",
       "      <td>87.0</td>\n",
       "      <td>6.941000</td>\n",
       "      <td>58.693400</td>\n",
       "      <td>51.752400</td>\n",
       "      <td>18.423000</td>\n",
       "      <td>10.067600</td>\n",
       "      <td>15.9994</td>\n",
       "      <td>54.80</td>\n",
       "      <td>2348.00</td>\n",
       "      <td>2293.20</td>\n",
       "      <td>650.322500</td>\n",
       "      <td>693.838750</td>\n",
       "      <td>54.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.125000</td>\n",
       "      <td>5.343750</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>26.750000</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>3.44</td>\n",
       "      <td>2.46</td>\n",
       "      <td>2.458750</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>3.44</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.125000</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>2.406250</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.125000</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.172500</td>\n",
       "      <td>16.593333</td>\n",
       "      <td>9.420833</td>\n",
       "      <td>10.887396</td>\n",
       "      <td>2.852969</td>\n",
       "      <td>9.105000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.524</td>\n",
       "      <td>1.524</td>\n",
       "      <td>0.190500</td>\n",
       "      <td>0.333375</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.595395</td>\n",
       "      <td>0.595395</td>\n",
       "      <td>0.074424</td>\n",
       "      <td>0.130243</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>112.125000</td>\n",
       "      <td>100.125000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.552847</td>\n",
       "      <td>1.0429</td>\n",
       "      <td>832.376927</td>\n",
       "      <td>650.447500</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>8.875000</td>\n",
       "      <td>36.013290</td>\n",
       "      <td>92.030000</td>\n",
       "      <td>1.078152</td>\n",
       "      <td>37.893750</td>\n",
       "      <td>0.582225</td>\n",
       "      <td>10.0849</td>\n",
       "      <td>32.9962</td>\n",
       "      <td>ortho222</td>\n",
       "      <td>CAT A</td>\n",
       "      <td>[[0.0, 0.0, 0.0, -0.044, 0.0, 0.0], [0.0, 0.0,...</td>\n",
       "      <td>0.162210</td>\n",
       "      <td>[-0.044, -0.162, -0.065]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1354 rows × 146 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MagpieData minimum Number  MagpieData maximum Number  \\\n",
       "0                           8.0                       23.0   \n",
       "1                           8.0                       51.0   \n",
       "2                           6.0                       92.0   \n",
       "3                           8.0                       17.0   \n",
       "4                          16.0                       82.0   \n",
       "...                         ...                        ...   \n",
       "1349                        3.0                       25.0   \n",
       "1350                       11.0                       41.0   \n",
       "1351                        8.0                       82.0   \n",
       "1352                        8.0                       42.0   \n",
       "1353                        3.0                       28.0   \n",
       "\n",
       "      MagpieData range Number  MagpieData mean Number  \\\n",
       "0                        15.0               11.000000   \n",
       "1                        43.0               19.222222   \n",
       "2                        86.0               19.714286   \n",
       "3                         9.0               10.347826   \n",
       "4                        66.0               39.428571   \n",
       "...                       ...                     ...   \n",
       "1349                     22.0                9.750000   \n",
       "1350                     30.0               21.750000   \n",
       "1351                     74.0               25.142857   \n",
       "1352                     34.0               14.200000   \n",
       "1353                     25.0                8.875000   \n",
       "\n",
       "      MagpieData avg_dev Number  MagpieData mode Number  \\\n",
       "0                      3.000000                     8.0   \n",
       "1                     14.962963                     8.0   \n",
       "2                     20.653061                     8.0   \n",
       "3                      2.449905                     8.0   \n",
       "4                     26.775510                    16.0   \n",
       "...                         ...                     ...   \n",
       "1349                   4.291667                     8.0   \n",
       "1350                   8.437500                    16.0   \n",
       "1351                  19.591837                     8.0   \n",
       "1352                   7.480000                     8.0   \n",
       "1353                   4.781250                     8.0   \n",
       "\n",
       "      MagpieData minimum MendeleevNumber  MagpieData maximum MendeleevNumber  \\\n",
       "0                                    2.0                                87.0   \n",
       "1                                   49.0                                87.0   \n",
       "2                                   20.0                                87.0   \n",
       "3                                    2.0                                94.0   \n",
       "4                                   29.0                                88.0   \n",
       "...                                  ...                                 ...   \n",
       "1349                                 1.0                                87.0   \n",
       "1350                                 2.0                                88.0   \n",
       "1351                                74.0                                87.0   \n",
       "1352                                 2.0                                93.0   \n",
       "1353                                 1.0                                87.0   \n",
       "\n",
       "      MagpieData range MendeleevNumber  MagpieData mean MendeleevNumber  \\\n",
       "0                                 85.0                        50.000000   \n",
       "1                                 38.0                        81.777778   \n",
       "2                                 67.0                        76.000000   \n",
       "3                                 92.0                        69.521739   \n",
       "4                                 59.0                        70.142857   \n",
       "...                                ...                              ...   \n",
       "1349                              86.0                        69.083333   \n",
       "1350                              86.0                        66.125000   \n",
       "1351                              13.0                        82.428571   \n",
       "1352                              91.0                        59.800000   \n",
       "1353                              86.0                        60.375000   \n",
       "\n",
       "      MagpieData avg_dev MendeleevNumber  MagpieData mode MendeleevNumber  \\\n",
       "0                              37.000000                             87.0   \n",
       "1                               7.679012                             87.0   \n",
       "2                              16.000000                             87.0   \n",
       "3                              23.485822                             87.0   \n",
       "4                              23.510204                             88.0   \n",
       "...                                  ...                              ...   \n",
       "1349                           25.541667                             87.0   \n",
       "1350                           21.875000                             88.0   \n",
       "1351                            5.224490                             87.0   \n",
       "1352                           36.240000                             87.0   \n",
       "1353                           29.687500                             87.0   \n",
       "\n",
       "      MagpieData minimum AtomicWeight  MagpieData maximum AtomicWeight  \\\n",
       "0                           15.999400                        50.941500   \n",
       "1                           15.999400                       121.760000   \n",
       "2                           12.010700                       238.028910   \n",
       "3                           15.999400                        35.453000   \n",
       "4                           32.065000                       207.200000   \n",
       "...                               ...                              ...   \n",
       "1349                         6.941000                        54.938045   \n",
       "1350                        22.989769                        92.906380   \n",
       "1351                        15.999400                       207.200000   \n",
       "1352                        15.999400                        95.960000   \n",
       "1353                         6.941000                        58.693400   \n",
       "\n",
       "      MagpieData range AtomicWeight  MagpieData mean AtomicWeight  \\\n",
       "0                         34.942100                     22.988551   \n",
       "1                        105.760600                     43.162500   \n",
       "2                        226.018210                     47.148087   \n",
       "3                         19.453600                     21.069826   \n",
       "4                        175.135000                     93.330100   \n",
       "...                             ...                           ...   \n",
       "1349                      47.997045                     20.230281   \n",
       "1350                      69.916611                     46.406019   \n",
       "1351                     191.200600                     58.663371   \n",
       "1352                      79.960600                     30.213978   \n",
       "1353                      51.752400                     18.423000   \n",
       "\n",
       "      MagpieData avg_dev AtomicWeight  MagpieData mode AtomicWeight  \\\n",
       "0                            6.989151                       15.9994   \n",
       "1                           36.217467                       15.9994   \n",
       "2                           54.537378                       15.9994   \n",
       "3                            5.290879                       15.9994   \n",
       "4                           70.017257                       32.0650   \n",
       "...                               ...                           ...   \n",
       "1349                         9.365788                       15.9994   \n",
       "1350                        20.195081                       32.0650   \n",
       "1351                        48.758824                       15.9994   \n",
       "1352                        16.702933                       15.9994   \n",
       "1353                        10.067600                       15.9994   \n",
       "\n",
       "      MagpieData minimum MeltingT  MagpieData maximum MeltingT  \\\n",
       "0                           54.80                      2183.00   \n",
       "1                           54.80                      2180.00   \n",
       "2                           54.80                      3823.00   \n",
       "3                           54.80                      1687.00   \n",
       "4                          388.36                      1629.00   \n",
       "...                           ...                          ...   \n",
       "1349                        54.80                      1519.00   \n",
       "1350                       370.87                      2750.00   \n",
       "1351                        54.80                       600.61   \n",
       "1352                        53.50                      2896.00   \n",
       "1353                        54.80                      2348.00   \n",
       "\n",
       "      MagpieData range MeltingT  MagpieData mean MeltingT  \\\n",
       "0                       2128.20                439.351250   \n",
       "1                       2125.20                435.295556   \n",
       "2                       3768.20                786.428571   \n",
       "3                       1632.20                442.351739   \n",
       "4                       1240.64                773.150000   \n",
       "...                         ...                       ...   \n",
       "1349                    1464.20                287.048333   \n",
       "1350                    2379.13                923.731250   \n",
       "1351                     545.81                203.661429   \n",
       "1352                    2842.50                426.483000   \n",
       "1353                    2293.20                650.322500   \n",
       "\n",
       "      MagpieData avg_dev MeltingT  MagpieData mode MeltingT  \\\n",
       "0                      435.912188                     54.80   \n",
       "1                      507.327407                     54.80   \n",
       "2                     1045.183673                     54.80   \n",
       "3                      452.808658                     54.80   \n",
       "4                      489.057143                    388.36   \n",
       "...                           ...                       ...   \n",
       "1349                   270.956389                     54.80   \n",
       "1350                   673.586563                    388.36   \n",
       "1351                   170.127347                     54.80   \n",
       "1352                   493.903400                     53.50   \n",
       "1353                   693.838750                     54.80   \n",
       "\n",
       "      MagpieData minimum Column  MagpieData maximum Column  \\\n",
       "0                           1.0                       16.0   \n",
       "1                           6.0                       16.0   \n",
       "2                           3.0                       16.0   \n",
       "3                           1.0                       17.0   \n",
       "4                           3.0                       16.0   \n",
       "...                         ...                        ...   \n",
       "1349                        1.0                       16.0   \n",
       "1350                        1.0                       16.0   \n",
       "1351                       13.0                       16.0   \n",
       "1352                        1.0                       17.0   \n",
       "1353                        1.0                       16.0   \n",
       "\n",
       "      MagpieData range Column  MagpieData mean Column  \\\n",
       "0                        15.0                9.000000   \n",
       "1                        10.0               14.555556   \n",
       "2                        13.0               13.857143   \n",
       "3                        16.0               12.782609   \n",
       "4                        13.0               12.000000   \n",
       "...                       ...                     ...   \n",
       "1349                     15.0               12.583333   \n",
       "1350                     15.0               11.500000   \n",
       "1351                      3.0               14.857143   \n",
       "1352                     16.0               10.800000   \n",
       "1353                     15.0               11.125000   \n",
       "\n",
       "      MagpieData avg_dev Column  MagpieData mode Column  \\\n",
       "0                      7.000000                    16.0   \n",
       "1                      2.024691                    16.0   \n",
       "2                      3.102041                    16.0   \n",
       "3                      4.098299                    16.0   \n",
       "4                      5.142857                    16.0   \n",
       "...                         ...                     ...   \n",
       "1349                   4.791667                    16.0   \n",
       "1350                   4.500000                    16.0   \n",
       "1351                   1.306122                    16.0   \n",
       "1352                   6.840000                    16.0   \n",
       "1353                   5.343750                    16.0   \n",
       "\n",
       "      MagpieData minimum Row  MagpieData maximum Row  MagpieData range Row  \\\n",
       "0                        2.0                     4.0                   2.0   \n",
       "1                        2.0                     5.0                   3.0   \n",
       "2                        2.0                     7.0                   5.0   \n",
       "3                        2.0                     3.0                   1.0   \n",
       "4                        3.0                     6.0                   3.0   \n",
       "...                      ...                     ...                   ...   \n",
       "1349                     2.0                     4.0                   2.0   \n",
       "1350                     3.0                     5.0                   2.0   \n",
       "1351                     2.0                     6.0                   4.0   \n",
       "1352                     2.0                     5.0                   3.0   \n",
       "1353                     2.0                     4.0                   2.0   \n",
       "\n",
       "      MagpieData mean Row  MagpieData avg_dev Row  MagpieData mode Row  \\\n",
       "0                2.625000                0.625000                  2.0   \n",
       "1                2.888889                1.185185                  2.0   \n",
       "2                2.714286                1.224490                  2.0   \n",
       "3                2.478261                0.499055                  2.0   \n",
       "4                4.285714                1.469388                  3.0   \n",
       "...                   ...                     ...                  ...   \n",
       "1349             2.333333                0.500000                  2.0   \n",
       "1350             3.500000                0.625000                  3.0   \n",
       "1351             3.142857                1.306122                  2.0   \n",
       "1352             2.800000                0.960000                  2.0   \n",
       "1353             2.250000                0.437500                  2.0   \n",
       "\n",
       "      MagpieData minimum CovalentRadius  MagpieData maximum CovalentRadius  \\\n",
       "0                                  66.0                              166.0   \n",
       "1                                  66.0                              139.0   \n",
       "2                                  66.0                              196.0   \n",
       "3                                  66.0                              166.0   \n",
       "4                                 105.0                              194.0   \n",
       "...                                 ...                                ...   \n",
       "1349                               66.0                              139.0   \n",
       "1350                              105.0                              166.0   \n",
       "1351                               66.0                              146.0   \n",
       "1352                               57.0                              203.0   \n",
       "1353                               66.0                              128.0   \n",
       "\n",
       "      MagpieData range CovalentRadius  MagpieData mean CovalentRadius  \\\n",
       "0                               100.0                      114.375000   \n",
       "1                                73.0                       90.333333   \n",
       "2                               130.0                       86.000000   \n",
       "3                               100.0                       98.000000   \n",
       "4                                89.0                      136.285714   \n",
       "...                               ...                             ...   \n",
       "1349                             73.0                       89.250000   \n",
       "1350                             61.0                      126.750000   \n",
       "1351                             80.0                       93.428571   \n",
       "1352                            146.0                      109.500000   \n",
       "1353                             62.0                       91.000000   \n",
       "\n",
       "      MagpieData avg_dev CovalentRadius  MagpieData mode CovalentRadius  \\\n",
       "0                             48.375000                            66.0   \n",
       "1                             32.444444                            66.0   \n",
       "2                             31.428571                            66.0   \n",
       "3                             33.391304                            66.0   \n",
       "4                             35.755102                           105.0   \n",
       "...                                 ...                             ...   \n",
       "1349                          27.125000                            66.0   \n",
       "1350                          21.750000                           105.0   \n",
       "1351                          31.346939                            66.0   \n",
       "1352                          57.600000                            57.0   \n",
       "1353                          26.750000                            66.0   \n",
       "\n",
       "      MagpieData minimum Electronegativity  \\\n",
       "0                                     0.93   \n",
       "1                                     1.66   \n",
       "2                                     1.38   \n",
       "3                                     0.93   \n",
       "4                                     1.21   \n",
       "...                                    ...   \n",
       "1349                                  0.98   \n",
       "1350                                  0.93   \n",
       "1351                                  1.81   \n",
       "1352                                  0.82   \n",
       "1353                                  0.98   \n",
       "\n",
       "      MagpieData maximum Electronegativity  \\\n",
       "0                                     3.44   \n",
       "1                                     3.44   \n",
       "2                                     3.44   \n",
       "3                                     3.44   \n",
       "4                                     2.58   \n",
       "...                                    ...   \n",
       "1349                                  3.44   \n",
       "1350                                  2.58   \n",
       "1351                                  3.44   \n",
       "1352                                  3.98   \n",
       "1353                                  3.44   \n",
       "\n",
       "      MagpieData range Electronegativity  MagpieData mean Electronegativity  \\\n",
       "0                                   2.51                           2.272500   \n",
       "1                                   1.78                           2.923333   \n",
       "2                                   2.06                           3.018571   \n",
       "3                                   2.51                           2.551739   \n",
       "4                                   1.37                           2.152857   \n",
       "...                                  ...                                ...   \n",
       "1349                                2.46                           2.664167   \n",
       "1350                                1.65                           2.081250   \n",
       "1351                                1.63                           2.815714   \n",
       "1352                                3.16                           2.699000   \n",
       "1353                                2.46                           2.458750   \n",
       "\n",
       "      MagpieData avg_dev Electronegativity  MagpieData mode Electronegativity  \\\n",
       "0                                 1.167500                               3.44   \n",
       "1                                 0.688889                               3.44   \n",
       "2                                 0.602041                               3.44   \n",
       "3                                 0.979773                               3.44   \n",
       "4                                 0.538776                               2.58   \n",
       "...                                    ...                                ...   \n",
       "1349                              0.905139                               3.44   \n",
       "1350                              0.498750                               2.58   \n",
       "1351                              0.713469                               3.44   \n",
       "1352                              1.213200                               3.44   \n",
       "1353                              0.981250                               3.44   \n",
       "\n",
       "      MagpieData minimum NsValence  MagpieData maximum NsValence  \\\n",
       "0                              1.0                           2.0   \n",
       "1                              1.0                           2.0   \n",
       "2                              2.0                           2.0   \n",
       "3                              1.0                           2.0   \n",
       "4                              2.0                           2.0   \n",
       "...                            ...                           ...   \n",
       "1349                           1.0                           2.0   \n",
       "1350                           1.0                           2.0   \n",
       "1351                           2.0                           2.0   \n",
       "1352                           1.0                           2.0   \n",
       "1353                           1.0                           2.0   \n",
       "\n",
       "      MagpieData range NsValence  MagpieData mean NsValence  \\\n",
       "0                            1.0                   1.625000   \n",
       "1                            1.0                   1.888889   \n",
       "2                            0.0                   2.000000   \n",
       "3                            1.0                   1.826087   \n",
       "4                            0.0                   2.000000   \n",
       "...                          ...                        ...   \n",
       "1349                         1.0                   1.833333   \n",
       "1350                         1.0                   1.500000   \n",
       "1351                         0.0                   2.000000   \n",
       "1352                         1.0                   1.600000   \n",
       "1353                         1.0                   1.750000   \n",
       "\n",
       "      MagpieData avg_dev NsValence  MagpieData mode NsValence  \\\n",
       "0                         0.468750                        2.0   \n",
       "1                         0.197531                        2.0   \n",
       "2                         0.000000                        2.0   \n",
       "3                         0.287335                        2.0   \n",
       "4                         0.000000                        2.0   \n",
       "...                            ...                        ...   \n",
       "1349                      0.277778                        2.0   \n",
       "1350                      0.500000                        2.0   \n",
       "1351                      0.000000                        2.0   \n",
       "1352                      0.480000                        2.0   \n",
       "1353                      0.375000                        2.0   \n",
       "\n",
       "      MagpieData minimum NpValence  MagpieData maximum NpValence  \\\n",
       "0                              0.0                           4.0   \n",
       "1                              0.0                           4.0   \n",
       "2                              0.0                           4.0   \n",
       "3                              0.0                           5.0   \n",
       "4                              0.0                           4.0   \n",
       "...                            ...                           ...   \n",
       "1349                           0.0                           4.0   \n",
       "1350                           0.0                           4.0   \n",
       "1351                           1.0                           4.0   \n",
       "1352                           0.0                           5.0   \n",
       "1353                           0.0                           4.0   \n",
       "\n",
       "      MagpieData range NpValence  MagpieData mean NpValence  \\\n",
       "0                            4.0                   2.000000   \n",
       "1                            4.0                   3.222222   \n",
       "2                            4.0                   3.142857   \n",
       "3                            5.0                   2.695652   \n",
       "4                            4.0                   2.571429   \n",
       "...                          ...                        ...   \n",
       "1349                         4.0                   2.833333   \n",
       "1350                         4.0                   2.000000   \n",
       "1351                         3.0                   2.857143   \n",
       "1352                         5.0                   2.700000   \n",
       "1353                         4.0                   2.125000   \n",
       "\n",
       "      MagpieData avg_dev NpValence  MagpieData mode NpValence  \\\n",
       "0                         2.000000                        4.0   \n",
       "1                         1.037037                        4.0   \n",
       "2                         1.224490                        4.0   \n",
       "3                         1.561437                        4.0   \n",
       "4                         1.632653                        4.0   \n",
       "...                            ...                        ...   \n",
       "1349                      1.416667                        4.0   \n",
       "1350                      2.000000                        4.0   \n",
       "1351                      1.306122                        4.0   \n",
       "1352                      2.160000                        4.0   \n",
       "1353                      1.875000                        4.0   \n",
       "\n",
       "      MagpieData minimum NdValence  MagpieData maximum NdValence  \\\n",
       "0                              0.0                           3.0   \n",
       "1                              0.0                          10.0   \n",
       "2                              0.0                           1.0   \n",
       "3                              0.0                           0.0   \n",
       "4                              0.0                          10.0   \n",
       "...                            ...                           ...   \n",
       "1349                           0.0                           5.0   \n",
       "1350                           0.0                          10.0   \n",
       "1351                           0.0                          10.0   \n",
       "1352                           0.0                           5.0   \n",
       "1353                           0.0                           8.0   \n",
       "\n",
       "      MagpieData range NdValence  MagpieData mean NdValence  \\\n",
       "0                            3.0                   0.375000   \n",
       "1                           10.0                   2.777778   \n",
       "2                            1.0                   0.142857   \n",
       "3                            0.0                   0.000000   \n",
       "4                           10.0                   1.428571   \n",
       "...                          ...                        ...   \n",
       "1349                         5.0                   0.416667   \n",
       "1350                        10.0                   3.000000   \n",
       "1351                        10.0                   4.285714   \n",
       "1352                         5.0                   0.500000   \n",
       "1353                         8.0                   1.000000   \n",
       "\n",
       "      MagpieData avg_dev NdValence  MagpieData mode NdValence  \\\n",
       "0                         0.656250                        0.0   \n",
       "1                         3.703704                        0.0   \n",
       "2                         0.244898                        0.0   \n",
       "3                         0.000000                        0.0   \n",
       "4                         2.448980                        0.0   \n",
       "...                            ...                        ...   \n",
       "1349                      0.763889                        0.0   \n",
       "1350                      3.750000                        0.0   \n",
       "1351                      4.897959                        0.0   \n",
       "1352                      0.900000                        0.0   \n",
       "1353                      1.750000                        0.0   \n",
       "\n",
       "      MagpieData minimum NfValence  MagpieData maximum NfValence  \\\n",
       "0                              0.0                           0.0   \n",
       "1                              0.0                           0.0   \n",
       "2                              0.0                           3.0   \n",
       "3                              0.0                           0.0   \n",
       "4                              0.0                          14.0   \n",
       "...                            ...                           ...   \n",
       "1349                           0.0                           0.0   \n",
       "1350                           0.0                           0.0   \n",
       "1351                           0.0                          14.0   \n",
       "1352                           0.0                           0.0   \n",
       "1353                           0.0                           0.0   \n",
       "\n",
       "      MagpieData range NfValence  MagpieData mean NfValence  \\\n",
       "0                            0.0                   0.000000   \n",
       "1                            0.0                   0.000000   \n",
       "2                            3.0                   0.428571   \n",
       "3                            0.0                   0.000000   \n",
       "4                           14.0                   4.571429   \n",
       "...                          ...                        ...   \n",
       "1349                         0.0                   0.000000   \n",
       "1350                         0.0                   0.000000   \n",
       "1351                        14.0                   2.000000   \n",
       "1352                         0.0                   0.000000   \n",
       "1353                         0.0                   0.000000   \n",
       "\n",
       "      MagpieData avg_dev NfValence  MagpieData mode NfValence  \\\n",
       "0                         0.000000                        0.0   \n",
       "1                         0.000000                        0.0   \n",
       "2                         0.734694                        0.0   \n",
       "3                         0.000000                        0.0   \n",
       "4                         5.224490                        0.0   \n",
       "...                            ...                        ...   \n",
       "1349                      0.000000                        0.0   \n",
       "1350                      0.000000                        0.0   \n",
       "1351                      3.428571                        0.0   \n",
       "1352                      0.000000                        0.0   \n",
       "1353                      0.000000                        0.0   \n",
       "\n",
       "      MagpieData minimum NValence  MagpieData maximum NValence  \\\n",
       "0                             1.0                          6.0   \n",
       "1                             6.0                         15.0   \n",
       "2                             4.0                          6.0   \n",
       "3                             1.0                          7.0   \n",
       "4                             6.0                         28.0   \n",
       "...                           ...                          ...   \n",
       "1349                          1.0                          7.0   \n",
       "1350                          1.0                         11.0   \n",
       "1351                          6.0                         28.0   \n",
       "1352                          1.0                          7.0   \n",
       "1353                          1.0                         10.0   \n",
       "\n",
       "      MagpieData range NValence  MagpieData mean NValence  \\\n",
       "0                           5.0                  4.000000   \n",
       "1                           9.0                  7.888889   \n",
       "2                           2.0                  5.714286   \n",
       "3                           6.0                  4.521739   \n",
       "4                          22.0                 10.571429   \n",
       "...                         ...                       ...   \n",
       "1349                        6.0                  5.083333   \n",
       "1350                       10.0                  6.500000   \n",
       "1351                       22.0                 11.142857   \n",
       "1352                        6.0                  4.800000   \n",
       "1353                        9.0                  4.875000   \n",
       "\n",
       "      MagpieData avg_dev NValence  MagpieData mode NValence  \\\n",
       "0                        2.250000                       6.0   \n",
       "1                        2.938272                       6.0   \n",
       "2                        0.489796                       6.0   \n",
       "3                        1.758034                       6.0   \n",
       "4                        5.224490                       6.0   \n",
       "...                           ...                       ...   \n",
       "1349                     1.388889                       6.0   \n",
       "1350                     2.250000                       6.0   \n",
       "1351                     5.877551                       6.0   \n",
       "1352                     2.280000                       6.0   \n",
       "1353                     2.406250                       6.0   \n",
       "\n",
       "      MagpieData minimum NsUnfilled  MagpieData maximum NsUnfilled  \\\n",
       "0                               0.0                            1.0   \n",
       "1                               0.0                            1.0   \n",
       "2                               0.0                            0.0   \n",
       "3                               0.0                            1.0   \n",
       "4                               0.0                            0.0   \n",
       "...                             ...                            ...   \n",
       "1349                            0.0                            1.0   \n",
       "1350                            0.0                            1.0   \n",
       "1351                            0.0                            0.0   \n",
       "1352                            0.0                            1.0   \n",
       "1353                            0.0                            1.0   \n",
       "\n",
       "      MagpieData range NsUnfilled  MagpieData mean NsUnfilled  \\\n",
       "0                             1.0                    0.375000   \n",
       "1                             1.0                    0.111111   \n",
       "2                             0.0                    0.000000   \n",
       "3                             1.0                    0.173913   \n",
       "4                             0.0                    0.000000   \n",
       "...                           ...                         ...   \n",
       "1349                          1.0                    0.166667   \n",
       "1350                          1.0                    0.500000   \n",
       "1351                          0.0                    0.000000   \n",
       "1352                          1.0                    0.400000   \n",
       "1353                          1.0                    0.250000   \n",
       "\n",
       "      MagpieData avg_dev NsUnfilled  MagpieData mode NsUnfilled  \\\n",
       "0                          0.468750                         0.0   \n",
       "1                          0.197531                         0.0   \n",
       "2                          0.000000                         0.0   \n",
       "3                          0.287335                         0.0   \n",
       "4                          0.000000                         0.0   \n",
       "...                             ...                         ...   \n",
       "1349                       0.277778                         0.0   \n",
       "1350                       0.500000                         0.0   \n",
       "1351                       0.000000                         0.0   \n",
       "1352                       0.480000                         0.0   \n",
       "1353                       0.375000                         0.0   \n",
       "\n",
       "      MagpieData minimum NpUnfilled  MagpieData maximum NpUnfilled  \\\n",
       "0                               0.0                            2.0   \n",
       "1                               0.0                            4.0   \n",
       "2                               0.0                            4.0   \n",
       "3                               0.0                            5.0   \n",
       "4                               0.0                            4.0   \n",
       "...                             ...                            ...   \n",
       "1349                            0.0                            3.0   \n",
       "1350                            0.0                            2.0   \n",
       "1351                            2.0                            5.0   \n",
       "1352                            0.0                            2.0   \n",
       "1353                            0.0                            5.0   \n",
       "\n",
       "      MagpieData range NpUnfilled  MagpieData mean NpUnfilled  \\\n",
       "0                             2.0                    1.000000   \n",
       "1                             4.0                    2.111111   \n",
       "2                             4.0                    2.000000   \n",
       "3                             5.0                    2.260870   \n",
       "4                             4.0                    1.714286   \n",
       "...                           ...                         ...   \n",
       "1349                          3.0                    1.666667   \n",
       "1350                          2.0                    1.000000   \n",
       "1351                          3.0                    3.142857   \n",
       "1352                          2.0                    0.900000   \n",
       "1353                          5.0                    1.625000   \n",
       "\n",
       "      MagpieData avg_dev NpUnfilled  MagpieData mode NpUnfilled  \\\n",
       "0                          1.000000                         2.0   \n",
       "1                          0.617284                         2.0   \n",
       "2                          0.571429                         2.0   \n",
       "3                          1.168242                         2.0   \n",
       "4                          0.979592                         2.0   \n",
       "...                             ...                         ...   \n",
       "1349                       0.833333                         2.0   \n",
       "1350                       1.000000                         2.0   \n",
       "1351                       1.306122                         2.0   \n",
       "1352                       0.720000                         1.0   \n",
       "1353                       1.218750                         2.0   \n",
       "\n",
       "      MagpieData maximum NdUnfilled  MagpieData range NdUnfilled  \\\n",
       "0                               7.0                          7.0   \n",
       "1                               5.0                          5.0   \n",
       "2                               9.0                          9.0   \n",
       "3                               0.0                          0.0   \n",
       "4                               0.0                          0.0   \n",
       "...                             ...                          ...   \n",
       "1349                            5.0                          5.0   \n",
       "1350                            6.0                          6.0   \n",
       "1351                            0.0                          0.0   \n",
       "1352                            5.0                          5.0   \n",
       "1353                            2.0                          2.0   \n",
       "\n",
       "      MagpieData mean NdUnfilled  MagpieData avg_dev NdUnfilled  \\\n",
       "0                       0.875000                       1.531250   \n",
       "1                       0.555556                       0.987654   \n",
       "2                       1.285714                       2.204082   \n",
       "3                       0.000000                       0.000000   \n",
       "4                       0.000000                       0.000000   \n",
       "...                          ...                            ...   \n",
       "1349                    0.416667                       0.763889   \n",
       "1350                    0.750000                       1.312500   \n",
       "1351                    0.000000                       0.000000   \n",
       "1352                    0.500000                       0.900000   \n",
       "1353                    0.250000                       0.437500   \n",
       "\n",
       "      MagpieData mode NdUnfilled  MagpieData maximum NfUnfilled  \\\n",
       "0                            0.0                            0.0   \n",
       "1                            0.0                            0.0   \n",
       "2                            0.0                           11.0   \n",
       "3                            0.0                            0.0   \n",
       "4                            0.0                            5.0   \n",
       "...                          ...                            ...   \n",
       "1349                         0.0                            0.0   \n",
       "1350                         0.0                            0.0   \n",
       "1351                         0.0                            0.0   \n",
       "1352                         0.0                            0.0   \n",
       "1353                         0.0                            0.0   \n",
       "\n",
       "      MagpieData range NfUnfilled  MagpieData mean NfUnfilled  \\\n",
       "0                             0.0                    0.000000   \n",
       "1                             0.0                    0.000000   \n",
       "2                            11.0                    1.571429   \n",
       "3                             0.0                    0.000000   \n",
       "4                             5.0                    1.428571   \n",
       "...                           ...                         ...   \n",
       "1349                          0.0                    0.000000   \n",
       "1350                          0.0                    0.000000   \n",
       "1351                          0.0                    0.000000   \n",
       "1352                          0.0                    0.000000   \n",
       "1353                          0.0                    0.000000   \n",
       "\n",
       "      MagpieData avg_dev NfUnfilled  MagpieData minimum NUnfilled  \\\n",
       "0                          0.000000                           1.0   \n",
       "1                          0.000000                           2.0   \n",
       "2                          2.693878                           2.0   \n",
       "3                          0.000000                           1.0   \n",
       "4                          2.040816                           2.0   \n",
       "...                             ...                           ...   \n",
       "1349                       0.000000                           1.0   \n",
       "1350                       0.000000                           1.0   \n",
       "1351                       0.000000                           2.0   \n",
       "1352                       0.000000                           1.0   \n",
       "1353                       0.000000                           1.0   \n",
       "\n",
       "      MagpieData maximum NUnfilled  MagpieData range NUnfilled  \\\n",
       "0                              7.0                         6.0   \n",
       "1                              6.0                         4.0   \n",
       "2                             20.0                        18.0   \n",
       "3                              5.0                         4.0   \n",
       "4                              5.0                         3.0   \n",
       "...                            ...                         ...   \n",
       "1349                           5.0                         4.0   \n",
       "1350                           7.0                         6.0   \n",
       "1351                           5.0                         3.0   \n",
       "1352                           6.0                         5.0   \n",
       "1353                           5.0                         4.0   \n",
       "\n",
       "      MagpieData mean NUnfilled  MagpieData avg_dev NUnfilled  \\\n",
       "0                      2.250000                      1.187500   \n",
       "1                      2.777778                      1.037037   \n",
       "2                      4.857143                      4.326531   \n",
       "3                      2.434783                      1.077505   \n",
       "4                      3.142857                      1.306122   \n",
       "...                         ...                           ...   \n",
       "1349                   2.250000                      0.708333   \n",
       "1350                   2.250000                      1.187500   \n",
       "1351                   3.142857                      1.306122   \n",
       "1352                   1.800000                      0.960000   \n",
       "1353                   2.125000                      0.718750   \n",
       "\n",
       "      MagpieData mode NUnfilled  MagpieData minimum GSvolume_pa  \\\n",
       "0                           2.0                        9.105000   \n",
       "1                           2.0                        9.105000   \n",
       "2                           2.0                        5.640000   \n",
       "3                           2.0                        9.105000   \n",
       "4                           2.0                       25.786875   \n",
       "...                         ...                             ...   \n",
       "1349                        2.0                        9.105000   \n",
       "1350                        2.0                       11.070000   \n",
       "1351                        2.0                        9.105000   \n",
       "1352                        1.0                        9.105000   \n",
       "1353                        2.0                        7.172500   \n",
       "\n",
       "      MagpieData maximum GSvolume_pa  MagpieData range GSvolume_pa  \\\n",
       "0                          29.243333                     20.138333   \n",
       "1                          33.285000                     24.180000   \n",
       "2                          20.025000                     14.385000   \n",
       "3                          29.243333                     20.138333   \n",
       "4                          31.736667                      5.949792   \n",
       "...                              ...                           ...   \n",
       "1349                       22.570238                     13.465238   \n",
       "1350                       29.243333                     18.173333   \n",
       "1351                       28.110000                     19.005000   \n",
       "1352                       73.106667                     64.001667   \n",
       "1353                       16.593333                      9.420833   \n",
       "\n",
       "      MagpieData mean GSvolume_pa  MagpieData avg_dev GSvolume_pa  \\\n",
       "0                       17.145000                        9.073750   \n",
       "1                       14.518333                        7.957407   \n",
       "2                       10.170000                        2.815714   \n",
       "3                       15.716993                        6.899471   \n",
       "4                       27.818690                        2.322075   \n",
       "...                           ...                             ...   \n",
       "1349                    12.712477                        4.579539   \n",
       "1350                    21.588854                        6.111641   \n",
       "1351                    14.606429                        6.287347   \n",
       "1352                    24.758417                       20.236283   \n",
       "1353                    10.887396                        2.852969   \n",
       "\n",
       "      MagpieData mode GSvolume_pa  MagpieData minimum GSbandgap  \\\n",
       "0                        9.105000                           0.0   \n",
       "1                        9.105000                           0.0   \n",
       "2                        9.105000                           0.0   \n",
       "3                        9.105000                           0.0   \n",
       "4                       25.786875                           0.0   \n",
       "...                           ...                           ...   \n",
       "1349                     9.105000                           0.0   \n",
       "1350                    25.786875                           0.0   \n",
       "1351                     9.105000                           0.0   \n",
       "1352                     9.105000                           0.0   \n",
       "1353                     9.105000                           0.0   \n",
       "\n",
       "      MagpieData maximum GSbandgap  MagpieData range GSbandgap  \\\n",
       "0                            0.000                       0.000   \n",
       "1                            0.000                       0.000   \n",
       "2                            4.496                       4.496   \n",
       "3                            2.493                       2.493   \n",
       "4                            2.202                       2.202   \n",
       "...                            ...                         ...   \n",
       "1349                         1.625                       1.625   \n",
       "1350                         2.202                       2.202   \n",
       "1351                         0.000                       0.000   \n",
       "1352                         1.970                       1.970   \n",
       "1353                         1.524                       1.524   \n",
       "\n",
       "      MagpieData mean GSbandgap  MagpieData avg_dev GSbandgap  \\\n",
       "0                      0.000000                      0.000000   \n",
       "1                      0.000000                      0.000000   \n",
       "2                      0.642286                      1.101061   \n",
       "3                      0.209217                      0.345664   \n",
       "4                      1.258286                      1.078531   \n",
       "...                         ...                           ...   \n",
       "1349                   0.270833                      0.451389   \n",
       "1350                   1.101000                      1.101000   \n",
       "1351                   0.000000                      0.000000   \n",
       "1352                   0.591000                      0.827400   \n",
       "1353                   0.190500                      0.333375   \n",
       "\n",
       "      MagpieData mode GSbandgap  MagpieData maximum GSmagmom  \\\n",
       "0                         0.000                     0.000000   \n",
       "1                         0.000                     0.000000   \n",
       "2                         0.000                     0.000000   \n",
       "3                         0.000                     0.000000   \n",
       "4                         2.202                     0.000000   \n",
       "...                         ...                          ...   \n",
       "1349                      0.000                     0.000310   \n",
       "1350                      2.202                     0.000000   \n",
       "1351                      0.000                     0.000000   \n",
       "1352                      0.000                     0.000000   \n",
       "1353                      0.000                     0.595395   \n",
       "\n",
       "      MagpieData range GSmagmom  MagpieData mean GSmagmom  \\\n",
       "0                      0.000000                  0.000000   \n",
       "1                      0.000000                  0.000000   \n",
       "2                      0.000000                  0.000000   \n",
       "3                      0.000000                  0.000000   \n",
       "4                      0.000000                  0.000000   \n",
       "...                         ...                       ...   \n",
       "1349                   0.000310                  0.000026   \n",
       "1350                   0.000000                  0.000000   \n",
       "1351                   0.000000                  0.000000   \n",
       "1352                   0.000000                  0.000000   \n",
       "1353                   0.595395                  0.074424   \n",
       "\n",
       "      MagpieData avg_dev GSmagmom  MagpieData mode GSmagmom  \\\n",
       "0                        0.000000                       0.0   \n",
       "1                        0.000000                       0.0   \n",
       "2                        0.000000                       0.0   \n",
       "3                        0.000000                       0.0   \n",
       "4                        0.000000                       0.0   \n",
       "...                           ...                       ...   \n",
       "1349                     0.000047                       0.0   \n",
       "1350                     0.000000                       0.0   \n",
       "1351                     0.000000                       0.0   \n",
       "1352                     0.000000                       0.0   \n",
       "1353                     0.130243                       0.0   \n",
       "\n",
       "      MagpieData minimum SpaceGroupNumber  \\\n",
       "0                                    12.0   \n",
       "1                                    12.0   \n",
       "2                                    12.0   \n",
       "3                                    12.0   \n",
       "4                                    70.0   \n",
       "...                                   ...   \n",
       "1349                                  2.0   \n",
       "1350                                 70.0   \n",
       "1351                                 12.0   \n",
       "1352                                 12.0   \n",
       "1353                                 12.0   \n",
       "\n",
       "      MagpieData maximum SpaceGroupNumber  MagpieData range SpaceGroupNumber  \\\n",
       "0                                   229.0                              217.0   \n",
       "1                                   229.0                              217.0   \n",
       "2                                   194.0                              182.0   \n",
       "3                                   229.0                              217.0   \n",
       "4                                   225.0                              155.0   \n",
       "...                                   ...                                ...   \n",
       "1349                                229.0                              227.0   \n",
       "1350                                229.0                              159.0   \n",
       "1351                                225.0                              213.0   \n",
       "1352                                229.0                              217.0   \n",
       "1353                                229.0                              217.0   \n",
       "\n",
       "      MagpieData mean SpaceGroupNumber  MagpieData avg_dev SpaceGroupNumber  \\\n",
       "0                           120.500000                           108.500000   \n",
       "1                            67.555556                            74.074074   \n",
       "2                            45.285714                            47.551020   \n",
       "3                           107.826087                           103.803403   \n",
       "4                           127.571429                            65.795918   \n",
       "...                                ...                                  ...   \n",
       "1349                         63.583333                            80.708333   \n",
       "1350                        148.500000                            78.500000   \n",
       "1351                         57.285714                            51.755102   \n",
       "1352                         99.700000                           103.440000   \n",
       "1353                        112.125000                           100.125000   \n",
       "\n",
       "      MagpieData mode SpaceGroupNumber  $\\delta$  Δ$\\chi$          ΔTm  \\\n",
       "0                                 12.0  0.590535   1.1870   675.083204   \n",
       "1                                 12.0  0.587625   0.7370   677.987410   \n",
       "2                                 12.0  0.636473   0.7361  1316.972832   \n",
       "3                                 12.0  0.581325   1.0277   562.105536   \n",
       "4                                 70.0  0.440894   0.6515   546.022223   \n",
       "...                                ...       ...      ...          ...   \n",
       "1349                              12.0  0.575800   0.9822   403.386616   \n",
       "1350                              70.0  0.344270   0.5720   803.444295   \n",
       "1351                              12.0  0.530646   0.7385   194.902036   \n",
       "1352                              12.0  0.754440   1.3058   833.637612   \n",
       "1353                              12.0  0.552847   1.0429   832.376927   \n",
       "\n",
       "           Tm(K)       VEC         AN           K          B        ΔB  \\\n",
       "0     439.351250  4.000000  11.000000   56.388290  49.142500  0.961850   \n",
       "1     435.295556  5.666667  19.222222   20.573276  64.595556  0.525916   \n",
       "2     782.757143  5.285714  19.714286   23.876129  57.257143  0.328981   \n",
       "3     442.351739  4.521739  10.347826   74.579472  52.044348  0.561106   \n",
       "4     773.150000  4.857143  39.428571    8.260000  22.028571  0.758010   \n",
       "...          ...       ...        ...         ...        ...       ...   \n",
       "1349  287.048333  5.083333   9.750000   14.871505  44.910000  0.666821   \n",
       "1350  923.731250  6.500000  21.750000  124.352500  60.887500  1.142579   \n",
       "1351  203.661429  4.857143  25.142857   13.300903  52.480000  0.050409   \n",
       "1352  426.492000  4.800000  14.200000   47.916284  56.386000  1.098664   \n",
       "1353  650.447500  4.875000   8.875000   36.013290  92.030000  1.078152   \n",
       "\n",
       "              G        ΔG    ΔSmix  $\\lambda$ subcategory category  \\\n",
       "0     28.987500  0.687330   8.1005    23.2283    orthomm2    CAT B   \n",
       "1     46.166667  0.570860   8.3366    24.1428  hextetramm    CAT B   \n",
       "2     53.357143  0.441039   6.6205    16.3430    orthomm2    CAT B   \n",
       "3     34.400000  0.448683  10.9024    32.2616       cubic    CAT A   \n",
       "4     32.085714  0.448433   7.9457    40.8755    ortho222    CAT A   \n",
       "...         ...       ...      ...        ...         ...      ...   \n",
       "1349  37.158333  0.396665   9.3012    28.0541    ortho222    CAT A   \n",
       "1350  39.037500  0.353994  10.0849    85.0892    orthomm2    CAT B   \n",
       "1351  38.300000  0.348557   7.9457    28.2177    orthomm2    CAT B   \n",
       "1352  28.840000  0.656173  12.5108    21.9804  hextetramm    CAT B   \n",
       "1353  37.893750  0.582225  10.0849    32.9962    ortho222    CAT A   \n",
       "\n",
       "                                                  total   e_ij_max  \\\n",
       "0     [[0.0, 0.0, 0.0, 0.0, 0.383, 0.0], [0.0, 0.0, ...   1.190230   \n",
       "1     [[0.0, 0.0, 0.0, 0.0, -0.047, 0.0], [0.0, 0.0,...   0.084143   \n",
       "2     [[0.0, 0.0, 0.0, 0.0, -0.09, 0.0], [0.0, 0.0, ...   0.517998   \n",
       "3     [[0.0, 0.0, 0.0, 0.201, 0.0, 0.0], [0.0, 0.0, ...   0.200670   \n",
       "4     [[0.0, 0.0, 0.0, -0.656, 0.0, 0.0], [0.0, 0.0,...   0.656000   \n",
       "...                                                 ...        ...   \n",
       "1349  [[0.0, 0.0, 0.0, 0.248, 0.0, 0.0], [0.0, 0.0, ...   0.248230   \n",
       "1350  [[0.0, 0.0, 0.0, 0.0, -11.854, 0.0], [0.0, 0.0...  11.854210   \n",
       "1351  [[0.0, 0.0, 0.0, 0.0, 0.285, 0.0], [0.0, 0.0, ...   1.073001   \n",
       "1352  [[0.0, 0.0, 0.0, 0.0, 0.042, 0.0], [0.0, 0.0, ...   0.768228   \n",
       "1353  [[0.0, 0.0, 0.0, -0.044, 0.0, 0.0], [0.0, 0.0,...   0.162210   \n",
       "\n",
       "                                        target  \n",
       "0           [0.383, 0.415, 0.5, 0.493, -0.961]  \n",
       "1                      [-0.047, -0.047, 0.051]  \n",
       "2          [-0.09, 0.022, 0.381, 0.013, -0.35]  \n",
       "3                                        0.201  \n",
       "4                     [-0.656, -0.562, -0.562]  \n",
       "...                                        ...  \n",
       "1349                     [0.248, 0.184, 0.184]  \n",
       "1350  [-11.854, -0.641, -0.262, -0.42, -0.343]  \n",
       "1351       [0.285, 0.634, 0.631, 0.49, -0.716]  \n",
       "1352                    [0.042, -0.113, 0.752]  \n",
       "1353                  [-0.044, -0.162, -0.065]  \n",
       "\n",
       "[1354 rows x 146 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e34e9bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def str_to_list(s):\n",
    "    return ast.literal_eval(s)\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "df_features['total'] = df_features['total'].apply(str_to_list)\n",
    "df_features['target'] = df_features['target'].apply(str_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a81e03c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cubic = df_features[df_features['subcategory'] == 'cubic'] \n",
    "df_tetra42m = df_features[df_features['subcategory'] == 'tetra-42m'] \n",
    "df_ortho222 = df_features[df_features['subcategory'] == 'ortho222'] \n",
    "\n",
    "df_orthomm2 = df_features[df_features['subcategory'] == 'orthomm2'] \n",
    "df_hextetramm = df_features[df_features['subcategory'] == 'hextetramm'] \n",
    "\n",
    "\n",
    "def tensor_preprocessing(df):\n",
    "    \n",
    "    df = df.reset_index(drop=True)\n",
    "    target = df['target']\n",
    "    df = df.drop(['subcategory', 'category', 'total', 'e_ij_max', 'target'], axis=1)\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, target, test_size=0.1, random_state=33)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# X_train_hextetramm, X_test_hextetramm, y_train_hextetramm, y_test_hextetramm\n",
    "\n",
    "X_train_cubic, X_test_cubic, y_train_cubic, y_test_cubic = tensor_preprocessing(df_cubic)\n",
    "X_train_tetra42m, X_test_tetra42m, y_train_tetra42m, y_test_tetra42m  = tensor_preprocessing(df_tetra42m)\n",
    "X_train_ortho222, X_test_ortho222, y_train_ortho222, y_test_ortho222  = tensor_preprocessing(df_ortho222)\n",
    "\n",
    "X_train_orthomm2, X_test_orthomm2, y_train_orthomm2, y_test_orthomm2  = tensor_preprocessing(df_orthomm2)\n",
    "X_train_hextetramm, X_test_hextetramm, y_train_hextetramm, y_test_hextetramm = tensor_preprocessing(df_hextetramm)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e908764d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(367, 152, 305, 335, 195)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cubic.shape[0], df_tetra42m.shape[0], df_ortho222.shape[0], df_orthomm2.shape[0], df_hextetramm.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a1678f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cf45c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "433a09a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path='model_files//for_regression//cubic'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222ddd41",
   "metadata": {},
   "source": [
    "# Model for Piezo Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007e7094",
   "metadata": {},
   "source": [
    "## 1. 1  CAT A: Cubic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "244e47b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of components for PCA: 33\n",
      "Explained variance for 96%  33 components:  [0.20062337 0.1360266  0.09961418 0.08188529 0.0597429  0.0422679\n",
      " 0.03658428 0.03431791 0.02834419 0.02589469 0.02275784 0.0181531\n",
      " 0.01706819 0.01604073 0.01515965 0.01463005 0.0123841  0.01168724\n",
      " 0.01032211 0.00944983 0.00811485 0.00746072 0.00675899 0.00584221\n",
      " 0.00566479 0.00540779 0.00512741 0.00489895 0.00439256 0.0039943\n",
      " 0.00388084 0.00381632 0.00328879]\n",
      "Cumulative: [0.20062337 0.33664997 0.43626415 0.51814944 0.57789234 0.62016025\n",
      " 0.65674453 0.69106244 0.71940663 0.74530132 0.76805915 0.78621225\n",
      " 0.80328044 0.81932116 0.83448081 0.84911086 0.86149496 0.87318221\n",
      " 0.88350431 0.89295414 0.90106899 0.90852971 0.9152887  0.92113091\n",
      " 0.9267957  0.9322035  0.9373309  0.94222986 0.94662242 0.95061672\n",
      " 0.95449757 0.95831389 0.96160268]\n"
     ]
    }
   ],
   "source": [
    "# %run functions.ipynb\n",
    "# scaler_cubic, std_train_cubic = std_data(X_train_cubic)\n",
    "# input_name = list(X_train_cubic.columns.values)\n",
    "\n",
    "# X_train_cubic, X_test_cubic, y_train_cubic, y_test_cubic = tensor_preprocessing(df_cubic)\n",
    "# X_train_tetra42m, X_test_tetra42m, y_train_tetra42m, y_test_tetra42m  = tensor_preprocessing(df_tetra42m)\n",
    "# X_train_ortho222, X_test_ortho222, y_train_ortho222, y_test_ortho222  = tensor_preprocessing(df_ortho222)\n",
    "\n",
    "# X_train_orthomm2, X_test_orthomm2, y_train_orthomm2, y_test_orthomm2  = tensor_preprocessing(df_orthomm2)\n",
    "# X_train_hextetramm, X_test_hextetramm, y_train_hextetramm, y_test_hextetramm = tensor_preprocessing(df_hextetramm)\n",
    "\n",
    "path='model_files//nn_model//cubic//'\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Standardize the data\n",
    "scaler_cubic = StandardScaler() # RENAME here 1 places\n",
    "std_train = scaler_cubic.fit_transform(X_train_cubic) # RENAME here 1 places\n",
    "\n",
    "# std_train_cubic = pd.DataFrame(data= std_train_cubic, columns=input_name)\n",
    "\n",
    "import pickle\n",
    "pickle.dump(scaler_cubic, open(path+'scaler_reg.pkl','wb')) # RENAME here 2 places\n",
    "\n",
    "std_test = scaler_cubic.transform(X_test_cubic) # RENAME here 1 places\n",
    "\n",
    "\n",
    "%run functions.ipynb\n",
    "\n",
    "pca_cubic, X_train_pca_cubic = pca_fs(std_train,\"plots\\\\\", title=\"a) PCA\") # RENAME here 1 places\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import pickle\n",
    "pickle.dump(pca_cubic, open(path+'pca_reg.pkl','wb')) # RENAME here 2 places\n",
    "\n",
    "X_test_pca_cubic =  pca_cubic.transform(std_test)      # RENAME here 1 places\n",
    "\n",
    "\n",
    "y_train = y_train_cubic\n",
    "X_train = X_train_pca_cubic\n",
    "\n",
    "y_test = y_test_cubic\n",
    "X_test = X_test_pca_cubic\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "y_train = np.array(list(y_train))\n",
    "y_test = np.array(list(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048d5f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66af45d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a8f836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "643982e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "30/30 - 1s - loss: 5.7170 - mae: 0.5981 - root_mean_squared_error: 0.7505 - val_loss: 5.1728 - val_mae: 1.0593 - val_root_mean_squared_error: 1.8583 - 532ms/epoch - 18ms/step\n",
      "Epoch 2/200\n",
      "30/30 - 0s - loss: 4.2623 - mae: 0.4867 - root_mean_squared_error: 0.6305 - val_loss: 3.9351 - val_mae: 0.9083 - val_root_mean_squared_error: 1.7757 - 103ms/epoch - 3ms/step\n",
      "Epoch 3/200\n",
      "30/30 - 0s - loss: 3.1470 - mae: 0.3874 - root_mean_squared_error: 0.5409 - val_loss: 2.9431 - val_mae: 0.7268 - val_root_mean_squared_error: 1.6719 - 159ms/epoch - 5ms/step\n",
      "Epoch 4/200\n",
      "30/30 - 0s - loss: 2.2749 - mae: 0.3298 - root_mean_squared_error: 0.5080 - val_loss: 2.1496 - val_mae: 0.6807 - val_root_mean_squared_error: 1.6296 - 149ms/epoch - 5ms/step\n",
      "Epoch 5/200\n",
      "30/30 - 0s - loss: 1.5987 - mae: 0.3331 - root_mean_squared_error: 0.5045 - val_loss: 1.5558 - val_mae: 0.6852 - val_root_mean_squared_error: 1.6249 - 123ms/epoch - 4ms/step\n",
      "Epoch 6/200\n",
      "30/30 - 0s - loss: 1.0800 - mae: 0.3314 - root_mean_squared_error: 0.5026 - val_loss: 1.1117 - val_mae: 0.6907 - val_root_mean_squared_error: 1.6231 - 96ms/epoch - 3ms/step\n",
      "Epoch 7/200\n",
      "30/30 - 0s - loss: 0.7013 - mae: 0.3342 - root_mean_squared_error: 0.5040 - val_loss: 0.8024 - val_mae: 0.6921 - val_root_mean_squared_error: 1.6253 - 106ms/epoch - 4ms/step\n",
      "Epoch 8/200\n",
      "30/30 - 0s - loss: 0.4440 - mae: 0.3353 - root_mean_squared_error: 0.5050 - val_loss: 0.5999 - val_mae: 0.6923 - val_root_mean_squared_error: 1.6262 - 101ms/epoch - 3ms/step\n",
      "Epoch 9/200\n",
      "30/30 - 0s - loss: 0.2816 - mae: 0.3358 - root_mean_squared_error: 0.5053 - val_loss: 0.4773 - val_mae: 0.6925 - val_root_mean_squared_error: 1.6261 - 104ms/epoch - 3ms/step\n",
      "Epoch 10/200\n",
      "30/30 - 0s - loss: 0.1871 - mae: 0.3338 - root_mean_squared_error: 0.5044 - val_loss: 0.4083 - val_mae: 0.6925 - val_root_mean_squared_error: 1.6262 - 93ms/epoch - 3ms/step\n",
      "Epoch 11/200\n",
      "30/30 - 0s - loss: 0.1333 - mae: 0.3349 - root_mean_squared_error: 0.5046 - val_loss: 0.3666 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6245 - 94ms/epoch - 3ms/step\n",
      "Epoch 12/200\n",
      "30/30 - 0s - loss: 0.1014 - mae: 0.3345 - root_mean_squared_error: 0.5041 - val_loss: 0.3392 - val_mae: 0.6939 - val_root_mean_squared_error: 1.6222 - 91ms/epoch - 3ms/step\n",
      "Epoch 13/200\n",
      "30/30 - 0s - loss: 0.0810 - mae: 0.3369 - root_mean_squared_error: 0.5068 - val_loss: 0.3240 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6235 - 95ms/epoch - 3ms/step\n",
      "Epoch 14/200\n",
      "30/30 - 0s - loss: 0.0671 - mae: 0.3372 - root_mean_squared_error: 0.5063 - val_loss: 0.3139 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6242 - 94ms/epoch - 3ms/step\n",
      "Epoch 15/200\n",
      "30/30 - 0s - loss: 0.0582 - mae: 0.3381 - root_mean_squared_error: 0.5067 - val_loss: 0.3086 - val_mae: 0.6926 - val_root_mean_squared_error: 1.6258 - 91ms/epoch - 3ms/step\n",
      "Epoch 16/200\n",
      "30/30 - 0s - loss: 0.0524 - mae: 0.3355 - root_mean_squared_error: 0.5052 - val_loss: 0.3030 - val_mae: 0.6928 - val_root_mean_squared_error: 1.6251 - 100ms/epoch - 3ms/step\n",
      "Epoch 17/200\n",
      "30/30 - 0s - loss: 0.0483 - mae: 0.3372 - root_mean_squared_error: 0.5067 - val_loss: 0.3006 - val_mae: 0.6926 - val_root_mean_squared_error: 1.6259 - 92ms/epoch - 3ms/step\n",
      "Epoch 18/200\n",
      "30/30 - 0s - loss: 0.0456 - mae: 0.3371 - root_mean_squared_error: 0.5063 - val_loss: 0.2991 - val_mae: 0.6924 - val_root_mean_squared_error: 1.6263 - 99ms/epoch - 3ms/step\n",
      "Epoch 19/200\n",
      "30/30 - 0s - loss: 0.0441 - mae: 0.3358 - root_mean_squared_error: 0.5051 - val_loss: 0.2958 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6245 - 102ms/epoch - 3ms/step\n",
      "Epoch 20/200\n",
      "30/30 - 0s - loss: 0.0433 - mae: 0.3378 - root_mean_squared_error: 0.5068 - val_loss: 0.2958 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6248 - 97ms/epoch - 3ms/step\n",
      "Epoch 21/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3365 - root_mean_squared_error: 0.5059 - val_loss: 0.2939 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6232 - 95ms/epoch - 3ms/step\n",
      "Epoch 22/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3389 - root_mean_squared_error: 0.5074 - val_loss: 0.2961 - val_mae: 0.6928 - val_root_mean_squared_error: 1.6252 - 106ms/epoch - 4ms/step\n",
      "Epoch 23/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3379 - root_mean_squared_error: 0.5068 - val_loss: 0.2961 - val_mae: 0.6928 - val_root_mean_squared_error: 1.6252 - 124ms/epoch - 4ms/step\n",
      "Epoch 24/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3356 - root_mean_squared_error: 0.5053 - val_loss: 0.2943 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6236 - 121ms/epoch - 4ms/step\n",
      "Epoch 25/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3384 - root_mean_squared_error: 0.5071 - val_loss: 0.2948 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6241 - 130ms/epoch - 4ms/step\n",
      "Epoch 26/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3381 - root_mean_squared_error: 0.5068 - val_loss: 0.2962 - val_mae: 0.6928 - val_root_mean_squared_error: 1.6253 - 130ms/epoch - 4ms/step\n",
      "Epoch 27/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3375 - root_mean_squared_error: 0.5066 - val_loss: 0.2967 - val_mae: 0.6926 - val_root_mean_squared_error: 1.6258 - 116ms/epoch - 4ms/step\n",
      "Epoch 28/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3377 - root_mean_squared_error: 0.5065 - val_loss: 0.2955 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6247 - 109ms/epoch - 4ms/step\n",
      "Epoch 29/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3382 - root_mean_squared_error: 0.5070 - val_loss: 0.2950 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6243 - 97ms/epoch - 3ms/step\n",
      "Epoch 30/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3399 - root_mean_squared_error: 0.5082 - val_loss: 0.2973 - val_mae: 0.6924 - val_root_mean_squared_error: 1.6262 - 93ms/epoch - 3ms/step\n",
      "Epoch 31/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3359 - root_mean_squared_error: 0.5054 - val_loss: 0.2944 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6238 - 101ms/epoch - 3ms/step\n",
      "Epoch 32/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3384 - root_mean_squared_error: 0.5073 - val_loss: 0.2949 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6242 - 93ms/epoch - 3ms/step\n",
      "Epoch 33/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3379 - root_mean_squared_error: 0.5069 - val_loss: 0.2949 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6242 - 119ms/epoch - 4ms/step\n",
      "Epoch 34/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3390 - root_mean_squared_error: 0.5074 - val_loss: 0.2962 - val_mae: 0.6928 - val_root_mean_squared_error: 1.6253 - 125ms/epoch - 4ms/step\n",
      "Epoch 35/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3363 - root_mean_squared_error: 0.5055 - val_loss: 0.2944 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6238 - 99ms/epoch - 3ms/step\n",
      "Epoch 36/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3384 - root_mean_squared_error: 0.5071 - val_loss: 0.2943 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6237 - 93ms/epoch - 3ms/step\n",
      "Epoch 37/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3397 - root_mean_squared_error: 0.5080 - val_loss: 0.2965 - val_mae: 0.6927 - val_root_mean_squared_error: 1.6255 - 120ms/epoch - 4ms/step\n",
      "Epoch 38/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3364 - root_mean_squared_error: 0.5057 - val_loss: 0.2948 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6241 - 123ms/epoch - 4ms/step\n",
      "Epoch 39/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3384 - root_mean_squared_error: 0.5072 - val_loss: 0.2947 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6240 - 123ms/epoch - 4ms/step\n",
      "Epoch 40/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3370 - root_mean_squared_error: 0.5062 - val_loss: 0.2933 - val_mae: 0.6937 - val_root_mean_squared_error: 1.6227 - 123ms/epoch - 4ms/step\n",
      "Epoch 41/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3380 - root_mean_squared_error: 0.5068 - val_loss: 0.2938 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6232 - 94ms/epoch - 3ms/step\n",
      "Epoch 42/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3383 - root_mean_squared_error: 0.5070 - val_loss: 0.2942 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6236 - 107ms/epoch - 4ms/step\n",
      "Epoch 43/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3376 - root_mean_squared_error: 0.5066 - val_loss: 0.2933 - val_mae: 0.6937 - val_root_mean_squared_error: 1.6227 - 126ms/epoch - 4ms/step\n",
      "Epoch 44/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3376 - root_mean_squared_error: 0.5066 - val_loss: 0.2941 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6235 - 121ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3390 - root_mean_squared_error: 0.5077 - val_loss: 0.2956 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6248 - 115ms/epoch - 4ms/step\n",
      "Epoch 46/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3370 - root_mean_squared_error: 0.5060 - val_loss: 0.2948 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6241 - 117ms/epoch - 4ms/step\n",
      "Epoch 47/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3372 - root_mean_squared_error: 0.5063 - val_loss: 0.2936 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6230 - 101ms/epoch - 3ms/step\n",
      "Epoch 48/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3398 - root_mean_squared_error: 0.5081 - val_loss: 0.2952 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6244 - 105ms/epoch - 4ms/step\n",
      "Epoch 49/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3383 - root_mean_squared_error: 0.5070 - val_loss: 0.2951 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6243 - 110ms/epoch - 4ms/step\n",
      "Epoch 50/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3398 - root_mean_squared_error: 0.5081 - val_loss: 0.2957 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6249 - 94ms/epoch - 3ms/step\n",
      "Epoch 51/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3382 - root_mean_squared_error: 0.5069 - val_loss: 0.2965 - val_mae: 0.6927 - val_root_mean_squared_error: 1.6256 - 98ms/epoch - 3ms/step\n",
      "Epoch 52/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3373 - root_mean_squared_error: 0.5064 - val_loss: 0.2957 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6248 - 115ms/epoch - 4ms/step\n",
      "Epoch 53/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3374 - root_mean_squared_error: 0.5063 - val_loss: 0.2952 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6244 - 102ms/epoch - 3ms/step\n",
      "Epoch 54/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3374 - root_mean_squared_error: 0.5064 - val_loss: 0.2943 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6237 - 89ms/epoch - 3ms/step\n",
      "Epoch 55/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3391 - root_mean_squared_error: 0.5075 - val_loss: 0.2948 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6241 - 103ms/epoch - 3ms/step\n",
      "Epoch 56/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3395 - root_mean_squared_error: 0.5078 - val_loss: 0.2950 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6243 - 92ms/epoch - 3ms/step\n",
      "Epoch 57/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3384 - root_mean_squared_error: 0.5072 - val_loss: 0.2953 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6245 - 99ms/epoch - 3ms/step\n",
      "Epoch 58/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3379 - root_mean_squared_error: 0.5067 - val_loss: 0.2954 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6246 - 115ms/epoch - 4ms/step\n",
      "Epoch 59/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3387 - root_mean_squared_error: 0.5071 - val_loss: 0.2967 - val_mae: 0.6926 - val_root_mean_squared_error: 1.6258 - 96ms/epoch - 3ms/step\n",
      "Epoch 60/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3356 - root_mean_squared_error: 0.5047 - val_loss: 0.2940 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6234 - 94ms/epoch - 3ms/step\n",
      "Epoch 61/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3385 - root_mean_squared_error: 0.5071 - val_loss: 0.2943 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6237 - 102ms/epoch - 3ms/step\n",
      "Epoch 62/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3393 - root_mean_squared_error: 0.5077 - val_loss: 0.2956 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6248 - 92ms/epoch - 3ms/step\n",
      "Epoch 63/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3382 - root_mean_squared_error: 0.5069 - val_loss: 0.2958 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6250 - 95ms/epoch - 3ms/step\n",
      "Epoch 64/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3360 - root_mean_squared_error: 0.5054 - val_loss: 0.2942 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6235 - 88ms/epoch - 3ms/step\n",
      "Epoch 65/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3367 - root_mean_squared_error: 0.5057 - val_loss: 0.2933 - val_mae: 0.6936 - val_root_mean_squared_error: 1.6228 - 90ms/epoch - 3ms/step\n",
      "Epoch 66/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3390 - root_mean_squared_error: 0.5075 - val_loss: 0.2939 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6233 - 97ms/epoch - 3ms/step\n",
      "Epoch 67/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3385 - root_mean_squared_error: 0.5072 - val_loss: 0.2951 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6244 - 91ms/epoch - 3ms/step\n",
      "Epoch 68/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3380 - root_mean_squared_error: 0.5067 - val_loss: 0.2953 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6245 - 93ms/epoch - 3ms/step\n",
      "Epoch 69/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3386 - root_mean_squared_error: 0.5071 - val_loss: 0.2959 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6250 - 87ms/epoch - 3ms/step\n",
      "Epoch 70/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3372 - root_mean_squared_error: 0.5063 - val_loss: 0.2956 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6248 - 96ms/epoch - 3ms/step\n",
      "Epoch 71/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3376 - root_mean_squared_error: 0.5065 - val_loss: 0.2952 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6244 - 89ms/epoch - 3ms/step\n",
      "Epoch 72/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3375 - root_mean_squared_error: 0.5065 - val_loss: 0.2941 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6235 - 105ms/epoch - 4ms/step\n",
      "Epoch 73/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3388 - root_mean_squared_error: 0.5074 - val_loss: 0.2949 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6242 - 117ms/epoch - 4ms/step\n",
      "Epoch 74/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3389 - root_mean_squared_error: 0.5075 - val_loss: 0.2950 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6242 - 99ms/epoch - 3ms/step\n",
      "Epoch 75/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3390 - root_mean_squared_error: 0.5076 - val_loss: 0.2958 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6250 - 94ms/epoch - 3ms/step\n",
      "Epoch 76/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3362 - root_mean_squared_error: 0.5057 - val_loss: 0.2942 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6236 - 89ms/epoch - 3ms/step\n",
      "Epoch 77/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3379 - root_mean_squared_error: 0.5067 - val_loss: 0.2928 - val_mae: 0.6939 - val_root_mean_squared_error: 1.6224 - 115ms/epoch - 4ms/step\n",
      "Epoch 78/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3406 - root_mean_squared_error: 0.5088 - val_loss: 0.2949 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6242 - 103ms/epoch - 3ms/step\n",
      "Epoch 79/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3384 - root_mean_squared_error: 0.5072 - val_loss: 0.2954 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6246 - 97ms/epoch - 3ms/step\n",
      "Epoch 80/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3380 - root_mean_squared_error: 0.5068 - val_loss: 0.2953 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6245 - 96ms/epoch - 3ms/step\n",
      "Epoch 81/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3382 - root_mean_squared_error: 0.5069 - val_loss: 0.2941 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6235 - 108ms/epoch - 4ms/step\n",
      "Epoch 82/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3399 - root_mean_squared_error: 0.5080 - val_loss: 0.2951 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6244 - 102ms/epoch - 3ms/step\n",
      "Epoch 83/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3371 - root_mean_squared_error: 0.5062 - val_loss: 0.2944 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6237 - 89ms/epoch - 3ms/step\n",
      "Epoch 84/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3388 - root_mean_squared_error: 0.5075 - val_loss: 0.2945 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6238 - 107ms/epoch - 4ms/step\n",
      "Epoch 85/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3386 - root_mean_squared_error: 0.5071 - val_loss: 0.2952 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6245 - 114ms/epoch - 4ms/step\n",
      "Epoch 86/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3382 - root_mean_squared_error: 0.5069 - val_loss: 0.2945 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6238 - 100ms/epoch - 3ms/step\n",
      "Epoch 87/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3390 - root_mean_squared_error: 0.5075 - val_loss: 0.2955 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6247 - 93ms/epoch - 3ms/step\n",
      "Epoch 88/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3371 - root_mean_squared_error: 0.5064 - val_loss: 0.2948 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6241 - 88ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3377 - root_mean_squared_error: 0.5066 - val_loss: 0.2950 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6242 - 101ms/epoch - 3ms/step\n",
      "Epoch 90/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3371 - root_mean_squared_error: 0.5062 - val_loss: 0.2936 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6230 - 122ms/epoch - 4ms/step\n",
      "Epoch 91/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3377 - root_mean_squared_error: 0.5065 - val_loss: 0.2936 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6230 - 127ms/epoch - 4ms/step\n",
      "Epoch 92/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3379 - root_mean_squared_error: 0.5066 - val_loss: 0.2934 - val_mae: 0.6936 - val_root_mean_squared_error: 1.6229 - 125ms/epoch - 4ms/step\n",
      "Epoch 93/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3391 - root_mean_squared_error: 0.5073 - val_loss: 0.2937 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6231 - 134ms/epoch - 4ms/step\n",
      "Epoch 94/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3391 - root_mean_squared_error: 0.5075 - val_loss: 0.2951 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6244 - 104ms/epoch - 3ms/step\n",
      "Epoch 95/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3379 - root_mean_squared_error: 0.5067 - val_loss: 0.2941 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6235 - 91ms/epoch - 3ms/step\n",
      "Epoch 96/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3379 - root_mean_squared_error: 0.5064 - val_loss: 0.2941 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6235 - 89ms/epoch - 3ms/step\n",
      "Epoch 97/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3377 - root_mean_squared_error: 0.5068 - val_loss: 0.2944 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6237 - 107ms/epoch - 4ms/step\n",
      "Epoch 98/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3380 - root_mean_squared_error: 0.5070 - val_loss: 0.2957 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6249 - 123ms/epoch - 4ms/step\n",
      "Epoch 99/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3367 - root_mean_squared_error: 0.5060 - val_loss: 0.2944 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6238 - 109ms/epoch - 4ms/step\n",
      "Epoch 100/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3378 - root_mean_squared_error: 0.5064 - val_loss: 0.2933 - val_mae: 0.6936 - val_root_mean_squared_error: 1.6228 - 102ms/epoch - 3ms/step\n",
      "Epoch 101/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3395 - root_mean_squared_error: 0.5079 - val_loss: 0.2941 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6234 - 89ms/epoch - 3ms/step\n",
      "Epoch 102/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3384 - root_mean_squared_error: 0.5069 - val_loss: 0.2941 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6234 - 105ms/epoch - 3ms/step\n",
      "Epoch 103/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3372 - root_mean_squared_error: 0.5062 - val_loss: 0.2933 - val_mae: 0.6936 - val_root_mean_squared_error: 1.6228 - 102ms/epoch - 3ms/step\n",
      "Epoch 104/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3397 - root_mean_squared_error: 0.5082 - val_loss: 0.2943 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6237 - 88ms/epoch - 3ms/step\n",
      "Epoch 105/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3380 - root_mean_squared_error: 0.5068 - val_loss: 0.2945 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6238 - 85ms/epoch - 3ms/step\n",
      "Epoch 106/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3379 - root_mean_squared_error: 0.5066 - val_loss: 0.2947 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6240 - 95ms/epoch - 3ms/step\n",
      "Epoch 107/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3374 - root_mean_squared_error: 0.5065 - val_loss: 0.2940 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6234 - 98ms/epoch - 3ms/step\n",
      "Epoch 108/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3382 - root_mean_squared_error: 0.5071 - val_loss: 0.2932 - val_mae: 0.6937 - val_root_mean_squared_error: 1.6227 - 117ms/epoch - 4ms/step\n",
      "Epoch 109/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3388 - root_mean_squared_error: 0.5074 - val_loss: 0.2929 - val_mae: 0.6938 - val_root_mean_squared_error: 1.6225 - 120ms/epoch - 4ms/step\n",
      "Epoch 110/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3404 - root_mean_squared_error: 0.5084 - val_loss: 0.2944 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6237 - 103ms/epoch - 3ms/step\n",
      "Epoch 111/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3378 - root_mean_squared_error: 0.5066 - val_loss: 0.2945 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6238 - 96ms/epoch - 3ms/step\n",
      "Epoch 112/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3394 - root_mean_squared_error: 0.5077 - val_loss: 0.2951 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6243 - 97ms/epoch - 3ms/step\n",
      "Epoch 113/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3384 - root_mean_squared_error: 0.5073 - val_loss: 0.2948 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6241 - 93ms/epoch - 3ms/step\n",
      "Epoch 114/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3369 - root_mean_squared_error: 0.5062 - val_loss: 0.2939 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6233 - 98ms/epoch - 3ms/step\n",
      "Epoch 115/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3390 - root_mean_squared_error: 0.5074 - val_loss: 0.2939 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6233 - 93ms/epoch - 3ms/step\n",
      "Epoch 116/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3389 - root_mean_squared_error: 0.5075 - val_loss: 0.2941 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6235 - 99ms/epoch - 3ms/step\n",
      "Epoch 117/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3368 - root_mean_squared_error: 0.5061 - val_loss: 0.2935 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6230 - 91ms/epoch - 3ms/step\n",
      "Epoch 118/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3391 - root_mean_squared_error: 0.5077 - val_loss: 0.2938 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6232 - 91ms/epoch - 3ms/step\n",
      "Epoch 119/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3380 - root_mean_squared_error: 0.5067 - val_loss: 0.2942 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6236 - 98ms/epoch - 3ms/step\n",
      "Epoch 120/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3377 - root_mean_squared_error: 0.5068 - val_loss: 0.2935 - val_mae: 0.6936 - val_root_mean_squared_error: 1.6229 - 96ms/epoch - 3ms/step\n",
      "Epoch 121/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3383 - root_mean_squared_error: 0.5069 - val_loss: 0.2937 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6231 - 101ms/epoch - 3ms/step\n",
      "Epoch 122/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3383 - root_mean_squared_error: 0.5070 - val_loss: 0.2931 - val_mae: 0.6938 - val_root_mean_squared_error: 1.6226 - 95ms/epoch - 3ms/step\n",
      "Epoch 123/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3399 - root_mean_squared_error: 0.5082 - val_loss: 0.2946 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6239 - 93ms/epoch - 3ms/step\n",
      "Epoch 124/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3397 - root_mean_squared_error: 0.5080 - val_loss: 0.2952 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6245 - 94ms/epoch - 3ms/step\n",
      "Epoch 125/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3384 - root_mean_squared_error: 0.5071 - val_loss: 0.2948 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6241 - 89ms/epoch - 3ms/step\n",
      "Epoch 126/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3373 - root_mean_squared_error: 0.5063 - val_loss: 0.2941 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6235 - 94ms/epoch - 3ms/step\n",
      "Epoch 127/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3381 - root_mean_squared_error: 0.5068 - val_loss: 0.2939 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6233 - 111ms/epoch - 4ms/step\n",
      "Epoch 128/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3395 - root_mean_squared_error: 0.5079 - val_loss: 0.2945 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6239 - 97ms/epoch - 3ms/step\n",
      "Epoch 129/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3387 - root_mean_squared_error: 0.5074 - val_loss: 0.2954 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6247 - 99ms/epoch - 3ms/step\n",
      "Epoch 130/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3377 - root_mean_squared_error: 0.5065 - val_loss: 0.2947 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6240 - 96ms/epoch - 3ms/step\n",
      "Epoch 131/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3384 - root_mean_squared_error: 0.5071 - val_loss: 0.2952 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6244 - 98ms/epoch - 3ms/step\n",
      "Epoch 132/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3379 - root_mean_squared_error: 0.5067 - val_loss: 0.2945 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6238 - 101ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3388 - root_mean_squared_error: 0.5074 - val_loss: 0.2949 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6242 - 101ms/epoch - 3ms/step\n",
      "Epoch 134/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3374 - root_mean_squared_error: 0.5063 - val_loss: 0.2941 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6235 - 100ms/epoch - 3ms/step\n",
      "Epoch 135/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3388 - root_mean_squared_error: 0.5078 - val_loss: 0.2958 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6250 - 91ms/epoch - 3ms/step\n",
      "Epoch 136/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3377 - root_mean_squared_error: 0.5065 - val_loss: 0.2959 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6250 - 91ms/epoch - 3ms/step\n",
      "Epoch 137/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3386 - root_mean_squared_error: 0.5072 - val_loss: 0.2977 - val_mae: 0.6923 - val_root_mean_squared_error: 1.6266 - 94ms/epoch - 3ms/step\n",
      "Epoch 138/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3349 - root_mean_squared_error: 0.5046 - val_loss: 0.2953 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6245 - 91ms/epoch - 3ms/step\n",
      "Epoch 139/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3368 - root_mean_squared_error: 0.5060 - val_loss: 0.2932 - val_mae: 0.6937 - val_root_mean_squared_error: 1.6226 - 98ms/epoch - 3ms/step\n",
      "Epoch 140/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3397 - root_mean_squared_error: 0.5081 - val_loss: 0.2937 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6231 - 97ms/epoch - 3ms/step\n",
      "Epoch 141/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3380 - root_mean_squared_error: 0.5068 - val_loss: 0.2938 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6232 - 91ms/epoch - 3ms/step\n",
      "Epoch 142/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3386 - root_mean_squared_error: 0.5071 - val_loss: 0.2937 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6232 - 90ms/epoch - 3ms/step\n",
      "Epoch 143/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3400 - root_mean_squared_error: 0.5081 - val_loss: 0.2953 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6245 - 94ms/epoch - 3ms/step\n",
      "Epoch 144/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3386 - root_mean_squared_error: 0.5074 - val_loss: 0.2955 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6247 - 96ms/epoch - 3ms/step\n",
      "Epoch 145/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3356 - root_mean_squared_error: 0.5053 - val_loss: 0.2944 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6237 - 134ms/epoch - 4ms/step\n",
      "Epoch 146/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3383 - root_mean_squared_error: 0.5070 - val_loss: 0.2946 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6239 - 153ms/epoch - 5ms/step\n",
      "Epoch 147/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3373 - root_mean_squared_error: 0.5065 - val_loss: 0.2942 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6236 - 168ms/epoch - 6ms/step\n",
      "Epoch 148/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3401 - root_mean_squared_error: 0.5082 - val_loss: 0.2966 - val_mae: 0.6926 - val_root_mean_squared_error: 1.6257 - 114ms/epoch - 4ms/step\n",
      "Epoch 149/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3371 - root_mean_squared_error: 0.5062 - val_loss: 0.2954 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6246 - 124ms/epoch - 4ms/step\n",
      "Epoch 150/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3383 - root_mean_squared_error: 0.5070 - val_loss: 0.2957 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6249 - 125ms/epoch - 4ms/step\n",
      "Epoch 151/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3366 - root_mean_squared_error: 0.5058 - val_loss: 0.2945 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6238 - 122ms/epoch - 4ms/step\n",
      "Epoch 152/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3392 - root_mean_squared_error: 0.5077 - val_loss: 0.2956 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6248 - 114ms/epoch - 4ms/step\n",
      "Epoch 153/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3359 - root_mean_squared_error: 0.5053 - val_loss: 0.2942 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6235 - 94ms/epoch - 3ms/step\n",
      "Epoch 154/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3390 - root_mean_squared_error: 0.5074 - val_loss: 0.2950 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6243 - 90ms/epoch - 3ms/step\n",
      "Epoch 155/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3386 - root_mean_squared_error: 0.5073 - val_loss: 0.2950 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6243 - 93ms/epoch - 3ms/step\n",
      "Epoch 156/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3386 - root_mean_squared_error: 0.5072 - val_loss: 0.2957 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6248 - 94ms/epoch - 3ms/step\n",
      "Epoch 157/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3386 - root_mean_squared_error: 0.5069 - val_loss: 0.2955 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6247 - 98ms/epoch - 3ms/step\n",
      "Epoch 158/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3375 - root_mean_squared_error: 0.5064 - val_loss: 0.2942 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6236 - 92ms/epoch - 3ms/step\n",
      "Epoch 159/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3380 - root_mean_squared_error: 0.5070 - val_loss: 0.2939 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6233 - 103ms/epoch - 3ms/step\n",
      "Epoch 160/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3388 - root_mean_squared_error: 0.5074 - val_loss: 0.2938 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6233 - 92ms/epoch - 3ms/step\n",
      "Epoch 161/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3393 - root_mean_squared_error: 0.5076 - val_loss: 0.2944 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6237 - 100ms/epoch - 3ms/step\n",
      "Epoch 162/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3392 - root_mean_squared_error: 0.5076 - val_loss: 0.2951 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6243 - 103ms/epoch - 3ms/step\n",
      "Epoch 163/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3377 - root_mean_squared_error: 0.5064 - val_loss: 0.2947 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6240 - 72ms/epoch - 2ms/step\n",
      "Epoch 164/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3392 - root_mean_squared_error: 0.5077 - val_loss: 0.2959 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6250 - 63ms/epoch - 2ms/step\n",
      "Epoch 165/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3384 - root_mean_squared_error: 0.5071 - val_loss: 0.2956 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6248 - 109ms/epoch - 4ms/step\n",
      "Epoch 166/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3375 - root_mean_squared_error: 0.5066 - val_loss: 0.2947 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6240 - 106ms/epoch - 4ms/step\n",
      "Epoch 167/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3372 - root_mean_squared_error: 0.5060 - val_loss: 0.2942 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6236 - 90ms/epoch - 3ms/step\n",
      "Epoch 168/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3385 - root_mean_squared_error: 0.5073 - val_loss: 0.2942 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6236 - 93ms/epoch - 3ms/step\n",
      "Epoch 169/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3384 - root_mean_squared_error: 0.5071 - val_loss: 0.2941 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6235 - 93ms/epoch - 3ms/step\n",
      "Epoch 170/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3397 - root_mean_squared_error: 0.5081 - val_loss: 0.2953 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6245 - 110ms/epoch - 4ms/step\n",
      "Epoch 171/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3382 - root_mean_squared_error: 0.5070 - val_loss: 0.2953 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6246 - 112ms/epoch - 4ms/step\n",
      "Epoch 172/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3380 - root_mean_squared_error: 0.5070 - val_loss: 0.2949 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6242 - 92ms/epoch - 3ms/step\n",
      "Epoch 173/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3386 - root_mean_squared_error: 0.5073 - val_loss: 0.2951 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6243 - 99ms/epoch - 3ms/step\n",
      "Epoch 174/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3383 - root_mean_squared_error: 0.5072 - val_loss: 0.2955 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6247 - 88ms/epoch - 3ms/step\n",
      "Epoch 175/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3381 - root_mean_squared_error: 0.5066 - val_loss: 0.2950 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6243 - 94ms/epoch - 3ms/step\n",
      "Epoch 176/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3381 - root_mean_squared_error: 0.5066 - val_loss: 0.2942 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6235 - 92ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3382 - root_mean_squared_error: 0.5069 - val_loss: 0.2937 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6231 - 98ms/epoch - 3ms/step\n",
      "Epoch 178/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3391 - root_mean_squared_error: 0.5076 - val_loss: 0.2942 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6235 - 98ms/epoch - 3ms/step\n",
      "Epoch 179/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3388 - root_mean_squared_error: 0.5072 - val_loss: 0.2946 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6239 - 97ms/epoch - 3ms/step\n",
      "Epoch 180/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3377 - root_mean_squared_error: 0.5065 - val_loss: 0.2937 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6231 - 95ms/epoch - 3ms/step\n",
      "Epoch 181/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3388 - root_mean_squared_error: 0.5076 - val_loss: 0.2944 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6237 - 96ms/epoch - 3ms/step\n",
      "Epoch 182/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3373 - root_mean_squared_error: 0.5065 - val_loss: 0.2940 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6234 - 94ms/epoch - 3ms/step\n",
      "Epoch 183/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3385 - root_mean_squared_error: 0.5071 - val_loss: 0.2944 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6237 - 105ms/epoch - 4ms/step\n",
      "Epoch 184/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3387 - root_mean_squared_error: 0.5075 - val_loss: 0.2953 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6245 - 91ms/epoch - 3ms/step\n",
      "Epoch 185/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3378 - root_mean_squared_error: 0.5067 - val_loss: 0.2951 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6244 - 88ms/epoch - 3ms/step\n",
      "Epoch 186/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3378 - root_mean_squared_error: 0.5067 - val_loss: 0.2947 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6240 - 88ms/epoch - 3ms/step\n",
      "Epoch 187/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3378 - root_mean_squared_error: 0.5065 - val_loss: 0.2938 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6232 - 101ms/epoch - 3ms/step\n",
      "Epoch 188/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3374 - root_mean_squared_error: 0.5066 - val_loss: 0.2934 - val_mae: 0.6936 - val_root_mean_squared_error: 1.6228 - 98ms/epoch - 3ms/step\n",
      "Epoch 189/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3394 - root_mean_squared_error: 0.5077 - val_loss: 0.2934 - val_mae: 0.6936 - val_root_mean_squared_error: 1.6229 - 92ms/epoch - 3ms/step\n",
      "Epoch 190/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3390 - root_mean_squared_error: 0.5076 - val_loss: 0.2937 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6231 - 91ms/epoch - 3ms/step\n",
      "Epoch 191/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3381 - root_mean_squared_error: 0.5069 - val_loss: 0.2936 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6231 - 92ms/epoch - 3ms/step\n",
      "Epoch 192/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3388 - root_mean_squared_error: 0.5073 - val_loss: 0.2930 - val_mae: 0.6938 - val_root_mean_squared_error: 1.6225 - 90ms/epoch - 3ms/step\n",
      "Epoch 193/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3409 - root_mean_squared_error: 0.5090 - val_loss: 0.2938 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6232 - 118ms/epoch - 4ms/step\n",
      "Epoch 194/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3380 - root_mean_squared_error: 0.5068 - val_loss: 0.2930 - val_mae: 0.6938 - val_root_mean_squared_error: 1.6225 - 103ms/epoch - 3ms/step\n",
      "Epoch 195/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3408 - root_mean_squared_error: 0.5084 - val_loss: 0.2942 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6235 - 95ms/epoch - 3ms/step\n",
      "Epoch 196/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3387 - root_mean_squared_error: 0.5074 - val_loss: 0.2941 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6234 - 96ms/epoch - 3ms/step\n",
      "Epoch 197/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3402 - root_mean_squared_error: 0.5082 - val_loss: 0.2961 - val_mae: 0.6928 - val_root_mean_squared_error: 1.6252 - 98ms/epoch - 3ms/step\n",
      "Epoch 198/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3373 - root_mean_squared_error: 0.5061 - val_loss: 0.2948 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6241 - 111ms/epoch - 4ms/step\n",
      "Epoch 199/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3388 - root_mean_squared_error: 0.5074 - val_loss: 0.2957 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6248 - 105ms/epoch - 3ms/step\n",
      "Epoch 200/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3374 - root_mean_squared_error: 0.5065 - val_loss: 0.2960 - val_mae: 0.6928 - val_root_mean_squared_error: 1.6252 - 110ms/epoch - 4ms/step\n",
      "Epoch 1/200\n",
      "30/30 - 1s - loss: 1.6230 - mae: 0.8518 - root_mean_squared_error: 1.0577 - val_loss: 1.4339 - val_mae: 1.1359 - val_root_mean_squared_error: 1.8680 - 503ms/epoch - 17ms/step\n",
      "Epoch 2/200\n",
      "30/30 - 0s - loss: 0.8386 - mae: 0.4820 - root_mean_squared_error: 0.6323 - val_loss: 0.8759 - val_mae: 0.8342 - val_root_mean_squared_error: 1.7125 - 57ms/epoch - 2ms/step\n",
      "Epoch 3/200\n",
      "30/30 - 0s - loss: 0.4270 - mae: 0.3519 - root_mean_squared_error: 0.5114 - val_loss: 0.5483 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6347 - 90ms/epoch - 3ms/step\n",
      "Epoch 4/200\n",
      "30/30 - 0s - loss: 0.2142 - mae: 0.3369 - root_mean_squared_error: 0.5052 - val_loss: 0.4057 - val_mae: 0.6893 - val_root_mean_squared_error: 1.6253 - 112ms/epoch - 4ms/step\n",
      "Epoch 5/200\n",
      "30/30 - 0s - loss: 0.1206 - mae: 0.3364 - root_mean_squared_error: 0.5058 - val_loss: 0.3489 - val_mae: 0.6900 - val_root_mean_squared_error: 1.6282 - 63ms/epoch - 2ms/step\n",
      "Epoch 6/200\n",
      "30/30 - 0s - loss: 0.0776 - mae: 0.3324 - root_mean_squared_error: 0.5022 - val_loss: 0.3144 - val_mae: 0.6923 - val_root_mean_squared_error: 1.6218 - 99ms/epoch - 3ms/step\n",
      "Epoch 7/200\n",
      "30/30 - 0s - loss: 0.0602 - mae: 0.3433 - root_mean_squared_error: 0.5117 - val_loss: 0.3112 - val_mae: 0.6909 - val_root_mean_squared_error: 1.6292 - 91ms/epoch - 3ms/step\n",
      "Epoch 8/200\n",
      "30/30 - 0s - loss: 0.0503 - mae: 0.3367 - root_mean_squared_error: 0.5059 - val_loss: 0.3006 - val_mae: 0.6924 - val_root_mean_squared_error: 1.6250 - 111ms/epoch - 4ms/step\n",
      "Epoch 9/200\n",
      "30/30 - 0s - loss: 0.0467 - mae: 0.3387 - root_mean_squared_error: 0.5071 - val_loss: 0.2942 - val_mae: 0.6942 - val_root_mean_squared_error: 1.6215 - 126ms/epoch - 4ms/step\n",
      "Epoch 10/200\n",
      "30/30 - 0s - loss: 0.0448 - mae: 0.3365 - root_mean_squared_error: 0.5060 - val_loss: 0.2961 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6243 - 123ms/epoch - 4ms/step\n",
      "Epoch 11/200\n",
      "30/30 - 0s - loss: 0.0441 - mae: 0.3451 - root_mean_squared_error: 0.5116 - val_loss: 0.2945 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6234 - 90ms/epoch - 3ms/step\n",
      "Epoch 12/200\n",
      "30/30 - 0s - loss: 0.0435 - mae: 0.3350 - root_mean_squared_error: 0.5045 - val_loss: 0.2961 - val_mae: 0.6928 - val_root_mean_squared_error: 1.6250 - 101ms/epoch - 3ms/step\n",
      "Epoch 13/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3390 - root_mean_squared_error: 0.5072 - val_loss: 0.2933 - val_mae: 0.6937 - val_root_mean_squared_error: 1.6227 - 88ms/epoch - 3ms/step\n",
      "Epoch 14/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3380 - root_mean_squared_error: 0.5070 - val_loss: 0.2946 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6239 - 87ms/epoch - 3ms/step\n",
      "Epoch 15/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3409 - root_mean_squared_error: 0.5086 - val_loss: 0.2947 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6240 - 88ms/epoch - 3ms/step\n",
      "Epoch 16/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3363 - root_mean_squared_error: 0.5054 - val_loss: 0.2951 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6244 - 96ms/epoch - 3ms/step\n",
      "Epoch 17/200\n",
      "30/30 - 0s - loss: 0.0431 - mae: 0.3415 - root_mean_squared_error: 0.5091 - val_loss: 0.2922 - val_mae: 0.6942 - val_root_mean_squared_error: 1.6218 - 87ms/epoch - 3ms/step\n",
      "Epoch 18/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3378 - root_mean_squared_error: 0.5062 - val_loss: 0.2957 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6249 - 102ms/epoch - 3ms/step\n",
      "Epoch 19/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3390 - root_mean_squared_error: 0.5081 - val_loss: 0.2945 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6238 - 109ms/epoch - 4ms/step\n",
      "Epoch 20/200\n",
      "30/30 - 0s - loss: 0.0431 - mae: 0.3387 - root_mean_squared_error: 0.5080 - val_loss: 0.2989 - val_mae: 0.6920 - val_root_mean_squared_error: 1.6276 - 81ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3376 - root_mean_squared_error: 0.5064 - val_loss: 0.2939 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6233 - 87ms/epoch - 3ms/step\n",
      "Epoch 22/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3383 - root_mean_squared_error: 0.5071 - val_loss: 0.2948 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6241 - 91ms/epoch - 3ms/step\n",
      "Epoch 23/200\n",
      "30/30 - 0s - loss: 0.0427 - mae: 0.3377 - root_mean_squared_error: 0.5067 - val_loss: 0.2948 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6241 - 76ms/epoch - 3ms/step\n",
      "Epoch 24/200\n",
      "30/30 - 0s - loss: 0.0427 - mae: 0.3388 - root_mean_squared_error: 0.5071 - val_loss: 0.2947 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6240 - 80ms/epoch - 3ms/step\n",
      "Epoch 25/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3407 - root_mean_squared_error: 0.5080 - val_loss: 0.2918 - val_mae: 0.6944 - val_root_mean_squared_error: 1.6215 - 85ms/epoch - 3ms/step\n",
      "Epoch 26/200\n",
      "30/30 - 0s - loss: 0.0434 - mae: 0.3400 - root_mean_squared_error: 0.5084 - val_loss: 0.2970 - val_mae: 0.6925 - val_root_mean_squared_error: 1.6260 - 82ms/epoch - 3ms/step\n",
      "Epoch 27/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3382 - root_mean_squared_error: 0.5071 - val_loss: 0.2944 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6237 - 92ms/epoch - 3ms/step\n",
      "Epoch 28/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3392 - root_mean_squared_error: 0.5072 - val_loss: 0.2952 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6244 - 89ms/epoch - 3ms/step\n",
      "Epoch 29/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3369 - root_mean_squared_error: 0.5063 - val_loss: 0.2946 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6239 - 81ms/epoch - 3ms/step\n",
      "Epoch 30/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3403 - root_mean_squared_error: 0.5088 - val_loss: 0.2948 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6241 - 88ms/epoch - 3ms/step\n",
      "Epoch 31/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3370 - root_mean_squared_error: 0.5063 - val_loss: 0.2923 - val_mae: 0.6941 - val_root_mean_squared_error: 1.6219 - 85ms/epoch - 3ms/step\n",
      "Epoch 32/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3375 - root_mean_squared_error: 0.5064 - val_loss: 0.2941 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6235 - 95ms/epoch - 3ms/step\n",
      "Epoch 33/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3395 - root_mean_squared_error: 0.5079 - val_loss: 0.2929 - val_mae: 0.6939 - val_root_mean_squared_error: 1.6224 - 111ms/epoch - 4ms/step\n",
      "Epoch 34/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3389 - root_mean_squared_error: 0.5074 - val_loss: 0.2945 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6239 - 97ms/epoch - 3ms/step\n",
      "Epoch 35/200\n",
      "30/30 - 0s - loss: 0.0432 - mae: 0.3423 - root_mean_squared_error: 0.5102 - val_loss: 0.2941 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6235 - 86ms/epoch - 3ms/step\n",
      "Epoch 36/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3376 - root_mean_squared_error: 0.5061 - val_loss: 0.2963 - val_mae: 0.6927 - val_root_mean_squared_error: 1.6254 - 97ms/epoch - 3ms/step\n",
      "Epoch 37/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3361 - root_mean_squared_error: 0.5054 - val_loss: 0.2955 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6247 - 87ms/epoch - 3ms/step\n",
      "Epoch 38/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3421 - root_mean_squared_error: 0.5094 - val_loss: 0.2913 - val_mae: 0.6946 - val_root_mean_squared_error: 1.6210 - 82ms/epoch - 3ms/step\n",
      "Epoch 39/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3389 - root_mean_squared_error: 0.5074 - val_loss: 0.2944 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6238 - 80ms/epoch - 3ms/step\n",
      "Epoch 40/200\n",
      "30/30 - 0s - loss: 0.0427 - mae: 0.3358 - root_mean_squared_error: 0.5052 - val_loss: 0.2951 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6244 - 91ms/epoch - 3ms/step\n",
      "Epoch 41/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3406 - root_mean_squared_error: 0.5088 - val_loss: 0.2941 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6235 - 92ms/epoch - 3ms/step\n",
      "Epoch 42/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3366 - root_mean_squared_error: 0.5056 - val_loss: 0.2950 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6242 - 104ms/epoch - 3ms/step\n",
      "Epoch 43/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3387 - root_mean_squared_error: 0.5078 - val_loss: 0.2925 - val_mae: 0.6940 - val_root_mean_squared_error: 1.6221 - 90ms/epoch - 3ms/step\n",
      "Epoch 44/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3395 - root_mean_squared_error: 0.5078 - val_loss: 0.2933 - val_mae: 0.6936 - val_root_mean_squared_error: 1.6228 - 89ms/epoch - 3ms/step\n",
      "Epoch 45/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3386 - root_mean_squared_error: 0.5070 - val_loss: 0.2945 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6239 - 93ms/epoch - 3ms/step\n",
      "Epoch 46/200\n",
      "30/30 - 0s - loss: 0.0427 - mae: 0.3386 - root_mean_squared_error: 0.5072 - val_loss: 0.2936 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6230 - 83ms/epoch - 3ms/step\n",
      "Epoch 47/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3383 - root_mean_squared_error: 0.5069 - val_loss: 0.2940 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6234 - 98ms/epoch - 3ms/step\n",
      "Epoch 48/200\n",
      "30/30 - 0s - loss: 0.0426 - mae: 0.3384 - root_mean_squared_error: 0.5066 - val_loss: 0.2931 - val_mae: 0.6938 - val_root_mean_squared_error: 1.6226 - 100ms/epoch - 3ms/step\n",
      "Epoch 49/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3379 - root_mean_squared_error: 0.5066 - val_loss: 0.2948 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6241 - 102ms/epoch - 3ms/step\n",
      "Epoch 50/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3399 - root_mean_squared_error: 0.5077 - val_loss: 0.2942 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6236 - 103ms/epoch - 3ms/step\n",
      "Epoch 51/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3385 - root_mean_squared_error: 0.5072 - val_loss: 0.2935 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6230 - 85ms/epoch - 3ms/step\n",
      "Epoch 52/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3389 - root_mean_squared_error: 0.5076 - val_loss: 0.2956 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6248 - 43ms/epoch - 1ms/step\n",
      "Epoch 53/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3393 - root_mean_squared_error: 0.5078 - val_loss: 0.2944 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6237 - 88ms/epoch - 3ms/step\n",
      "Epoch 54/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3391 - root_mean_squared_error: 0.5076 - val_loss: 0.2941 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6235 - 90ms/epoch - 3ms/step\n",
      "Epoch 55/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3393 - root_mean_squared_error: 0.5083 - val_loss: 0.2946 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6239 - 92ms/epoch - 3ms/step\n",
      "Epoch 56/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3377 - root_mean_squared_error: 0.5069 - val_loss: 0.2953 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6245 - 91ms/epoch - 3ms/step\n",
      "Epoch 57/200\n",
      "30/30 - 0s - loss: 0.0431 - mae: 0.3379 - root_mean_squared_error: 0.5066 - val_loss: 0.2968 - val_mae: 0.6926 - val_root_mean_squared_error: 1.6258 - 97ms/epoch - 3ms/step\n",
      "Epoch 58/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3397 - root_mean_squared_error: 0.5079 - val_loss: 0.2943 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6237 - 104ms/epoch - 3ms/step\n",
      "Epoch 59/200\n",
      "30/30 - 0s - loss: 0.0431 - mae: 0.3366 - root_mean_squared_error: 0.5060 - val_loss: 0.2957 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6249 - 112ms/epoch - 4ms/step\n",
      "Epoch 60/200\n",
      "30/30 - 0s - loss: 0.0431 - mae: 0.3405 - root_mean_squared_error: 0.5090 - val_loss: 0.2925 - val_mae: 0.6941 - val_root_mean_squared_error: 1.6220 - 107ms/epoch - 4ms/step\n",
      "Epoch 61/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3386 - root_mean_squared_error: 0.5068 - val_loss: 0.2964 - val_mae: 0.6927 - val_root_mean_squared_error: 1.6255 - 84ms/epoch - 3ms/step\n",
      "Epoch 62/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3392 - root_mean_squared_error: 0.5073 - val_loss: 0.2929 - val_mae: 0.6938 - val_root_mean_squared_error: 1.6225 - 100ms/epoch - 3ms/step\n",
      "Epoch 63/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3383 - root_mean_squared_error: 0.5071 - val_loss: 0.2927 - val_mae: 0.6939 - val_root_mean_squared_error: 1.6222 - 113ms/epoch - 4ms/step\n",
      "Epoch 64/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3389 - root_mean_squared_error: 0.5077 - val_loss: 0.2950 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6243 - 112ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3374 - root_mean_squared_error: 0.5065 - val_loss: 0.2947 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6240 - 110ms/epoch - 4ms/step\n",
      "Epoch 66/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3425 - root_mean_squared_error: 0.5099 - val_loss: 0.2935 - val_mae: 0.6936 - val_root_mean_squared_error: 1.6229 - 99ms/epoch - 3ms/step\n",
      "Epoch 67/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3373 - root_mean_squared_error: 0.5065 - val_loss: 0.2984 - val_mae: 0.6921 - val_root_mean_squared_error: 1.6271 - 102ms/epoch - 3ms/step\n",
      "Epoch 68/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3379 - root_mean_squared_error: 0.5065 - val_loss: 0.2941 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6235 - 91ms/epoch - 3ms/step\n",
      "Epoch 69/200\n",
      "30/30 - 0s - loss: 0.0432 - mae: 0.3378 - root_mean_squared_error: 0.5067 - val_loss: 0.2945 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6238 - 101ms/epoch - 3ms/step\n",
      "Epoch 70/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3390 - root_mean_squared_error: 0.5076 - val_loss: 0.2943 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6237 - 97ms/epoch - 3ms/step\n",
      "Epoch 71/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3383 - root_mean_squared_error: 0.5073 - val_loss: 0.2950 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6242 - 78ms/epoch - 3ms/step\n",
      "Epoch 72/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3379 - root_mean_squared_error: 0.5068 - val_loss: 0.2942 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6235 - 98ms/epoch - 3ms/step\n",
      "Epoch 73/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3395 - root_mean_squared_error: 0.5077 - val_loss: 0.2951 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6244 - 98ms/epoch - 3ms/step\n",
      "Epoch 74/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3359 - root_mean_squared_error: 0.5056 - val_loss: 0.2952 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6244 - 120ms/epoch - 4ms/step\n",
      "Epoch 75/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3391 - root_mean_squared_error: 0.5078 - val_loss: 0.2928 - val_mae: 0.6939 - val_root_mean_squared_error: 1.6223 - 104ms/epoch - 3ms/step\n",
      "Epoch 76/200\n",
      "30/30 - 0s - loss: 0.0431 - mae: 0.3396 - root_mean_squared_error: 0.5080 - val_loss: 0.2958 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6250 - 87ms/epoch - 3ms/step\n",
      "Epoch 77/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3394 - root_mean_squared_error: 0.5077 - val_loss: 0.2955 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6247 - 89ms/epoch - 3ms/step\n",
      "Epoch 78/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3369 - root_mean_squared_error: 0.5060 - val_loss: 0.2951 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6244 - 103ms/epoch - 3ms/step\n",
      "Epoch 79/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3387 - root_mean_squared_error: 0.5074 - val_loss: 0.2933 - val_mae: 0.6936 - val_root_mean_squared_error: 1.6228 - 94ms/epoch - 3ms/step\n",
      "Epoch 80/200\n",
      "30/30 - 0s - loss: 0.0427 - mae: 0.3387 - root_mean_squared_error: 0.5070 - val_loss: 0.2943 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6237 - 88ms/epoch - 3ms/step\n",
      "Epoch 81/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3389 - root_mean_squared_error: 0.5074 - val_loss: 0.2958 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6249 - 81ms/epoch - 3ms/step\n",
      "Epoch 82/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3383 - root_mean_squared_error: 0.5073 - val_loss: 0.2927 - val_mae: 0.6940 - val_root_mean_squared_error: 1.6222 - 87ms/epoch - 3ms/step\n",
      "Epoch 83/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3393 - root_mean_squared_error: 0.5076 - val_loss: 0.2937 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6231 - 91ms/epoch - 3ms/step\n",
      "Epoch 84/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3385 - root_mean_squared_error: 0.5074 - val_loss: 0.2954 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6246 - 96ms/epoch - 3ms/step\n",
      "Epoch 85/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3377 - root_mean_squared_error: 0.5068 - val_loss: 0.2948 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6241 - 114ms/epoch - 4ms/step\n",
      "Epoch 86/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3391 - root_mean_squared_error: 0.5074 - val_loss: 0.2925 - val_mae: 0.6941 - val_root_mean_squared_error: 1.6220 - 86ms/epoch - 3ms/step\n",
      "Epoch 87/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3375 - root_mean_squared_error: 0.5066 - val_loss: 0.2943 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6236 - 91ms/epoch - 3ms/step\n",
      "Epoch 88/200\n",
      "30/30 - 0s - loss: 0.0431 - mae: 0.3369 - root_mean_squared_error: 0.5067 - val_loss: 0.2942 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6236 - 96ms/epoch - 3ms/step\n",
      "Epoch 89/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3389 - root_mean_squared_error: 0.5072 - val_loss: 0.2937 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6231 - 94ms/epoch - 3ms/step\n",
      "Epoch 90/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3419 - root_mean_squared_error: 0.5095 - val_loss: 0.2935 - val_mae: 0.6936 - val_root_mean_squared_error: 1.6229 - 93ms/epoch - 3ms/step\n",
      "Epoch 91/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3384 - root_mean_squared_error: 0.5071 - val_loss: 0.2942 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6236 - 94ms/epoch - 3ms/step\n",
      "Epoch 92/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3389 - root_mean_squared_error: 0.5076 - val_loss: 0.2946 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6239 - 85ms/epoch - 3ms/step\n",
      "Epoch 93/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3399 - root_mean_squared_error: 0.5081 - val_loss: 0.2949 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6242 - 89ms/epoch - 3ms/step\n",
      "Epoch 94/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3384 - root_mean_squared_error: 0.5072 - val_loss: 0.2962 - val_mae: 0.6928 - val_root_mean_squared_error: 1.6253 - 112ms/epoch - 4ms/step\n",
      "Epoch 95/200\n",
      "30/30 - 0s - loss: 0.0427 - mae: 0.3383 - root_mean_squared_error: 0.5067 - val_loss: 0.2942 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6236 - 101ms/epoch - 3ms/step\n",
      "Epoch 96/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3399 - root_mean_squared_error: 0.5078 - val_loss: 0.2942 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6236 - 117ms/epoch - 4ms/step\n",
      "Epoch 97/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3398 - root_mean_squared_error: 0.5081 - val_loss: 0.2945 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6238 - 105ms/epoch - 4ms/step\n",
      "Epoch 98/200\n",
      "30/30 - 0s - loss: 0.0431 - mae: 0.3357 - root_mean_squared_error: 0.5057 - val_loss: 0.2956 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6248 - 101ms/epoch - 3ms/step\n",
      "Epoch 99/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3406 - root_mean_squared_error: 0.5084 - val_loss: 0.2932 - val_mae: 0.6937 - val_root_mean_squared_error: 1.6227 - 91ms/epoch - 3ms/step\n",
      "Epoch 100/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3389 - root_mean_squared_error: 0.5077 - val_loss: 0.2957 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6249 - 85ms/epoch - 3ms/step\n",
      "Epoch 101/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3404 - root_mean_squared_error: 0.5084 - val_loss: 0.2937 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6231 - 86ms/epoch - 3ms/step\n",
      "Epoch 102/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3368 - root_mean_squared_error: 0.5059 - val_loss: 0.2963 - val_mae: 0.6927 - val_root_mean_squared_error: 1.6254 - 95ms/epoch - 3ms/step\n",
      "Epoch 103/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3372 - root_mean_squared_error: 0.5064 - val_loss: 0.2958 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6249 - 110ms/epoch - 4ms/step\n",
      "Epoch 104/200\n",
      "30/30 - 0s - loss: 0.0427 - mae: 0.3385 - root_mean_squared_error: 0.5068 - val_loss: 0.2933 - val_mae: 0.6936 - val_root_mean_squared_error: 1.6228 - 97ms/epoch - 3ms/step\n",
      "Epoch 105/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3394 - root_mean_squared_error: 0.5077 - val_loss: 0.2945 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6239 - 84ms/epoch - 3ms/step\n",
      "Epoch 106/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3396 - root_mean_squared_error: 0.5081 - val_loss: 0.2946 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6239 - 80ms/epoch - 3ms/step\n",
      "Epoch 107/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3379 - root_mean_squared_error: 0.5066 - val_loss: 0.2945 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6238 - 98ms/epoch - 3ms/step\n",
      "Epoch 108/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3367 - root_mean_squared_error: 0.5059 - val_loss: 0.2961 - val_mae: 0.6928 - val_root_mean_squared_error: 1.6253 - 95ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3368 - root_mean_squared_error: 0.5058 - val_loss: 0.2937 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6231 - 87ms/epoch - 3ms/step\n",
      "Epoch 110/200\n",
      "30/30 - 0s - loss: 0.0431 - mae: 0.3408 - root_mean_squared_error: 0.5089 - val_loss: 0.2924 - val_mae: 0.6941 - val_root_mean_squared_error: 1.6219 - 90ms/epoch - 3ms/step\n",
      "Epoch 111/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3386 - root_mean_squared_error: 0.5072 - val_loss: 0.2951 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6243 - 88ms/epoch - 3ms/step\n",
      "Epoch 112/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3376 - root_mean_squared_error: 0.5067 - val_loss: 0.2943 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6237 - 82ms/epoch - 3ms/step\n",
      "Epoch 113/200\n",
      "30/30 - 0s - loss: 0.0427 - mae: 0.3385 - root_mean_squared_error: 0.5069 - val_loss: 0.2944 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6238 - 87ms/epoch - 3ms/step\n",
      "Epoch 114/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3389 - root_mean_squared_error: 0.5075 - val_loss: 0.2930 - val_mae: 0.6938 - val_root_mean_squared_error: 1.6225 - 88ms/epoch - 3ms/step\n",
      "Epoch 115/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3388 - root_mean_squared_error: 0.5071 - val_loss: 0.2934 - val_mae: 0.6936 - val_root_mean_squared_error: 1.6229 - 93ms/epoch - 3ms/step\n",
      "Epoch 116/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3384 - root_mean_squared_error: 0.5071 - val_loss: 0.2942 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6236 - 92ms/epoch - 3ms/step\n",
      "Epoch 117/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3403 - root_mean_squared_error: 0.5083 - val_loss: 0.2941 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6234 - 112ms/epoch - 4ms/step\n",
      "Epoch 118/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3389 - root_mean_squared_error: 0.5075 - val_loss: 0.2939 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6233 - 110ms/epoch - 4ms/step\n",
      "Epoch 119/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3380 - root_mean_squared_error: 0.5068 - val_loss: 0.2946 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6239 - 88ms/epoch - 3ms/step\n",
      "Epoch 120/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3377 - root_mean_squared_error: 0.5069 - val_loss: 0.2940 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6234 - 99ms/epoch - 3ms/step\n",
      "Epoch 121/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3387 - root_mean_squared_error: 0.5077 - val_loss: 0.2934 - val_mae: 0.6936 - val_root_mean_squared_error: 1.6229 - 95ms/epoch - 3ms/step\n",
      "Epoch 122/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3362 - root_mean_squared_error: 0.5058 - val_loss: 0.2958 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6249 - 84ms/epoch - 3ms/step\n",
      "Epoch 123/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3380 - root_mean_squared_error: 0.5067 - val_loss: 0.2933 - val_mae: 0.6937 - val_root_mean_squared_error: 1.6227 - 85ms/epoch - 3ms/step\n",
      "Epoch 124/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3400 - root_mean_squared_error: 0.5081 - val_loss: 0.2927 - val_mae: 0.6939 - val_root_mean_squared_error: 1.6223 - 98ms/epoch - 3ms/step\n",
      "Epoch 125/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3393 - root_mean_squared_error: 0.5077 - val_loss: 0.2933 - val_mae: 0.6936 - val_root_mean_squared_error: 1.6228 - 84ms/epoch - 3ms/step\n",
      "Epoch 126/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3360 - root_mean_squared_error: 0.5057 - val_loss: 0.2959 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6250 - 86ms/epoch - 3ms/step\n",
      "Epoch 127/200\n",
      "30/30 - 0s - loss: 0.0431 - mae: 0.3429 - root_mean_squared_error: 0.5103 - val_loss: 0.2923 - val_mae: 0.6942 - val_root_mean_squared_error: 1.6219 - 101ms/epoch - 3ms/step\n",
      "Epoch 128/200\n",
      "30/30 - 0s - loss: 0.0431 - mae: 0.3354 - root_mean_squared_error: 0.5053 - val_loss: 0.2966 - val_mae: 0.6926 - val_root_mean_squared_error: 1.6257 - 86ms/epoch - 3ms/step\n",
      "Epoch 129/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3387 - root_mean_squared_error: 0.5073 - val_loss: 0.2930 - val_mae: 0.6938 - val_root_mean_squared_error: 1.6225 - 91ms/epoch - 3ms/step\n",
      "Epoch 130/200\n",
      "30/30 - 0s - loss: 0.0431 - mae: 0.3424 - root_mean_squared_error: 0.5104 - val_loss: 0.2947 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6240 - 101ms/epoch - 3ms/step\n",
      "Epoch 131/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3363 - root_mean_squared_error: 0.5059 - val_loss: 0.2966 - val_mae: 0.6927 - val_root_mean_squared_error: 1.6256 - 107ms/epoch - 4ms/step\n",
      "Epoch 132/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3382 - root_mean_squared_error: 0.5071 - val_loss: 0.2936 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6231 - 86ms/epoch - 3ms/step\n",
      "Epoch 133/200\n",
      "30/30 - 0s - loss: 0.0427 - mae: 0.3394 - root_mean_squared_error: 0.5078 - val_loss: 0.2948 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6241 - 84ms/epoch - 3ms/step\n",
      "Epoch 134/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3369 - root_mean_squared_error: 0.5058 - val_loss: 0.2954 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6246 - 90ms/epoch - 3ms/step\n",
      "Epoch 135/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3400 - root_mean_squared_error: 0.5082 - val_loss: 0.2932 - val_mae: 0.6937 - val_root_mean_squared_error: 1.6227 - 89ms/epoch - 3ms/step\n",
      "Epoch 136/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3385 - root_mean_squared_error: 0.5072 - val_loss: 0.2942 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6236 - 86ms/epoch - 3ms/step\n",
      "Epoch 137/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3379 - root_mean_squared_error: 0.5067 - val_loss: 0.2948 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6241 - 109ms/epoch - 4ms/step\n",
      "Epoch 138/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3402 - root_mean_squared_error: 0.5081 - val_loss: 0.2934 - val_mae: 0.6936 - val_root_mean_squared_error: 1.6229 - 109ms/epoch - 4ms/step\n",
      "Epoch 139/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3399 - root_mean_squared_error: 0.5079 - val_loss: 0.2944 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6238 - 99ms/epoch - 3ms/step\n",
      "Epoch 140/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3395 - root_mean_squared_error: 0.5081 - val_loss: 0.2950 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6243 - 85ms/epoch - 3ms/step\n",
      "Epoch 141/200\n",
      "30/30 - 0s - loss: 0.0427 - mae: 0.3370 - root_mean_squared_error: 0.5060 - val_loss: 0.2953 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6245 - 94ms/epoch - 3ms/step\n",
      "Epoch 142/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3384 - root_mean_squared_error: 0.5074 - val_loss: 0.2940 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6234 - 91ms/epoch - 3ms/step\n",
      "Epoch 143/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3369 - root_mean_squared_error: 0.5062 - val_loss: 0.2963 - val_mae: 0.6927 - val_root_mean_squared_error: 1.6254 - 97ms/epoch - 3ms/step\n",
      "Epoch 144/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3393 - root_mean_squared_error: 0.5077 - val_loss: 0.2924 - val_mae: 0.6941 - val_root_mean_squared_error: 1.6220 - 90ms/epoch - 3ms/step\n",
      "Epoch 145/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3375 - root_mean_squared_error: 0.5064 - val_loss: 0.2954 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6246 - 94ms/epoch - 3ms/step\n",
      "Epoch 146/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3380 - root_mean_squared_error: 0.5067 - val_loss: 0.2937 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6231 - 95ms/epoch - 3ms/step\n",
      "Epoch 147/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3382 - root_mean_squared_error: 0.5072 - val_loss: 0.2938 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6232 - 80ms/epoch - 3ms/step\n",
      "Epoch 148/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3407 - root_mean_squared_error: 0.5084 - val_loss: 0.2935 - val_mae: 0.6936 - val_root_mean_squared_error: 1.6229 - 92ms/epoch - 3ms/step\n",
      "Epoch 149/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3404 - root_mean_squared_error: 0.5087 - val_loss: 0.2924 - val_mae: 0.6941 - val_root_mean_squared_error: 1.6219 - 93ms/epoch - 3ms/step\n",
      "Epoch 150/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3381 - root_mean_squared_error: 0.5069 - val_loss: 0.2949 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6242 - 94ms/epoch - 3ms/step\n",
      "Epoch 151/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3394 - root_mean_squared_error: 0.5080 - val_loss: 0.2938 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6232 - 116ms/epoch - 4ms/step\n",
      "Epoch 152/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3363 - root_mean_squared_error: 0.5052 - val_loss: 0.2958 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6249 - 123ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3374 - root_mean_squared_error: 0.5064 - val_loss: 0.2947 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6240 - 124ms/epoch - 4ms/step\n",
      "Epoch 154/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3405 - root_mean_squared_error: 0.5083 - val_loss: 0.2940 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6234 - 118ms/epoch - 4ms/step\n",
      "Epoch 155/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3374 - root_mean_squared_error: 0.5066 - val_loss: 0.2944 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6238 - 108ms/epoch - 4ms/step\n",
      "Epoch 156/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3396 - root_mean_squared_error: 0.5078 - val_loss: 0.2927 - val_mae: 0.6940 - val_root_mean_squared_error: 1.6222 - 92ms/epoch - 3ms/step\n",
      "Epoch 157/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3383 - root_mean_squared_error: 0.5071 - val_loss: 0.2950 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6243 - 83ms/epoch - 3ms/step\n",
      "Epoch 158/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3398 - root_mean_squared_error: 0.5078 - val_loss: 0.2929 - val_mae: 0.6939 - val_root_mean_squared_error: 1.6224 - 85ms/epoch - 3ms/step\n",
      "Epoch 159/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3376 - root_mean_squared_error: 0.5067 - val_loss: 0.2951 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6244 - 94ms/epoch - 3ms/step\n",
      "Epoch 160/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3386 - root_mean_squared_error: 0.5070 - val_loss: 0.2946 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6239 - 104ms/epoch - 3ms/step\n",
      "Epoch 161/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3397 - root_mean_squared_error: 0.5080 - val_loss: 0.2935 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6230 - 89ms/epoch - 3ms/step\n",
      "Epoch 162/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3403 - root_mean_squared_error: 0.5083 - val_loss: 0.2932 - val_mae: 0.6937 - val_root_mean_squared_error: 1.6226 - 87ms/epoch - 3ms/step\n",
      "Epoch 163/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3390 - root_mean_squared_error: 0.5075 - val_loss: 0.2960 - val_mae: 0.6928 - val_root_mean_squared_error: 1.6252 - 87ms/epoch - 3ms/step\n",
      "Epoch 164/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3368 - root_mean_squared_error: 0.5060 - val_loss: 0.2940 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6234 - 87ms/epoch - 3ms/step\n",
      "Epoch 165/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3373 - root_mean_squared_error: 0.5064 - val_loss: 0.2955 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6247 - 93ms/epoch - 3ms/step\n",
      "Epoch 166/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3378 - root_mean_squared_error: 0.5066 - val_loss: 0.2950 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6243 - 94ms/epoch - 3ms/step\n",
      "Epoch 167/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3406 - root_mean_squared_error: 0.5088 - val_loss: 0.2937 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6232 - 111ms/epoch - 4ms/step\n",
      "Epoch 168/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3397 - root_mean_squared_error: 0.5081 - val_loss: 0.2922 - val_mae: 0.6942 - val_root_mean_squared_error: 1.6217 - 99ms/epoch - 3ms/step\n",
      "Epoch 169/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3413 - root_mean_squared_error: 0.5090 - val_loss: 0.2939 - val_mae: 0.6934 - val_root_mean_squared_error: 1.6233 - 82ms/epoch - 3ms/step\n",
      "Epoch 170/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3375 - root_mean_squared_error: 0.5065 - val_loss: 0.2958 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6249 - 87ms/epoch - 3ms/step\n",
      "Epoch 171/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3371 - root_mean_squared_error: 0.5064 - val_loss: 0.2936 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6231 - 93ms/epoch - 3ms/step\n",
      "Epoch 172/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3385 - root_mean_squared_error: 0.5071 - val_loss: 0.2950 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6242 - 97ms/epoch - 3ms/step\n",
      "Epoch 173/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3378 - root_mean_squared_error: 0.5067 - val_loss: 0.2948 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6241 - 79ms/epoch - 3ms/step\n",
      "Epoch 174/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3383 - root_mean_squared_error: 0.5070 - val_loss: 0.2944 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6238 - 86ms/epoch - 3ms/step\n",
      "Epoch 175/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3392 - root_mean_squared_error: 0.5076 - val_loss: 0.2938 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6232 - 96ms/epoch - 3ms/step\n",
      "Epoch 176/200\n",
      "30/30 - 0s - loss: 0.0427 - mae: 0.3387 - root_mean_squared_error: 0.5072 - val_loss: 0.2944 - val_mae: 0.6933 - val_root_mean_squared_error: 1.6237 - 89ms/epoch - 3ms/step\n",
      "Epoch 177/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3375 - root_mean_squared_error: 0.5065 - val_loss: 0.2946 - val_mae: 0.6932 - val_root_mean_squared_error: 1.6239 - 108ms/epoch - 4ms/step\n",
      "Epoch 178/200\n",
      "30/30 - 0s - loss: 0.0427 - mae: 0.3384 - root_mean_squared_error: 0.5070 - val_loss: 0.2932 - val_mae: 0.6937 - val_root_mean_squared_error: 1.6227 - 89ms/epoch - 3ms/step\n",
      "Epoch 179/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3409 - root_mean_squared_error: 0.5091 - val_loss: 0.2931 - val_mae: 0.6937 - val_root_mean_squared_error: 1.6226 - 85ms/epoch - 3ms/step\n",
      "Epoch 180/200\n",
      "30/30 - 0s - loss: 0.0427 - mae: 0.3373 - root_mean_squared_error: 0.5062 - val_loss: 0.2960 - val_mae: 0.6928 - val_root_mean_squared_error: 1.6252 - 83ms/epoch - 3ms/step\n",
      "Epoch 181/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3383 - root_mean_squared_error: 0.5073 - val_loss: 0.2938 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6233 - 100ms/epoch - 3ms/step\n",
      "Epoch 182/200\n",
      "30/30 - 0s - loss: 0.0427 - mae: 0.3380 - root_mean_squared_error: 0.5067 - val_loss: 0.2950 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6242 - 88ms/epoch - 3ms/step\n",
      "Epoch 183/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3371 - root_mean_squared_error: 0.5063 - val_loss: 0.2964 - val_mae: 0.6927 - val_root_mean_squared_error: 1.6255 - 89ms/epoch - 3ms/step\n",
      "Epoch 184/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3381 - root_mean_squared_error: 0.5068 - val_loss: 0.2930 - val_mae: 0.6938 - val_root_mean_squared_error: 1.6225 - 87ms/epoch - 3ms/step\n",
      "Epoch 185/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3380 - root_mean_squared_error: 0.5069 - val_loss: 0.2953 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6245 - 93ms/epoch - 3ms/step\n",
      "Epoch 186/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3390 - root_mean_squared_error: 0.5076 - val_loss: 0.2934 - val_mae: 0.6936 - val_root_mean_squared_error: 1.6228 - 111ms/epoch - 4ms/step\n",
      "Epoch 187/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3385 - root_mean_squared_error: 0.5072 - val_loss: 0.2949 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6242 - 92ms/epoch - 3ms/step\n",
      "Epoch 188/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3386 - root_mean_squared_error: 0.5071 - val_loss: 0.2933 - val_mae: 0.6936 - val_root_mean_squared_error: 1.6228 - 86ms/epoch - 3ms/step\n",
      "Epoch 189/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3381 - root_mean_squared_error: 0.5068 - val_loss: 0.2951 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6244 - 97ms/epoch - 3ms/step\n",
      "Epoch 190/200\n",
      "30/30 - 0s - loss: 0.0427 - mae: 0.3383 - root_mean_squared_error: 0.5070 - val_loss: 0.2935 - val_mae: 0.6936 - val_root_mean_squared_error: 1.6230 - 105ms/epoch - 3ms/step\n",
      "Epoch 191/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3402 - root_mean_squared_error: 0.5085 - val_loss: 0.2935 - val_mae: 0.6936 - val_root_mean_squared_error: 1.6229 - 102ms/epoch - 3ms/step\n",
      "Epoch 192/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3391 - root_mean_squared_error: 0.5077 - val_loss: 0.2949 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6242 - 88ms/epoch - 3ms/step\n",
      "Epoch 193/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3383 - root_mean_squared_error: 0.5072 - val_loss: 0.2938 - val_mae: 0.6935 - val_root_mean_squared_error: 1.6232 - 91ms/epoch - 3ms/step\n",
      "Epoch 194/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3376 - root_mean_squared_error: 0.5065 - val_loss: 0.2954 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6246 - 109ms/epoch - 4ms/step\n",
      "Epoch 195/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3405 - root_mean_squared_error: 0.5087 - val_loss: 0.2931 - val_mae: 0.6937 - val_root_mean_squared_error: 1.6226 - 99ms/epoch - 3ms/step\n",
      "Epoch 196/200\n",
      "30/30 - 0s - loss: 0.0428 - mae: 0.3380 - root_mean_squared_error: 0.5068 - val_loss: 0.2952 - val_mae: 0.6931 - val_root_mean_squared_error: 1.6244 - 83ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3363 - root_mean_squared_error: 0.5056 - val_loss: 0.2958 - val_mae: 0.6929 - val_root_mean_squared_error: 1.6249 - 106ms/epoch - 4ms/step\n",
      "Epoch 198/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3369 - root_mean_squared_error: 0.5061 - val_loss: 0.2960 - val_mae: 0.6928 - val_root_mean_squared_error: 1.6251 - 93ms/epoch - 3ms/step\n",
      "Epoch 199/200\n",
      "30/30 - 0s - loss: 0.0430 - mae: 0.3411 - root_mean_squared_error: 0.5091 - val_loss: 0.2930 - val_mae: 0.6938 - val_root_mean_squared_error: 1.6225 - 88ms/epoch - 3ms/step\n",
      "Epoch 200/200\n",
      "30/30 - 0s - loss: 0.0429 - mae: 0.3381 - root_mean_squared_error: 0.5071 - val_loss: 0.2955 - val_mae: 0.6930 - val_root_mean_squared_error: 1.6247 - 91ms/epoch - 3ms/step\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f072c17b9d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f071c7c81f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Root Mean Squared Error (Ensemble): 0.42531473179485246\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Define the individual neural network models\n",
    "def create_model_1():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(32, activation='relu', input_shape=(num_features,)))\n",
    "    model.add(tf.keras.layers.Dense(24, activation='LeakyReLU'))\n",
    "    model.add(tf.keras.layers.Dense(14, activation='PReLU'))\n",
    "    model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(4, activation='selu'))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae', 'RootMeanSquaredError'])\n",
    "    return model\n",
    "\n",
    "def create_model_2(learning_rate=0.001, l2_regularization=0.1):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu', input_shape=(num_features,),\n",
    "                                   kernel_regularizer=tf.keras.regularizers.l2(l2_regularization)))\n",
    "    model.add(tf.keras.layers.Dropout(0.05))\n",
    "    model.add(tf.keras.layers.Dense(48, activation='PReLU',\n",
    "                                   kernel_regularizer=tf.keras.regularizers.l2(l2_regularization)))\n",
    "    model.add(tf.keras.layers.Dropout(0.05))\n",
    "    model.add(tf.keras.layers.Dense(30, activation='relu',\n",
    "                                   kernel_regularizer=tf.keras.regularizers.l2(l2_regularization)))\n",
    "    model.add(tf.keras.layers.Dropout(0.05))\n",
    "    model.add(tf.keras.layers.Dense(18, activation='selu',\n",
    "                                   kernel_regularizer=tf.keras.regularizers.l2(l2_regularization)))\n",
    "    model.add(tf.keras.layers.Dropout(0.05))\n",
    "    model.add(tf.keras.layers.Dense(8, activation='relu',\n",
    "                                   kernel_regularizer=tf.keras.regularizers.l2(l2_regularization)))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='msle', metrics=['mae', 'RootMeanSquaredError'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def create_model_3(learning_rate=0.008, dropout_rate=0.1, l2_regularization=0.02):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu', input_shape=(num_features,),\n",
    "                                   kernel_regularizer=tf.keras.regularizers.l2(l2_regularization)))\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(32, activation='relu',\n",
    "                                   kernel_regularizer=tf.keras.regularizers.l2(l2_regularization)))\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(16, activation='relu',\n",
    "                                   kernel_regularizer=tf.keras.regularizers.l2(l2_regularization)))\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='msle', metrics=['mae', 'RootMeanSquaredError'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define the RNN model\n",
    "def create_RNN():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.SimpleRNN(units=64, activation='relu', return_sequences=True, input_shape=(num_features, 1)),\n",
    "        tf.keras.layers.SimpleRNN(units=32, activation='relu', return_sequences=True),\n",
    "        tf.keras.layers.SimpleRNN(units=16, activation='selu'),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(units=8, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(units=4, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae', 'RootMeanSquaredError'])   \n",
    "    return model\n",
    "\n",
    "# Define the CNN model\n",
    "def create_CNN():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(num_features, 1)),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=24, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.05),\n",
    "        tf.keras.layers.Dense(units=16, activation='selu'),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(units=8, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(units=4, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae', 'RootMeanSquaredError'])   \n",
    "    return model\n",
    "\n",
    "# Create instances of the individual models\n",
    "model_2 = create_model_2(learning_rate=0.0008, l2_regularization=0.04)\n",
    "model_3 = create_model_3(learning_rate=0.002, dropout_rate=0.05, l2_regularization=0.02)\n",
    "\n",
    "# Train the individual models and store the training history\n",
    "history_2 = model_2.fit(X_train, y_train, epochs=200, batch_size=10, verbose=2, validation_split=0.1)\n",
    "history_3 = model_3.fit(X_train, y_train, epochs=200, batch_size=10, verbose=2, validation_split=0.1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions using the individual models\n",
    "y_pred_2 = model_2.predict(X_test)\n",
    "y_pred_3 = model_3.predict(X_test)\n",
    "\n",
    "# Combine the predictions\n",
    "y_pred_ensemble = (y_pred_2 + y_pred_3) / 2\n",
    "\n",
    "# Calculate the root mean squared error (RMSE) for the ensemble prediction\n",
    "mse_ensemble = mean_squared_error(y_test, y_pred_ensemble)\n",
    "rmse_ensemble = np.sqrt(mse_ensemble)\n",
    "print(\"Root Mean Squared Error (Ensemble):\", rmse_ensemble)\n",
    "\n",
    "# model_1.save('model_files/nn_model/cubic/model_1.h5')\n",
    "model_2.save('model_files/nn_model/cubic/model_2.h5')\n",
    "model_3.save('model_files/nn_model/cubic/model_3.h5')\n",
    "# model_4.save('model_files/nn_model/cubic/model_4.h5')\n",
    "# model_5.save('model_files/nn_model/cubic/model_5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47b5e264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "def plot_learning_curve(history, model_name):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(f'{model_name} Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "   \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62e31ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot learning curves for model_2 and model_3\n",
    "plot_learning_curve(history_2, \"Model 2\")\n",
    "plot_learning_curve(history_3, \"Model 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb13302c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2404701b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 33) dtype=float32 (created by layer 'input_3')>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.Input(shape=(num_features,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25aef64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-203666985224433.84\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_mean = r2_score(y_pred_ensemble,y_test_cubic)\n",
    "print(r2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa06a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "modela2 = model_2.fit(X_train, y_train, epochs=200, batch_size=10, verbose=2, validation_split=0.1)\n",
    "modela3 = model_3.fit(X_train, y_train, epochs=200, batch_size=10, verbose=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee6ac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "def plot_learning_curves(history):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the learning curves using the training history\n",
    "# plot_learning_curves(history_pca_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf98f358",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(modela2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ffaa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(modela3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31917f65",
   "metadata": {},
   "source": [
    "## 1.2  CAT A: tetra42m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c47100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run functions.ipynb\n",
    "# scaler_cubic, std_train_cubic = std_data(X_train_cubic)\n",
    "# input_name = list(X_train_cubic.columns.values)\n",
    "\n",
    "# X_train_cubic, X_test_cubic, y_train_cubic, y_test_cubic = tensor_preprocessing(df_cubic)\n",
    "# X_train_tetra42m, X_test_tetra42m, y_train_tetra42m, y_test_tetra42m  = tensor_preprocessing(df_tetra42m)\n",
    "# X_train_ortho222, X_test_ortho222, y_train_ortho222, y_test_ortho222  = tensor_preprocessing(df_ortho222)\n",
    "\n",
    "# X_train_orthomm2, X_test_orthomm2, y_train_orthomm2, y_test_orthomm2  = tensor_preprocessing(df_orthomm2)\n",
    "# X_train_hextetramm, X_test_hextetramm, y_train_hextetramm, y_test_hextetramm = tensor_preprocessing(df_hextetramm)\n",
    "\n",
    "path='model_files//nn_model//tetra42m//'\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Standardize the data\n",
    "scaler_tetra42m = StandardScaler() # RENAME here 1 places\n",
    "std_train = scaler_tetra42m.fit_transform(X_train_tetra42m) # RENAME here 1 places\n",
    "\n",
    "# std_train_cubic = pd.DataFrame(data= std_train_cubic, columns=input_name)\n",
    "\n",
    "import pickle\n",
    "pickle.dump(scaler_tetra42m, open(path+'scaler_reg.pkl','wb')) # RENAME here 2 places\n",
    "\n",
    "std_test = scaler_tetra42m.transform(X_test_tetra42m) # RENAME here 1 places\n",
    "\n",
    "\n",
    "%run functions.ipynb\n",
    "\n",
    "pca_tetra42m, X_train_pca_tetra42m = pca_fs(std_train,\"plots\\\\\", title=\"a) PCA\") # RENAME here 1 places\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import pickle\n",
    "pickle.dump(pca_tetra42m, open(path+'pca_reg.pkl','wb')) # RENAME here 2 places\n",
    "\n",
    "\n",
    "X_test_pca_tetra42m =  pca_tetra42m.transform(std_test)      # RENAME here 1 places\n",
    "\n",
    "y_train = y_train_tetra42m\n",
    "X_train = X_train_pca_tetra42m\n",
    "\n",
    "y_test = y_test_tetra42m\n",
    "X_test = X_test_pca_tetra42m\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "y_train = np.array(list(y_train))\n",
    "y_test = np.array(list(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45da1880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Define the individual neural network models\n",
    "def create_model_1():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(32, activation='relu', input_shape=(num_features,)))\n",
    "    model.add(tf.keras.layers.Dense(24, activation='LeakyReLU'))\n",
    "    model.add(tf.keras.layers.Dense(14, activation='PReLU'))\n",
    "    model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(4, activation='selu'))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(y_train.shape[1]))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae', 'RootMeanSquaredError'])\n",
    "    return model\n",
    "\n",
    "def create_model_2():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu', input_shape=(num_features,)))\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(y_train.shape[1]))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae', 'RootMeanSquaredError'])\n",
    "    return model\n",
    "\n",
    "def create_model_3():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(32, activation='relu', input_shape=(num_features,)))\n",
    "    model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(y_train.shape[1]))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae', 'RootMeanSquaredError'])\n",
    "    return model\n",
    "\n",
    "# Define the RNN model\n",
    "def create_RNN():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.SimpleRNN(units=64, activation='relu', return_sequences=True, input_shape=(num_features, 1)),\n",
    "        tf.keras.layers.SimpleRNN(units=32, activation='relu', return_sequences=True),\n",
    "        tf.keras.layers.SimpleRNN(units=16, activation='selu'),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(units=8, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(units=4, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=y_train.shape[1])\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae', 'RootMeanSquaredError'])   \n",
    "    return model\n",
    "\n",
    "# Define the CNN model\n",
    "def create_CNN():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(num_features, 1)),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=24, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.05),\n",
    "        tf.keras.layers.Dense(units=16, activation='selu'),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(units=8, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(units=4, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=y_train.shape[1])\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae', 'RootMeanSquaredError'])    \n",
    "    return model\n",
    "\n",
    "# Create instances of the individual models\n",
    "model_1 = create_model_1()\n",
    "model_2 = create_model_2()\n",
    "model_3 = create_model_3()\n",
    "model_4 = create_RNN()\n",
    "model_5 = create_CNN()\n",
    "\n",
    "\n",
    "# Train the individual models\n",
    "model_1.fit(X_train, y_train, epochs=50, batch_size=8, verbose=2, validation_split=0.1)\n",
    "model_2.fit(X_train, y_train, epochs=50, batch_size=8, verbose=2, validation_split=0.1)\n",
    "model_3.fit(X_train, y_train, epochs=50, batch_size=8, verbose=2, validation_split=0.1)\n",
    "model_4.fit(X_train, y_train, epochs=50, batch_size=8, verbose=2, validation_split=0.1)\n",
    "model_5.fit(X_train, y_train, epochs=50, batch_size=8, verbose=2, validation_split=0.1)\n",
    "\n",
    "\n",
    "# Make predictions using the individual models\n",
    "y_pred_1 = model_1.predict(X_test)\n",
    "y_pred_2 = model_2.predict(X_test)\n",
    "y_pred_3 = model_3.predict(X_test)\n",
    "y_pred_4 = model_4.predict(X_test)\n",
    "y_pred_5 = model_5.predict(X_test)\n",
    "\n",
    "\n",
    "# Combine the predictions\n",
    "y_pred_ensemble = (y_pred_1 + y_pred_2 + y_pred_3 + y_pred_4 + y_pred_5) / 5\n",
    "\n",
    "# Calculate the root mean squared error (RMSE)\n",
    "mse = mean_squared_error(y_test, y_pred_ensemble)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "\n",
    "model_1.save('model_files/nn_model/tetra42m/model_1.h5')\n",
    "model_2.save('model_files/nn_model/tetra42m/model_2.h5')\n",
    "model_3.save('model_files/nn_model/tetra42m/model_3.h5')\n",
    "model_4.save('model_files/nn_model/tetra42m/model_4.h5')\n",
    "model_5.save('model_files/nn_model/tetra42m/model_5.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d6f58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafe9bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_mean = r2_score(y_pred_ensemble,y_test)\n",
    "print(r2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611f1946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2f1c6d1",
   "metadata": {},
   "source": [
    "## 1.3   CAT A: ortho222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c88895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run functions.ipynb\n",
    "# scaler_cubic, std_train_cubic = std_data(X_train_cubic)\n",
    "# input_name = list(X_train_cubic.columns.values)\n",
    "\n",
    "# X_train_cubic, X_test_cubic, y_train_cubic, y_test_cubic = tensor_preprocessing(df_cubic)\n",
    "# X_train_tetra42m, X_test_tetra42m, y_train_tetra42m, y_test_tetra42m  = tensor_preprocessing(df_tetra42m)\n",
    "# X_train_ortho222, X_test_ortho222, y_train_ortho222, y_test_ortho222  = tensor_preprocessing(df_ortho222)\n",
    "\n",
    "# X_train_orthomm2, X_test_orthomm2, y_train_orthomm2, y_test_orthomm2  = tensor_preprocessing(df_orthomm2)\n",
    "# X_train_hextetramm, X_test_hextetramm, y_train_hextetramm, y_test_hextetramm = tensor_preprocessing(df_hextetramm)\n",
    "path='model_files//nn_model//ortho222//'\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Standardize the data\n",
    "scaler_ortho222 = StandardScaler() # RENAME here 1 places\n",
    "std_train = scaler_ortho222.fit_transform(X_train_ortho222) # RENAME here 1 places\n",
    "\n",
    "# std_train_cubic = pd.DataFrame(data= std_train_cubic, columns=input_name)\n",
    "\n",
    "import pickle\n",
    "pickle.dump(scaler_ortho222, open(path+'scaler_reg.pkl','wb')) # RENAME here 2 places\n",
    "\n",
    "std_test = scaler_ortho222.transform(X_test_ortho222) # RENAME here 1 places\n",
    "\n",
    "\n",
    "%run functions.ipynb\n",
    "\n",
    "pca_ortho222, X_train_pca_ortho222 = pca_fs(std_train,\"plots\\\\\", title=\"a) PCA\") # RENAME here 1 places\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import pickle\n",
    "pickle.dump(pca_ortho222, open(path+'pca_reg.pkl','wb')) # RENAME here 2 places\n",
    "\n",
    "\n",
    "X_test_pca_ortho222 =  pca_ortho222.transform(std_test)      # RENAME here 1 places\n",
    "\n",
    "y_train = y_train_ortho222\n",
    "X_train = X_train_pca_ortho222\n",
    "\n",
    "y_test = y_test_ortho222\n",
    "X_test = X_test_pca_ortho222\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "y_train = np.array(list(y_train))\n",
    "y_test = np.array(list(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c277219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the individual neural network models\n",
    "def create_model_1():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(32, activation='relu', input_shape=(num_features,)))\n",
    "    model.add(tf.keras.layers.Dense(24, activation='LeakyReLU'))\n",
    "    model.add(tf.keras.layers.Dense(14, activation='PReLU'))\n",
    "    model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(4, activation='selu'))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(y_train.shape[1]))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae', 'RootMeanSquaredError'])\n",
    "    return model\n",
    "\n",
    "def create_model_2():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu', input_shape=(num_features,)))\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(y_train.shape[1]))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae', 'RootMeanSquaredError'])\n",
    "    return model\n",
    "\n",
    "def create_model_3():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(32, activation='relu', input_shape=(num_features,)))\n",
    "    model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(y_train.shape[1]))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae', 'RootMeanSquaredError'])\n",
    "    return model\n",
    "\n",
    "# Define the RNN model\n",
    "def create_RNN():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.SimpleRNN(units=64, activation='relu', return_sequences=True, input_shape=(num_features, 1)),\n",
    "        tf.keras.layers.SimpleRNN(units=32, activation='relu', return_sequences=True),\n",
    "        tf.keras.layers.SimpleRNN(units=16, activation='selu'),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(units=8, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(units=4, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=y_train.shape[1])\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae', 'RootMeanSquaredError'])   \n",
    "    return model\n",
    "\n",
    "# Define the CNN model\n",
    "def create_CNN():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(num_features, 1)),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=24, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.05),\n",
    "        tf.keras.layers.Dense(units=16, activation='selu'),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(units=8, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(units=4, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=y_train.shape[1])\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae', 'RootMeanSquaredError'])    \n",
    "    return model\n",
    "\n",
    "# Create instances of the individual models\n",
    "model_1 = create_model_1()\n",
    "model_2 = create_model_2()\n",
    "model_3 = create_model_3()\n",
    "model_4 = create_RNN()\n",
    "model_5 = create_CNN()\n",
    "\n",
    "\n",
    "# Train the individual models\n",
    "model_1.fit(X_train, y_train, epochs=50, batch_size=8, verbose=2, validation_split=0.1)\n",
    "model_2.fit(X_train, y_train, epochs=50, batch_size=8, verbose=2, validation_split=0.1)\n",
    "model_3.fit(X_train, y_train, epochs=50, batch_size=8, verbose=2, validation_split=0.1)\n",
    "model_4.fit(X_train, y_train, epochs=50, batch_size=8, verbose=2, validation_split=0.1)\n",
    "model_5.fit(X_train, y_train, epochs=50, batch_size=8, verbose=2, validation_split=0.1)\n",
    "\n",
    "\n",
    "# Make predictions using the individual models\n",
    "y_pred_1 = model_1.predict(X_test)\n",
    "y_pred_2 = model_2.predict(X_test)\n",
    "y_pred_3 = model_3.predict(X_test)\n",
    "y_pred_4 = model_4.predict(X_test)\n",
    "y_pred_5 = model_5.predict(X_test)\n",
    "\n",
    "\n",
    "# Combine the predictions\n",
    "y_pred_ensemble = (y_pred_1 + y_pred_2 + y_pred_3 + y_pred_4 + y_pred_5) / 5\n",
    "\n",
    "# Calculate the root mean squared error (RMSE)\n",
    "mse = mean_squared_error(y_test, y_pred_ensemble)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "\n",
    "model_1.save('model_files/nn_model/ortho222/model_1.h5')\n",
    "model_2.save('model_files/nn_model/ortho222/model_2.h5')\n",
    "model_3.save('model_files/nn_model/ortho222/model_3.h5')\n",
    "model_4.save('model_files/nn_model/ortho222/model_4.h5')\n",
    "model_5.save('model_files/nn_model/ortho222/model_5.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d13d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_mean = r2_score(y_pred_ensemble,y_test)\n",
    "print(r2_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c8983f",
   "metadata": {},
   "source": [
    "## 2.1   CAT B: orthomm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6c8ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run functions.ipynb\n",
    "# scaler_cubic, std_train_cubic = std_data(X_train_cubic)\n",
    "# input_name = list(X_train_cubic.columns.values)\n",
    "\n",
    "# X_train_cubic, X_test_cubic, y_train_cubic, y_test_cubic = tensor_preprocessing(df_cubic)\n",
    "# X_train_tetra42m, X_test_tetra42m, y_train_tetra42m, y_test_tetra42m  = tensor_preprocessing(df_tetra42m)\n",
    "# X_train_ortho222, X_test_ortho222, y_train_ortho222, y_test_ortho222  = tensor_preprocessing(df_ortho222)\n",
    "\n",
    "# X_train_orthomm2, X_test_orthomm2, y_train_orthomm2, y_test_orthomm2  = tensor_preprocessing(df_orthomm2)\n",
    "# X_train_hextetramm, X_test_hextetramm, y_train_hextetramm, y_test_hextetramm = tensor_preprocessing(df_hextetramm)\n",
    "\n",
    "path='model_files//nn_model//orthomm2//'\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Standardize the data\n",
    "scaler_orthomm2 = StandardScaler() # RENAME here 1 places\n",
    "std_train = scaler_orthomm2.fit_transform(X_train_orthomm2) # RENAME here 1 places\n",
    "\n",
    "# std_train_cubic = pd.DataFrame(data= std_train_cubic, columns=input_name)\n",
    "\n",
    "import pickle\n",
    "pickle.dump(scaler_orthomm2, open(path+'scaler_reg.pkl','wb')) # RENAME here 2 places\n",
    "\n",
    "std_test = scaler_orthomm2.transform(X_test_orthomm2) # RENAME here 1 places\n",
    "\n",
    "\n",
    "%run functions.ipynb\n",
    "\n",
    "\n",
    "pca_orthomm2, X_train_pca_orthomm2 = pca_fs(std_train,\"plots\\\\\", title=\"a) PCA\") # RENAME here 1 places\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import pickle\n",
    "pickle.dump(pca_orthomm2, open(path+'pca_reg.pkl','wb')) # RENAME here 2 places\n",
    "\n",
    "\n",
    "X_test_pca_orthomm2 =  pca_orthomm2.transform(std_test)      # RENAME here 1 places\n",
    "\n",
    "\n",
    "y_train = y_train_orthomm2\n",
    "X_train = X_train_pca_orthomm2\n",
    "\n",
    "y_test = y_test_orthomm2\n",
    "X_test = X_test_pca_orthomm2\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "y_train = np.array(list(y_train))\n",
    "y_test = np.array(list(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5f1f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Define the individual neural network models\n",
    "def create_model_1():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(32, activation='relu', input_shape=(num_features,)))\n",
    "    model.add(tf.keras.layers.Dense(24, activation='LeakyReLU'))\n",
    "    model.add(tf.keras.layers.Dense(14, activation='PReLU'))\n",
    "    model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(4, activation='selu'))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(y_train.shape[1]))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae', 'RootMeanSquaredError'])\n",
    "    return model\n",
    "\n",
    "def create_model_2():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu', input_shape=(num_features,)))\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(y_train.shape[1]))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae', 'RootMeanSquaredError'])\n",
    "    return model\n",
    "\n",
    "def create_model_3():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(32, activation='relu', input_shape=(num_features,)))\n",
    "    model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(y_train.shape[1]))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae', 'RootMeanSquaredError'])\n",
    "    return model\n",
    "\n",
    "# Define the RNN model\n",
    "def create_RNN():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.SimpleRNN(units=64, activation='relu', return_sequences=True, input_shape=(num_features, 1)),\n",
    "        tf.keras.layers.SimpleRNN(units=32, activation='relu', return_sequences=True),\n",
    "        tf.keras.layers.SimpleRNN(units=16, activation='selu'),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(units=8, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(units=4, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=y_train.shape[1])\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae', 'RootMeanSquaredError'])    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Define the CNN model\n",
    "def create_CNN():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(num_features, 1)),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=24, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.05),\n",
    "        tf.keras.layers.Dense(units=16, activation='selu'),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(units=8, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(units=4, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=y_train.shape[1])\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae', 'RootMeanSquaredError'])    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Create instances of the individual models\n",
    "model_1 = create_model_1()\n",
    "model_2 = create_model_2()\n",
    "model_3 = create_model_3()\n",
    "model_4 = create_RNN()\n",
    "model_5 = create_CNN()\n",
    "\n",
    "\n",
    "# Train the individual models\n",
    "model_1.fit(X_train, y_train, epochs=50, batch_size=8, verbose=2, validation_split=0.1)\n",
    "model_2.fit(X_train, y_train, epochs=50, batch_size=8, verbose=2, validation_split=0.1)\n",
    "model_3.fit(X_train, y_train, epochs=50, batch_size=8, verbose=2, validation_split=0.1)\n",
    "model_4.fit(X_train, y_train, epochs=50, batch_size=8, verbose=2, validation_split=0.1)\n",
    "model_5.fit(X_train, y_train, epochs=50, batch_size=8, verbose=2, validation_split=0.1)\n",
    "\n",
    "\n",
    "# Make predictions using the individual models\n",
    "y_pred_1 = model_1.predict(X_test)\n",
    "y_pred_2 = model_2.predict(X_test)\n",
    "y_pred_3 = model_3.predict(X_test)\n",
    "y_pred_4 = model_4.predict(X_test)\n",
    "y_pred_5 = model_5.predict(X_test)\n",
    "\n",
    "\n",
    "# Combine the predictions\n",
    "y_pred_ensemble = (y_pred_1 + y_pred_2 + y_pred_3 + y_pred_4 + y_pred_5) / 5\n",
    "\n",
    "# Calculate the root mean squared error (RMSE)\n",
    "mse = mean_squared_error(y_test, y_pred_ensemble)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "\n",
    "model_1.save('model_files/nn_model/orthomm2/model_1.h5')\n",
    "model_2.save('model_files/nn_model/orthomm2/model_2.h5')\n",
    "model_3.save('model_files/nn_model/orthomm2/model_3.h5')\n",
    "model_4.save('model_files/nn_model/orthomm2/model_4.h5')\n",
    "model_5.save('model_files/nn_model/orthomm2/model_5.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a6cb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_mean = r2_score(y_pred_ensemble,y_test)\n",
    "print(r2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe78cec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "930f24f3",
   "metadata": {},
   "source": [
    "## 2.2   CAT B: hextetramm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b0b94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run functions.ipynb\n",
    "# scaler_cubic, std_train_cubic = std_data(X_train_cubic)\n",
    "# input_name = list(X_train_cubic.columns.values)\n",
    "\n",
    "# X_train_cubic, X_test_cubic, y_train_cubic, y_test_cubic = tensor_preprocessing(df_cubic)\n",
    "# X_train_tetra42m, X_test_tetra42m, y_train_tetra42m, y_test_tetra42m  = tensor_preprocessing(df_tetra42m)\n",
    "# X_train_ortho222, X_test_ortho222, y_train_ortho222, y_test_ortho222  = tensor_preprocessing(df_ortho222)\n",
    "\n",
    "# X_train_orthomm2, X_test_orthomm2, y_train_orthomm2, y_test_orthomm2  = tensor_preprocessing(df_orthomm2)\n",
    "# X_train_hextetramm, X_test_hextetramm, y_train_hextetramm, y_test_hextetramm = tensor_preprocessing(df_hextetramm)\n",
    "\n",
    "path='model_files//nn_model//hextetramm//'\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Standardize the data\n",
    "scaler_hextetramm = StandardScaler() # RENAME here 1 places\n",
    "std_train = scaler_hextetramm.fit_transform(X_train_hextetramm) # RENAME here 1 places\n",
    "\n",
    "# std_train_cubic = pd.DataFrame(data= std_train_cubic, columns=input_name)\n",
    "\n",
    "import pickle\n",
    "pickle.dump(scaler_hextetramm, open(path+'scaler_reg.pkl','wb')) # RENAME here 2 places\n",
    "\n",
    "std_test = scaler_hextetramm.transform(X_test_hextetramm) # RENAME here 1 places\n",
    "\n",
    "\n",
    "%run functions.ipynb\n",
    "\n",
    "\n",
    "pca_hextetramm, X_train_pca_hextetramm = pca_fs(std_train,\"plots\\\\\", title=\"a) PCA\") # RENAME here 1 places\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import pickle\n",
    "pickle.dump(pca_hextetramm, open(path+'pca_reg.pkl','wb')) # RENAME here 2 places\n",
    "\n",
    "X_test_pca_hextetramm =  pca_hextetramm.transform(std_test)      # RENAME here 1 places\n",
    "\n",
    "\n",
    "y_train = y_train_hextetramm\n",
    "X_train = X_train_pca_hextetramm\n",
    "\n",
    "y_test = y_test_hextetramm\n",
    "X_test = X_test_pca_hextetramm\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "y_train = np.array(list(y_train))\n",
    "y_test = np.array(list(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6768eb91",
   "metadata": {},
   "source": [
    "###### X_train[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7878edc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68268c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(33)\n",
    "tf.random.set_seed(33)\n",
    "\n",
    "\n",
    "\n",
    "# Define the individual neural network models\n",
    "def create_model_1():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(28, activation='relu', input_shape=(num_features,)))\n",
    "    model.add(tf.keras.layers.Dense(20, activation='LeakyReLU'))\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(12, activation='PReLU'))\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "\n",
    "#     model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "#     tf.keras.layers.Dropout(0.1),\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(4, activation='relu'))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(y_train.shape[1]))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss='mse', metrics=['mae', 'RootMeanSquaredError'])\n",
    "    return model\n",
    "\n",
    "def create_model_2():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(28, activation='relu', input_shape=(num_features,)))\n",
    "    model.add(tf.keras.layers.Dense(18, activation='LeakyReLU'))\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(12, activation='selu'))\n",
    "#     tf.keras.layers.Dropout(0.1),\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(8, activation='PReLU'))\n",
    "    model.add(tf.keras.layers.Dense(6, activation='relu'))\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(y_train.shape[1]))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0015), loss='mse', metrics=['mae', 'RootMeanSquaredError'])\n",
    "    return model\n",
    "\n",
    "def create_model_3():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(30, activation='relu', input_shape=(num_features,)))\n",
    "    model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    model.add(tf.keras.layers.Dense(12, activation='selu'))\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    model.add(tf.keras.layers.Dense(y_train.shape[1]))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.002), loss='mse', metrics=['mae', 'RootMeanSquaredError'])\n",
    "    return model\n",
    "\n",
    "# Define the RNN model\n",
    "def create_RNN():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.SimpleRNN(units=64, activation='relu', return_sequences=True, input_shape=(num_features, 1)),\n",
    "        tf.keras.layers.SimpleRNN(units=32, activation='relu', return_sequences=True),\n",
    "        tf.keras.layers.SimpleRNN(units=16, activation='selu'),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(units=8, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(units=4, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=y_train.shape[1])\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae', 'RootMeanSquaredError']) \n",
    "    return model\n",
    "\n",
    "\n",
    "# Define the CNN model\n",
    "def create_CNN():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(num_features, 1)),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=24, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.05),\n",
    "        tf.keras.layers.Dense(units=16, activation='selu'),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(units=8, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(units=4, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=y_train.shape[1])\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae', 'RootMeanSquaredError'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Create instances of the individual models\n",
    "model_1 = create_model_1()\n",
    "model_2 = create_model_2()\n",
    "model_3 = create_model_3()\n",
    "model_4 = create_RNN()\n",
    "model_5 = create_CNN()\n",
    "\n",
    "\n",
    "# Train the individual models\n",
    "model_1.fit(X_train, y_train, epochs=100, batch_size=16, verbose=2, validation_split=0.1, shuffle=False)\n",
    "# model_2.fit(X_train, y_train, epochs=100, batch_size=16, verbose=2, validation_split=0.1, shuffle=False)\n",
    "# model_3.fit(X_train, y_train, epochs=100, batch_size=16, verbose=2, validation_split=0.1, shuffle=False)\n",
    "# model_4.fit(X_train, y_train, epochs=100, batch_size=8, verbose=2, validation_split=0.1)\n",
    "# model_5.fit(X_train, y_train, epochs=100, batch_size=8, verbose=2, validation_split=0.1)\n",
    "\n",
    "\n",
    "# Make predictions using the individual models\n",
    "y_pred_1 = model_1.predict(X_test)\n",
    "# y_pred_2 = model_2.predict(X_test)\n",
    "# y_pred_3 = model_3.predict(X_test)\n",
    "# y_pred_4 = model_4.predict(X_test)\n",
    "# y_pred_5 = model_5.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Combine the predictions\n",
    "# y_pred_ensemble = (y_pred_1 + y_pred_2 + y_pred_3 + y_pred_4 + y_pred_5) / 5\n",
    "y_pred_ensemble = (y_pred_1 + y_pred_2 + y_pred_3) / 3\n",
    "\n",
    "# Calculate the root mean squared error (RMSE)\n",
    "mse = mean_squared_error(y_test, y_pred_ensemble)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "\n",
    "model_1.save('model_files/nn_model/hextetramm/model_1.h5')\n",
    "# model_2.save('model_files/nn_model/hextetramm/model_2.h5')\n",
    "# model_3.save('model_files/nn_model/hextetramm/model_3.h5')\n",
    "# model_4.save('model_files/nn_model/hextetramm/model_4.h5')\n",
    "# model_5.save('model_files/nn_model/hextetramm/model_5.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637805c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_mean = r2_score(y_pred_ensemble,y_test)\n",
    "print(r2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c485399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_mean = r2_score(y_pred_1,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f179c3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c157ed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_pred_2,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a332630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_pred_3,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e331d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206747e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final hyperparameter-tuning\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner import Objective\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the model architecture\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(units=hp.Int('units_1', min_value=16, max_value=64, step=4),\n",
    "                           activation=hp.Choice('activation_1', values=['relu', 'LeakyReLU', 'selu']),\n",
    "                           input_shape=(num_features,)))\n",
    "    model.add(layers.Dropout(rate=hp.Float('dropout_1', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "    model.add(layers.Dense(units=hp.Int('units_2', min_value=12, max_value=32, step=4),\n",
    "                           activation=hp.Choice('activation_2', values=['relu', 'PReLU'])))\n",
    "    model.add(layers.Dropout(rate=hp.Float('dropout_2', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "    model.add(layers.Dense(units=hp.Int('units_3', min_value=8, max_value=24, step=4),\n",
    "                           activation=hp.Choice('activation_3', values=['relu', 'PReLU'])))\n",
    "    model.add(layers.Dropout(rate=hp.Float('dropout_3', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "    model.add(layers.Dense(units=hp.Int('units_4', min_value=2, max_value=12, step=2),\n",
    "                           activation=hp.Choice('activation_4', values=['relu'])))\n",
    "    \n",
    "    model.add(layers.Dense(y_train.shape[1]))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae', 'RootMeanSquaredError'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define the tuner\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective=Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n",
    "    max_trials=20,\n",
    "    executions_per_trial=5,\n",
    "    directory='hyperparameter_tuning',\n",
    "    project_name='your_model_name')\n",
    "\n",
    "# Create a ModelCheckpoint callback to save the best models\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='best_models/model_{epoch:02d}-{val_root_mean_squared_error:.2f}.h5',\n",
    "    monitor='val_root_mean_squared_error',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "tuner.search(X_train, y_train, epochs=100, validation_split=0.1, callbacks=[checkpoint_callback])\n",
    "\n",
    "# Get the best model and print the summary\n",
    "best_model = tuner.get_best_models(num_models=3)[0]\n",
    "best_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25a15e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cdc25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(best_model.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47b6c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5e47b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f330f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "# Define the neural network model\n",
    "def create_model(optimizer='adam', activation='relu', learning_rate=0.001, batch_size=32):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(32, activation=activation, input_shape=(num_features,)))\n",
    "    model.add(tf.keras.layers.Dense(24, activation=activation))\n",
    "    model.add(tf.keras.layers.Dense(14, activation=activation))\n",
    "    model.add(tf.keras.layers.Dense(8, activation=activation))\n",
    "    model.add(tf.keras.layers.Dense(4, activation=activation))\n",
    "    model.add(tf.keras.layers.Dense(y_train.shape[1]))\n",
    "    \n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = RMSprop(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(optimizer=opt, loss='mse', metrics=['mae', 'RootMeanSquaredError'])\n",
    "    return model\n",
    "\n",
    "# Prepare your data\n",
    "# X_train and y_train are your training data\n",
    "\n",
    "# Set the number of features\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "# Wrap the Keras model into a scikit-learn compatible regressor\n",
    "regressor = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'activation': ['relu', 'LeakyReLU', 'PReLU', 'selu'],\n",
    "    'learning_rate': [0.001, 0.005, 0.01, 0.05, 0.1],\n",
    "    'batch_size': [16, 32, 64]\n",
    "}\n",
    "\n",
    "# Define the scoring metric\n",
    "scoring = {\n",
    "    'neg_mean_squared_error': 'neg_mean_squared_error',\n",
    "    'neg_mean_absolute_error': 'neg_mean_absolute_error',\n",
    "    'neg_root_mean_squared_error': 'neg_root_mean_squared_error'\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=regressor, param_grid=param_grid, scoring=scoring, refit='neg_mean_squared_error')\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and scores\n",
    "print(\"Best Hyperparameters: \", grid_result.best_params_)\n",
    "print(\"Best Negative Mean Squared Error: \", grid_result.best_score_)\n",
    "print(\"Best Negative Mean Absolute Error: \", grid_result.cv_results_['mean_test_neg_mean_absolute_error'][grid_result.best_index_])\n",
    "print(\"Best Negative Root Mean Squared Error: \", grid_result.cv_results_['mean_test_neg_root_mean_squared_error'][grid_result.best_index_])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76a15e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = create_model(optimizer='adam', activation='relu', learning_rate=0.01, batch_size=32)\n",
    "\n",
    "model_1.fit(X_train, y_train, epochs=200, batch_size=32, verbose=2, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00ccdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.evaluate(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f018c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee70d950",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1 = model_1.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54db0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b2b389",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f414f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5e807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ae35a0",
   "metadata": {},
   "source": [
    "# Pre-Processing for Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba7f0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run functions.ipynb\n",
    "X_fs = X_pca_1\n",
    "scaler_with_targets, df_train_transfer = std_data(X_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc166797",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_pca.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82783f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71735a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(list(y_train))\n",
    "y_test = np.array(list(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fcd051",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e87acad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# convert string to nested list\n",
    "data_list = ast.literal_eval(y_train)\n",
    "\n",
    "# convert list to numpy array\n",
    "data_array = np.array(data_list, dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee4f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand(2520, 3, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d3cc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand(2520, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f660948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_pca = np.array(list(X_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1a9c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a14dcec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468de684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ce4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Generate random data\n",
    "# x_train = np.random.rand(100, 30)\n",
    "# y_train = np.random.rand(100, 3, 6)\n",
    "\n",
    "x_train = X_pca\n",
    "#y_train = np.random.rand(100, 3, 6)  \n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Reshape((40, 1), input_shape=(40,)),\n",
    "    tf.keras.layers.SimpleRNN(units=64, activation='tanh'),\n",
    "    tf.keras.layers.RepeatVector(3),\n",
    "    tf.keras.layers.SimpleRNN(units=64, activation='tanh', return_sequences=True),\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=6, activation='linear'))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Generate random test input\n",
    "x_test = test_pca_1  #np.random.rand(10, 40)\n",
    "\n",
    "# Get model prediction\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Print prediction shape and values\n",
    "print(\"Prediction shape:\", y_pred.shape)\n",
    "print(\"Prediction values:\\n\", y_pred[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114bf203",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1e206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorly.metrics.regression as reg\n",
    "import numpy as np\n",
    "\n",
    "# create example data\n",
    "y_true = y_test #np.random.rand(281, 18)\n",
    "y_pred = y_pred #np.random.rand(281, 18)\n",
    "\n",
    "# calculate RMSE\n",
    "rmse = reg.RMSE(y_true, y_pred)\n",
    "mse = reg.MSE(y_true, y_pred)\n",
    "\n",
    "# print result\n",
    "print(\"RMSE:\", rmse, \"MSE:\",mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191c3e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6301ecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorly as tl\n",
    "import numpy as np\n",
    "from tensorly import unfold\n",
    "y_train = unfold(np.array(list(y_train)), 0) # mode-1 unfolding\n",
    "y_test = unfold(np.array(list(y_test)), 0) # mode-1 unfolding\n",
    "\n",
    "unfolded = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a99f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5fc778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Generate random data\n",
    "# x_train = np.random.rand(100, 30)\n",
    "# y_train = np.random.rand(100, 3, 6)\n",
    "\n",
    "x_train = X_pca\n",
    "#y_train = np.random.rand(100, 3, 6)  \n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Reshape((40, 1), input_shape=(40,)),\n",
    "    tf.keras.layers.SimpleRNN(units=64, activation='sigmoid'),\n",
    "    tf.keras.layers.RepeatVector(3),\n",
    "    tf.keras.layers.SimpleRNN(units=48, activation='relu'),\n",
    "    tf.keras.layers.RepeatVector(30),\n",
    "    tf.keras.layers.SimpleRNN(units=20, activation='tanh'),\n",
    "    tf.keras.layers.RepeatVector(3),\n",
    "    tf.keras.layers.SimpleRNN(units=64, activation='tanh', return_sequences=True),\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=6, activation='sigmoid'))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse','mae'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=50, batch_size=64, verbose=2, validation_split=0.2)\n",
    "\n",
    "# Generate random test input\n",
    "x_test = test_pca_1  #np.random.rand(10, 40)\n",
    "\n",
    "# Get model prediction\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Print prediction shape and values\n",
    "print(\"Prediction shape:\", y_pred.shape)\n",
    "print(\"Prediction values:\\n\", y_pred[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44d0647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorly.metrics.regression as reg\n",
    "import numpy as np\n",
    "\n",
    "# create example data\n",
    "y_true = y_test #np.random.rand(281, 18)\n",
    "y_pred = y_pred #np.random.rand(281, 18)\n",
    "\n",
    "# calculate RMSE\n",
    "rmse = reg.RMSE(y_true, y_pred)\n",
    "mse = reg.MSE(y_true, y_pred)\n",
    "\n",
    "# print result\n",
    "print(\"RMSE:\", rmse, \"MSE:\",mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111de418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea5ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70d6fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.GRU(units=64, return_sequences=False, input_shape=(40, 1)),\n",
    "    tf.keras.layers.RepeatVector(3),\n",
    "    tf.keras.layers.GRU(units=64, return_sequences=True),\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=6, activation='linear'))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_pca, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Generate random test input\n",
    "x_test = test_pca_1  #np.random.rand(10, 30, 1)\n",
    "\n",
    "# Get model prediction\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Print prediction shape and values\n",
    "print(\"Prediction shape:\", y_pred.shape)\n",
    "print(\"Prediction values:\\n\", y_pred[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59878cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7f8a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorly.metrics.regression as reg\n",
    "import numpy as np\n",
    "\n",
    "# create example data\n",
    "y_true = y_test #np.random.rand(281, 18)\n",
    "y_pred = y_pred #np.random.rand(281, 18)\n",
    "\n",
    "# calculate RMSE\n",
    "rmse = reg.RMSE(y_true, y_pred)\n",
    "mse = reg.MSE(y_true, y_pred)\n",
    "\n",
    "# print result\n",
    "print(\"RMSE:\", rmse, \"MSE:\",mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3591471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3f5a18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fdc642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Transfer learning\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# generate some random input and output data\n",
    "X = X_pca #np.random.random((1000, 5))\n",
    "y = y_train #y_train #np.random.random((1000, 6))  # 2x3 output array for each input\n",
    "\n",
    "#X = np.random.random((1000, 5))\n",
    "#y = np.random.random((1000, 6))  # 2x3 output array for each input\n",
    "\n",
    "#y= y.reshape(y_pred.shape[0], 2, 3)\n",
    "#print(\"x:\",X)\n",
    "#print(\"y:\",y)\n",
    "print(np.shape(X)[1])\n",
    "print(\"###############################\")\n",
    "print(np.shape(X), np.shape(y))\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=np.shape(X)[1]))\n",
    "model.add(Dense(48, activation='relu'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "model.add(Dense(48, activation='relu'))\n",
    "#model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(18, activation='sigmoid'))  # output layer with 18 nodes, corresponding to 3x6 array\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='Nadam', loss='mse', metrics=['mse','mae'])\n",
    "\n",
    "# train the model\n",
    "model.fit(X, y, epochs=20, batch_size=32, verbose=2, validation_split=0.2)#, metrics=['mse','mae','accuracy'])\n",
    "\n",
    "# evaluate the model\n",
    "loss = model.evaluate(X, y, verbose=0)\n",
    "print(f'Test loss: {loss}')\n",
    "\n",
    "# make predictions on new data\n",
    "X_new = test_pca_1 #X_test_fs #np.random.random((5, 30))\n",
    "#X_new = np.random.random((5, 5))\n",
    "\n",
    "y_pred = model.predict(X_new)\n",
    "\n",
    "# reshape the predicted output to match the shape of the true output\n",
    "#y_pred = y_pred.reshape(y_pred.shape[0], 3, 6)\n",
    "\n",
    "# print the predicted output\n",
    "print(f'Predicted output shape: {y_pred.shape}')\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b452007",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb6cf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94209d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorly.metrics.regression as reg\n",
    "import numpy as np\n",
    "\n",
    "# create example data\n",
    "y_true = y_test #np.random.rand(281, 18)\n",
    "y_pred = y_pred #np.random.rand(281, 18)\n",
    "\n",
    "# calculate RMSE\n",
    "rmse = reg.RMSE(y_true, y_pred)\n",
    "mse = reg.MSE(y_true, y_pred)\n",
    "\n",
    "# print result\n",
    "print(\"RMSE:\", rmse, \"MSE:\",mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24293a05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e742857b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f891cdac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96b1de63",
   "metadata": {},
   "source": [
    "## Data Augmentation for Transfer modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f2cd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "\n",
    "aug_batch = 200\n",
    "aug_train_shape = 40\n",
    "aug_epochs = 50\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x, y, batch_size=aug_batch, shuffle=True):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_x = self.x[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_y = self.y[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            idx = np.arange(len(self.x))\n",
    "            np.random.shuffle(idx)\n",
    "            self.x = self.x[idx]\n",
    "            self.y = self.y[idx]\n",
    "\n",
    "# Generate some sample data\n",
    "# x_train = np.random.rand(2520, 30)\n",
    "# y_train = np.random.rand(2520, 3, 6)\n",
    "\n",
    "x_train = X_pca#np.random.rand(2520, 30)\n",
    "y_train = y_train #np.random.rand(2520, 3, 6)\n",
    "\n",
    "\n",
    "# Define the data augmentation parameters\n",
    "datagen = DataGenerator(\n",
    "    x_train, y_train,\n",
    "    batch_size=aug_batch,\n",
    "    shuffle=True)\n",
    "\n",
    "# Set the batch size and number of epochs\n",
    "\n",
    "\n",
    "# Create a new array to hold the augmented data\n",
    "aug_x_train = np.zeros((batch_size * aug_epochs, aug_train_shape))\n",
    "aug_y_train = np.zeros((batch_size * aug_epochs, 3, 6))\n",
    "\n",
    "# Generate augmented data\n",
    "train_generator = DataGenerator(x_train, y_train, batch_size=aug_batch, shuffle=True)\n",
    "aug_x_train, aug_y_train = train_generator.__getitem__(0)\n",
    "for i in range(1, epochs):\n",
    "    x_batch, y_batch = train_generator.__getitem__(i)\n",
    "    aug_x_train = np.vstack((aug_x_train, x_batch))\n",
    "    aug_y_train = np.vstack((aug_y_train, y_batch))\n",
    "\n",
    "# Print the shape of the augmented data\n",
    "print(\"Augmented data shapes:\")\n",
    "print(aug_x_train.shape)\n",
    "print(aug_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558d109b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac308a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using this to develop dataframe for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ff0ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_values =[]\n",
    "max_values =[]\n",
    "\n",
    "df_train_transfer = X_pca\n",
    "\n",
    "num_rows = 10000\n",
    "num_cols = len(X_pca.columns)\n",
    "\n",
    "print(len(X_pca.columns))\n",
    "for items in range(len(X_pca.columns)):\n",
    "    #print(items, input_name[items])\n",
    "    #print(df_train_transfer.iloc[:,[items]].min())\n",
    "    low = df_train_transfer.iloc[:,[items]].min().values\n",
    "    high = df_train_transfer.iloc[:,[items]].max().values\n",
    "    \n",
    "    low = low[0]\n",
    "    high = high[0]\n",
    "    #print(items)\n",
    "    #print(low,high)\n",
    "    min_values.append(low)\n",
    "    max_values.append(high)\n",
    "    \n",
    "data = np.random.uniform(low=min_values, high=max_values, size=(num_rows, num_cols))\n",
    "\n",
    "#df = pd.DataFrame(data, columns=column_name_transfer)\n",
    "df = pd.DataFrame(data)\n",
    "X_transfer = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3368ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transfer.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e89a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2becfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5a62016",
   "metadata": {},
   "source": [
    "## Bootstaping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee41586",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To generate 10000 new datasets with the same relationships between elements within the 6 columns \n",
    "# of original 2520x6 numpy array, use a technique called bootstrapping.\n",
    "# Bootstrapping involves randomly resampling the original data with replacement to create new datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c3f2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# your original 2520x6 numpy array\n",
    "original_data = y_train #np.random.rand(2520, 6)\n",
    "\n",
    "# number of datasets to generate\n",
    "num_datasets = 10000\n",
    "\n",
    "# create empty array to store new datasets\n",
    "new_datasets = np.empty((num_datasets, 18))\n",
    "\n",
    "# loop through number of datasets\n",
    "for i in range(num_datasets):\n",
    "    # randomly resample the original data with replacement along the first axis\n",
    "    resampled_data = np.random.choice(original_data.shape[0], size=original_data.shape[0], replace=True)\n",
    "    # use the resampled indices to select rows from the original data along the first axis\n",
    "    resampled_data = original_data[resampled_data]\n",
    "    # calculate the means of each column of the resampled data\n",
    "    means = np.mean(resampled_data, axis=0)\n",
    "    # calculate the standard deviations of each column of the resampled data\n",
    "    stds = np.std(resampled_data, axis=0)\n",
    "    # normalize the resampled data using the means and standard deviations\n",
    "    normalized_data = (resampled_data - means) / stds\n",
    "    # calculate the means of each column of the normalized data\n",
    "    new_means = np.mean(normalized_data, axis=0)\n",
    "    # calculate the standard deviations of each column of the normalized data\n",
    "    new_stds = np.std(normalized_data, axis=0)\n",
    "    # denormalize the normalized data using the new means and standard deviations\n",
    "    denormalized_data = (normalized_data * new_stds) + new_means\n",
    "    # add denormalized data to new_datasets array\n",
    "    new_datasets[i] = denormalized_data[0]\n",
    "y_transfer = new_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496d4845",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b93e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.rcParams.update({'font.size':18})\n",
    "#plt.rcParams.update({'font.size':22})\n",
    "plt.rcParams[\"figure.figsize\"]=(12, 10)\n",
    "#The above is same as\n",
    "# matplotlib.rcParams.update({'font.size':22})\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense,LeakyReLU, PReLU\n",
    "#from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow.python.keras.initializers import Constant\n",
    "#from tensorflow.python.keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d536f95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#https://stackoverflow.com/questions/24645153/pandas-dataframe-columns-scaling-with-sklearn\n",
    "scaler_mm=MinMaxScaler()\n",
    "Xscaled=scaler_mm.fit_transform(X_transfer)\n",
    "Yscaled=scaler_mm.fit_transform(y_transfer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5951495",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing things into train and test\n",
    "#X_train, X_test, y_train, y_test = train_test_split(Xscaled, Yscaled)\n",
    "\n",
    "#Keras model configuration for NN API\n",
    "model_p = Sequential()\n",
    "#model.add(Dense(6, input_dim=5, kernel_initializer='normal', activation='relu'))\n",
    "model_p.add(Dense(30, input_dim=30, kernel_initializer='normal', activation='relu'))\n",
    "#model.add(Dense(3, activation='relu'))\n",
    "#model.add(Dense(6, activation='LeakyReLU'))\n",
    "#model.add(Dense(4, activation='tanh'))\n",
    "#model.add(Dense(10, activation='LeakyReLU'))\n",
    "#model.add(Dense(3, activation='tanh'))\n",
    "#model.add(Dense(1, activation='linear'))\n",
    "#model.add(Dense(8, activation='relu'))\n",
    "model_p.add(Dense(24, activation='relu'))\n",
    "model_p.add(Dense(20, activation='relu'))\n",
    "\n",
    "model_p.add(Dense(2, activation='relu'))\n",
    "\n",
    "model_p.add(Dense(6, activation='relu'))\n",
    "model_p.add(Dense(12, activation='relu'))\n",
    "\n",
    "model_p.add(Dense(18, activation='sigmoid'))\n",
    "model_p.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967fb350",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.optimizers import Adam\n",
    "# https://stackoverflow.com/questions/62707558/importerror-cannot-import-name-adam-from-keras-optimizers\n",
    "# https://exerror.com/importerror-cannot-import-name-adam-from-keras-optimizers-error/\n",
    "# https://stackoverflow.com/questions/50056356/could-not-interpret-optimizer-identifier-error-in-keras\n",
    "# https://towardsdatascience.com/how-to-optimize-learning-rate-with-tensorflow-its-easier-than-you-think-164f980a7c7b\n",
    "#Error MSE and MAE- compiling\n",
    "#model.compile(loss='mse', optimizer='adam', metrics=['mse','mae','accuracy'])\n",
    "from tensorflow.python.keras.optimizers import adam_v2\n",
    "learning_rate=2.0E-02 #2.50E-04 #0.0015\n",
    "opt=adam_v2.Adam(learning_rate = learning_rate)\n",
    "#opt=Adam(learning_rate = learning_rate)\n",
    "#ValueError: Could not interpret optimizer identifier: <keras.optimizer_v2.adam.Adam object at 0x7fc9ec1a7d50>\n",
    "#opt=tensorflow.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "#opt=tf.optimizers.Adam(lr = learning_rate, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0., amsgrad = False)\n",
    "#Error MSE and MAE- compiling\n",
    "model_p.compile(loss='mse', optimizer=opt, metrics=['mse','mae','mape','accuracy']) # no apostrophe ' ' for optimizers defined by user \n",
    "#Fitting \n",
    "#history = model.fit(X_train, y_train, epochs=150, batch_size=50,  verbose=1, validation_split=0.2)\n",
    "#history = model.fit(X_train, y_train, epochs=200, batch_size=16,  verbose=1, validation_split=0.2)\n",
    "\n",
    "history = model_p.fit(Xscaled, Yscaled, epochs=50, batch_size=20,  verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1355ce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print and plot using matplotlib\n",
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'], color='k', marker='o')\n",
    "#plt.plot(history.history['val_loss'],color='r', marker='s') #no validation\n",
    "plt.title('model loss')\n",
    "plt.ylabel('mse (knmps^2)') #plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "#plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.legend(['train data', 'validation data'], loc='upper right')\n",
    "plt.show()\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "#plt.plot(history.history['val_accuracy']) #no validation\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train data', 'validation data'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b3883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################saving the loss history###############################\n",
    "#####################Prediction Pure Sn-40W###############################\n",
    "#####Reference############################################################\n",
    "##### https://stackoverflow.com/questions/38445982/how-to-log-keras-loss-output-to-a-file\n",
    "#####\n",
    "train_loss = history.history['loss']\n",
    "#val_loss   = history.history['val_loss']\n",
    "acc = history.history['accuracy']\n",
    "#val_acc = history.history['val_accuracy']\n",
    "############################################################################\n",
    "loss_history = np.array(train_loss)\n",
    "np.savetxt(\"train_loss_pretrained.txt\", loss_history, delimiter=\",\")\n",
    "#val_loss_history = np.array(val_loss)\n",
    "#np.savetxt(\"val_loss.txt\", val_loss_history, delimiter=\",\")\n",
    "############################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf4f3a0",
   "metadata": {},
   "source": [
    "# Transfering weight of psedo-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d1da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Innovation in transfer learning\n",
    "# Freeze the layers of the first model\n",
    "for layer in model_p.layers:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4138d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras model configuration for NN API\n",
    "model = Sequential()\n",
    "#model.add(Dense(6, input_dim=5, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(30, input_dim=30, kernel_initializer='normal', activation='relu'))\n",
    "#model.add(Dense(3, activation='relu'))\n",
    "#model.add(Dense(6, activation='LeakyReLU'))\n",
    "#model.add(Dense(4, activation='tanh'))\n",
    "#model.add(Dense(10, activation='LeakyReLU'))\n",
    "#model.add(Dense(3, activation='tanh'))\n",
    "#model.add(Dense(1, activation='linear'))\n",
    "#model.add(Dense(8, activation='relu'))\n",
    "\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "\n",
    "model.add(Dense(2, activation='relu'))\n",
    "\n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(18, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aade2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the weights of the second model to be the same as the first model (pretrained model)\n",
    "model.set_weights(model_p.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b70f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.optimizers import Adam\n",
    "# https://stackoverflow.com/questions/62707558/importerror-cannot-import-name-adam-from-keras-optimizers\n",
    "# https://exerror.com/importerror-cannot-import-name-adam-from-keras-optimizers-error/\n",
    "# https://stackoverflow.com/questions/50056356/could-not-interpret-optimizer-identifier-error-in-keras\n",
    "# https://towardsdatascience.com/how-to-optimize-learning-rate-with-tensorflow-its-easier-than-you-think-164f980a7c7b\n",
    "#Error MSE and MAE- compiling\n",
    "#model.compile(loss='mse', optimizer='adam', metrics=['mse','mae','accuracy'])\n",
    "from tensorflow.python.keras.optimizers import adam_v2\n",
    "learning_rate=2.0E-02 #2.50E-04 #0.0015\n",
    "opt=adam_v2.Adam(learning_rate = learning_rate)\n",
    "#opt=Adam(learning_rate = learning_rate)\n",
    "#ValueError: Could not interpret optimizer identifier: <keras.optimizer_v2.adam.Adam object at 0x7fc9ec1a7d50>\n",
    "#opt=tensorflow.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "#opt=tf.optimizers.Adam(lr = learning_rate, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0., amsgrad = False)\n",
    "#Error MSE and MAE- compiling\n",
    "model.compile(loss='mse', optimizer=opt, metrics=['mse','mae','accuracy']) # no apostrophe ' ' for optimizers defined by user \n",
    "#Fitting \n",
    "#history = model.fit(X_train, y_train, epochs=150, batch_size=50,  verbose=1, validation_split=0.2)\n",
    "history = model.fit(X_fs, y_train, epochs=20, batch_size=16,  verbose=1, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01593e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print and plot using matplotlib\n",
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'], color='k', marker='o')\n",
    "plt.plot(history.history['val_loss'],color='r', marker='s')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('mse (knmps^2)') #plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "#plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.legend(['train data', 'validation data'], loc='upper right')\n",
    "plt.show()\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train data', 'validation data'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bde44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################saving the loss history###############################\n",
    "#####################Prediction Pure Sn-40W###############################\n",
    "#####Reference############################################################\n",
    "##### https://stackoverflow.com/questions/38445982/how-to-log-keras-loss-output-to-a-file\n",
    "#####\n",
    "train_loss = history.history['loss']\n",
    "val_loss   = history.history['val_loss']\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "############################################################################\n",
    "loss_history = np.array(train_loss)\n",
    "np.savetxt(\"train_loss.txt\", loss_history, delimiter=\",\")\n",
    "val_loss_history = np.array(val_loss)\n",
    "np.savetxt(\"val_loss.txt\", val_loss_history, delimiter=\",\")\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bb68bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "future=model.save('annsigma2p0e-2.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff2e5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = model.evaluate(X_test_fs, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79588bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_datasets = model.predict(X_test_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947f2296",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbe1596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorly.metrics.regression as reg\n",
    "import numpy as np\n",
    "\n",
    "# create example data\n",
    "y_true = y_test #np.random.rand(281, 18)\n",
    "y_pred = predictions_datasets #np.random.rand(281, 18)\n",
    "\n",
    "# calculate RMSE\n",
    "rmse = reg.RMSE(y_true, y_pred)\n",
    "mse = reg.MSE(y_true, y_pred)\n",
    "\n",
    "# print result\n",
    "print(\"RMSE:\", rmse, \"MSE:\",mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedb6911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create heatmap plot with adjusted aspect ratio\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(np.vstack([y_true, y_pred]), cmap='rainbow',extent=[0, 20, 0, 20], aspect='auto',vmin=0.00001, vmax=0.001)\n",
    "ax.set_xlabel('y_true')\n",
    "ax.set_ylabel('y_pred')\n",
    "ax.set_title('Heatmap of y_true and y_pred for All Data Points\\n')\n",
    "fig.colorbar(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2e9be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4986fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3db995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Transfer learning\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# generate some random input and output data\n",
    "X = X_fs #np.random.random((1000, 5))\n",
    "y = y_train #y_train #np.random.random((1000, 6))  # 2x3 output array for each input\n",
    "\n",
    "#X = np.random.random((1000, 5))\n",
    "#y = np.random.random((1000, 6))  # 2x3 output array for each input\n",
    "\n",
    "#y= y.reshape(y_pred.shape[0], 2, 3)\n",
    "#print(\"x:\",X)\n",
    "#print(\"y:\",y)\n",
    "print(np.shape(X)[1])\n",
    "print(\"###############################\")\n",
    "print(np.shape(X), np.shape(y))\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=np.shape(X)[1]))\n",
    "model.add(Dense(48, activation='relu'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "model.add(Dense(48, activation='relu'))\n",
    "#model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(18, activation='sigmoid'))  # output layer with 18 nodes, corresponding to 3x6 array\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='Nadam', loss='mse', metrics=['mse','mae'])\n",
    "\n",
    "# train the model\n",
    "model.fit(X, y, epochs=20, batch_size=32, verbose=2, validation_split=0.2)#, metrics=['mse','mae','accuracy'])\n",
    "\n",
    "# evaluate the model\n",
    "loss = model.evaluate(X, y, verbose=0)\n",
    "print(f'Test loss: {loss}')\n",
    "\n",
    "# make predictions on new data\n",
    "X_new = X_test_fs #np.random.random((5, 30))\n",
    "#X_new = np.random.random((5, 5))\n",
    "\n",
    "y_pred = model.predict(X_new)\n",
    "\n",
    "# reshape the predicted output to match the shape of the true output\n",
    "#y_pred = y_pred.reshape(y_pred.shape[0], 3, 6)\n",
    "\n",
    "# print the predicted output\n",
    "print(f'Predicted output shape: {y_pred.shape}')\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c99cf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorly.metrics.regression as reg\n",
    "import numpy as np\n",
    "\n",
    "# create example data\n",
    "y_true = y_test #np.random.rand(281, 18)\n",
    "y_pred = y_pred #predictions_datasets #np.random.rand(281, 18)\n",
    "\n",
    "# calculate RMSE\n",
    "rmse = reg.RMSE(y_true, y_pred)\n",
    "mse = reg.MSE(y_true, y_pred)\n",
    "\n",
    "# print result\n",
    "print(\"RMSE:\", rmse, \"MSE:\",mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6a4c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f47417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this plot for data splitted\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import matplotlib.ticker as tck\n",
    "\n",
    "data = y_train#np.random.rand(2520, 18)\n",
    "\n",
    "plt.rc('font', size=18)\n",
    "\n",
    "# create plot\n",
    "fig = plt.figure(figsize=(12, 14))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# create colormap\n",
    "#norm = plt.Normalize(np.array(data).min(), np.array(data).max())\n",
    "norm = None #plt.Normalize(np.array(data).min(), np.array(data).max())\n",
    "cmap = plt.cm.gnuplot #Set1#nipy_spectral#prism#gist_rainbow#cubehelix #gist_stern #CMRmap # gnuplot#hsv #twilight#magma #cividis#inferno #ocean#prism #jet #nipy_spectral #CMRmap#gnuplot2 #plasma\n",
    "\n",
    "# create ScalarMappable object based on frequency values\n",
    "mappable = ScalarMappable(cmap=cmap, norm=norm)\n",
    "mappable.set_array(data)\n",
    "\n",
    "# plot surface with colors based on frequency values\n",
    "x, y = np.meshgrid(range(2520), range(18))\n",
    "z = data.T\n",
    "\n",
    "ax.plot_surface(x, y, z, cmap=cmap, alpha=0.8, linewidth=0.01, rstride=1, cstride=1)\n",
    "\n",
    "# add colorbar\n",
    "clb = fig.colorbar(mappable, ax=ax,  shrink=0.35, pad = -1.120)\n",
    "clb.ax.tick_params(labelsize=16) \n",
    "#clb.set_label(r'$\\alpha$', rotation=0)\n",
    "# Label and coordinate\n",
    "#ax.text(5, 10,10 , r\"$\\alpha$\", color='red', fontsize=12)\n",
    "ax.text(0, -1.1*max(ax.get_ybound()),1.1*max(ax.get_zbound()), r\"$\\alpha ^{  n}_{ij}$\", color='red', fontsize=18)\n",
    "\n",
    "\n",
    "# set labels and title\n",
    "ax.set_xlabel('Samples (k)', labelpad=16, fontsize=24,color='r')\n",
    "ax.set_ylabel('[i x j]', labelpad=20, fontsize=24,color='r')\n",
    "#ax.set_zlabel('Frequency', labelpad=8, fontsize=20,color='r')\n",
    "ax.set_title('Train tensors')\n",
    "ax.margins(x=0.05, y=-0.4)\n",
    "\n",
    "#ax.set_xticks(np.linspace(0, 6, 4))\n",
    "#ax.set_xticklabels(['1', '2', '6','2'])\n",
    "\n",
    "ax.yaxis.set_major_locator(MaxNLocator(4))\n",
    "#ax.set_xticks(np.linspace(0, 18, 4))\n",
    "ax.set_yticklabels(['1','6','12','18'])\n",
    "\n",
    "ax.xaxis.set_major_locator(MaxNLocator(4))\n",
    "ax.xaxis.set_minor_locator(tck.AutoMinorLocator(2))\n",
    "\n",
    "\n",
    "ax.zaxis.set_major_locator(MaxNLocator(1))\n",
    "ax.get_zaxis().set_visible(False)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), fontsize=18)\n",
    "#ax.set_zticklabels(ax.get_zticklabels(), fontsize=18)\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "# set camera angle and distance\n",
    "#ax.view_init(elev=20, azim=-40)\n",
    "ax.view_init(elev=18, azim=-60)\n",
    "#ax.dist=10\n",
    "\n",
    "# set background color and grid\n",
    "ax.xaxis.pane.fill = False\n",
    "ax.yaxis.pane.fill = False\n",
    "ax.zaxis.pane.fill = False\n",
    "ax.xaxis.pane.set_edgecolor('white')\n",
    "ax.yaxis.pane.set_edgecolor('white')\n",
    "ax.zaxis.pane.set_edgecolor('white')\n",
    "ax.grid(False)\n",
    "\n",
    "ax.set_zticks([]) # Remove the tick labels on the z-axis\n",
    "ax.set_zticklabels([])\n",
    "\n",
    "ax.zaxis.line.set_color((1.0, 1.0, 1.0, 0.0)) # Set the z-axis line color to transparent\n",
    "\n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c7447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this plot for data splitted\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import matplotlib.ticker as tck\n",
    "\n",
    "data = y_test#np.random.rand(2520, 18)\n",
    "\n",
    "plt.rc('font', size=18)\n",
    "\n",
    "# create plot\n",
    "fig = plt.figure(figsize=(12, 14))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# create colormap\n",
    "#norm = plt.Normalize(np.array(data).min(), np.array(data).max())\n",
    "norm = None #plt.Normalize(np.array(data).min(), np.array(data).max())\n",
    "cmap = plt.cm.gnuplot #Set1#nipy_spectral#prism#gist_rainbow#cubehelix #gist_stern #CMRmap # gnuplot#hsv #twilight#magma #cividis#inferno #ocean#prism #jet #nipy_spectral #CMRmap#gnuplot2 #plasma\n",
    "\n",
    "# create ScalarMappable object based on frequency values\n",
    "mappable = ScalarMappable(cmap=cmap, norm=norm)\n",
    "mappable.set_array(data)\n",
    "\n",
    "# plot surface with colors based on frequency values\n",
    "x, y = np.meshgrid(range(281), range(18))\n",
    "z = data.T\n",
    "\n",
    "ax.plot_surface(x, y, z, cmap=cmap, alpha=0.8, linewidth=0.01, rstride=1, cstride=1)\n",
    "\n",
    "# add colorbar\n",
    "clb = fig.colorbar(mappable, ax=ax,  shrink=0.35, pad = -1.120)\n",
    "clb.ax.tick_params(labelsize=16) \n",
    "#clb.set_label(r'$\\alpha$', rotation=0)\n",
    "# Label and coordinate\n",
    "#ax.text(5, 10,10 , r\"$\\alpha$\", color='red', fontsize=12)\n",
    "ax.text(0, -1.1*max(ax.get_ybound()),1.1*max(ax.get_zbound()), r\"$\\alpha ^{  n}_{ij}$\", color='red', fontsize=18)\n",
    "\n",
    "\n",
    "# set labels and title\n",
    "ax.set_xlabel('Samples (k)', labelpad=16, fontsize=24,color='r')\n",
    "ax.set_ylabel('[i x j]', labelpad=20, fontsize=24,color='r')\n",
    "#ax.set_zlabel('Frequency', labelpad=8, fontsize=20,color='r')\n",
    "ax.set_title('Test tensors')\n",
    "ax.margins(x=0.05, y=-0.4)\n",
    "\n",
    "#ax.set_xticks(np.linspace(0, 6, 4))\n",
    "#ax.set_xticklabels(['1', '2', '6','2'])\n",
    "\n",
    "ax.yaxis.set_major_locator(MaxNLocator(4))\n",
    "#ax.set_xticks(np.linspace(0, 18, 4))\n",
    "ax.set_yticklabels(['1','6','12','18'])\n",
    "\n",
    "ax.xaxis.set_major_locator(MaxNLocator(4))\n",
    "ax.xaxis.set_minor_locator(tck.AutoMinorLocator(2))\n",
    "\n",
    "\n",
    "ax.zaxis.set_major_locator(MaxNLocator(1))\n",
    "ax.get_zaxis().set_visible(False)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), fontsize=18)\n",
    "#ax.set_zticklabels(ax.get_zticklabels(), fontsize=18)\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "# set camera angle and distance\n",
    "#ax.view_init(elev=20, azim=-40)\n",
    "ax.view_init(elev=18, azim=-60)\n",
    "#ax.dist=10\n",
    "\n",
    "# set background color and grid\n",
    "ax.xaxis.pane.fill = False\n",
    "ax.yaxis.pane.fill = False\n",
    "ax.zaxis.pane.fill = False\n",
    "ax.xaxis.pane.set_edgecolor('white')\n",
    "ax.yaxis.pane.set_edgecolor('white')\n",
    "ax.zaxis.pane.set_edgecolor('white')\n",
    "ax.grid(False)\n",
    "\n",
    "ax.set_zticks([]) # Remove the tick labels on the z-axis\n",
    "ax.set_zticklabels([])\n",
    "\n",
    "ax.zaxis.line.set_color((1.0, 1.0, 1.0, 0.0)) # Set the z-axis line color to transparent\n",
    "\n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee619a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this plot for data splitted\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import matplotlib.ticker as tck\n",
    "\n",
    "data = y_test#np.random.rand(2520, 18)\n",
    "\n",
    "plt.rc('font', size=18)\n",
    "\n",
    "# create plot\n",
    "fig = plt.figure(figsize=(12, 14))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# create colormap\n",
    "#norm = plt.Normalize(np.array(data).min(), np.array(data).max())\n",
    "norm = None #plt.Normalize(np.array(data).min(), np.array(data).max())\n",
    "cmap = plt.cm.gnuplot #Set1#nipy_spectral#prism#gist_rainbow#cubehelix #gist_stern #CMRmap # gnuplot#hsv #twilight#magma #cividis#inferno #ocean#prism #jet #nipy_spectral #CMRmap#gnuplot2 #plasma\n",
    "\n",
    "# create ScalarMappable object based on frequency values\n",
    "mappable = ScalarMappable(cmap=cmap, norm=norm)\n",
    "mappable.set_array(data)\n",
    "\n",
    "# plot surface with colors based on frequency values\n",
    "x, y = np.meshgrid(range(2520), range(18))\n",
    "z = data.T\n",
    "\n",
    "ax.plot_surface(x, y, z, cmap=cmap, alpha=0.8, linewidth=0.01, rstride=1, cstride=1)\n",
    "\n",
    "# add colorbar\n",
    "clb = fig.colorbar(mappable, ax=ax,  shrink=0.35, pad = -1.120)\n",
    "clb.ax.tick_params(labelsize=16) \n",
    "clb.set_label(r'$\\alpha$', rotation=0)\n",
    "# Label and coordinate\n",
    "#ax.text(5, 10,10 , r\"$\\alpha$\", color='red', fontsize=12)\n",
    "ax.text(0, -1.1*max(ax.get_ybound()),1.1*max(ax.get_zbound()), r\"$\\alpha ^{  n}_{ij}$\", color='red', fontsize=18)\n",
    "\n",
    "\n",
    "# set labels and title\n",
    "ax.set_xlabel('Columns', labelpad=12, fontsize=20,color='r')\n",
    "ax.set_ylabel('Rows x N', labelpad=20, fontsize=20,color='r')\n",
    "#ax.set_zlabel('Frequency', labelpad=8, fontsize=20,color='r')\n",
    "ax.set_title('title')\n",
    "ax.margins(x=0.05, y=-0.4)\n",
    "\n",
    "#ax.set_xticks(np.linspace(0, 6, 4))\n",
    "#ax.set_xticklabels(['1', '2', '6','2'])\n",
    "\n",
    "ax.yaxis.set_major_locator(MaxNLocator(4))\n",
    "#ax.set_xticks(np.linspace(0, 18, 4))\n",
    "ax.set_yticklabels(['1','6','12','18'])\n",
    "\n",
    "ax.xaxis.set_major_locator(MaxNLocator(4))\n",
    "ax.xaxis.set_minor_locator(tck.AutoMinorLocator(2))\n",
    "\n",
    "\n",
    "ax.zaxis.set_major_locator(MaxNLocator(1))\n",
    "ax.get_zaxis().set_visible(False)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), fontsize=18)\n",
    "#ax.set_zticklabels(ax.get_zticklabels(), fontsize=18)\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "# set camera angle and distance\n",
    "#ax.view_init(elev=20, azim=-40)\n",
    "ax.view_init(elev=18, azim=-60)\n",
    "#ax.dist=10\n",
    "\n",
    "# set background color and grid\n",
    "ax.xaxis.pane.fill = False\n",
    "ax.yaxis.pane.fill = False\n",
    "ax.zaxis.pane.fill = False\n",
    "ax.xaxis.pane.set_edgecolor('white')\n",
    "ax.yaxis.pane.set_edgecolor('white')\n",
    "ax.zaxis.pane.set_edgecolor('white')\n",
    "ax.grid(False)\n",
    "\n",
    "ax.set_zticks([]) # Remove the tick labels on the z-axis\n",
    "ax.set_zticklabels([])\n",
    "\n",
    "ax.zaxis.line.set_color((1.0, 1.0, 1.0, 0.0)) # Set the z-axis line color to transparent\n",
    "\n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401523b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33eca13",
   "metadata": {},
   "source": [
    "## Below here the codes is not integrated to fit this new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6c904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this plot for data splitted\n",
    "# This codes actually works\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import matplotlib.ticker as tck\n",
    "\n",
    "data = y_train#np.random.rand(2520, 18)\n",
    "\n",
    "plt.rc('font', size=18)\n",
    "\n",
    "# create plot\n",
    "fig = plt.figure(figsize=(12, 14))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# create colormap\n",
    "norm = plt.Normalize(np.array(data).min(), np.array(data).max())\n",
    "cmap = plt.cm.Set1#nipy_spectral#prism#gist_rainbow#cubehelix #gist_stern #CMRmap # gnuplot#hsv #twilight#magma #cividis#inferno #ocean#prism #jet #nipy_spectral #CMRmap#gnuplot2 #plasma\n",
    "\n",
    "# create ScalarMappable object based on frequency values\n",
    "mappable = ScalarMappable(cmap=cmap, norm=norm)\n",
    "mappable.set_array(data)\n",
    "\n",
    "# plot surface with colors based on frequency values\n",
    "x, y = np.meshgrid(range(2520), range(18))\n",
    "z = data.T\n",
    "\n",
    "ax.plot_surface(x, y, z, cmap=cmap, alpha=0.9, linewidth=0.01, rstride=1, cstride=1)\n",
    "\n",
    "# add colorbar\n",
    "clb = fig.colorbar(mappable, ax=ax,  shrink=0.25, pad = -0.25)\n",
    "clb.ax.tick_params(labelsize=16) \n",
    "clb.set_label(r'$\\alpha$', rotation=0)\n",
    "# Label and coordinate\n",
    "#ax.text(5, 10,10 , r\"$\\alpha$\", color='red', fontsize=12)\n",
    "ax.text(4, max(ax.get_ybound()),1.1*max(ax.get_zbound()), r\"$\\alpha ^{  n}_{ij}$\", color='red', fontsize=18)\n",
    "\n",
    "\n",
    "# set labels and title\n",
    "ax.set_xlabel('Columns', labelpad=12, fontsize=20,color='r')\n",
    "ax.set_ylabel('Rows x N', labelpad=20, fontsize=20,color='r')\n",
    "#ax.set_zlabel('Frequency', labelpad=8, fontsize=20,color='r')\n",
    "ax.set_title('title')\n",
    "ax.margins(x=0, y=-0.25)\n",
    "\n",
    "#ax.set_xticks(np.linspace(0, 6, 4))\n",
    "#ax.set_xticklabels(['1', '2', '6','2'])\n",
    "\n",
    "ax.yaxis.set_major_locator(MaxNLocator(4))\n",
    "#ax.set_xticks(np.linspace(0, 18, 4))\n",
    "ax.set_yticklabels(['1','6','12','18'])\n",
    "\n",
    "ax.xaxis.set_major_locator(MaxNLocator(4))\n",
    "ax.xaxis.set_minor_locator(tck.AutoMinorLocator(2))\n",
    "\n",
    "\n",
    "ax.zaxis.set_major_locator(MaxNLocator(1))\n",
    "ax.get_zaxis().set_visible(False)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), fontsize=18)\n",
    "#ax.set_zticklabels(ax.get_zticklabels(), fontsize=18)\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "# set camera angle and distance\n",
    "#ax.view_init(elev=20, azim=-40)\n",
    "ax.view_init(elev=15, azim=-60)\n",
    "#ax.dist=12\n",
    "\n",
    "# set background color and grid\n",
    "ax.xaxis.pane.fill = False\n",
    "ax.yaxis.pane.fill = False\n",
    "ax.zaxis.pane.fill = False\n",
    "ax.xaxis.pane.set_edgecolor('white')\n",
    "ax.yaxis.pane.set_edgecolor('white')\n",
    "ax.zaxis.pane.set_edgecolor('white')\n",
    "ax.grid(False)\n",
    "\n",
    "ax.set_zticks([]) # Remove the tick labels on the z-axis\n",
    "ax.set_zticklabels([])\n",
    "\n",
    "ax.zaxis.line.set_color((1.0, 1.0, 1.0, 0.0)) # Set the z-axis line color to transparent\n",
    "\n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a28e657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a445241",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run functions.ipynb\n",
    "r2_plot(model,X_test_fs,y_test,\"name\",\"model_name\",plot_path=\"plots\\\\_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea35870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e5ded21",
   "metadata": {},
   "source": [
    "# Hyper Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaae698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%run functions.ipynb\n",
    "# For Learning rates, weight constraint and init_weights\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "#from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "\n",
    "input_dim = df_fs.shape[1]\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "# \n",
    "epochs = 150\n",
    "batch_size = [4,6]\n",
    "layers = [4,6,8]\n",
    "neuron_size = [24,48]\n",
    "optimizer = ['RMSprop','Adam', 'Nadam']\n",
    "activation = ['relu', 'selu']\n",
    "drops = [0.05,0.075, 0.1]\n",
    "learning_rate = [0.00025,0.0005,0.00075,0.001]\n",
    "weight_constraint = [2]\n",
    "init_weights = ['he_uniform']\n",
    "\n",
    "# Complete set of hyper-parameter\n",
    "\n",
    "#batch_size = [2,4,6]\n",
    "#layers = [4,6]\n",
    "#neuron_size = [32,48,64]\n",
    "#optimizer = ['SGD','RMSprop', 'Adam', 'Nadam']\n",
    "#activation = ['relu', 'selu', 'LeakyReLU', 'PReLU']\n",
    "#drops = [0.05,0.075,0.08,0.1,0.15,0.2]\n",
    "#learning_rate = [0.0001,0.00025,0.000375,0.0004,0.0005,0.00075,0.001,0.005,0.01]\n",
    "\n",
    "#For weight initialization\n",
    "#weight_constraint = [1,2,3]\n",
    "#init_weights = ['uniform', 'normal', 'he_uniform']\n",
    "\n",
    "\n",
    "model = KerasRegressor(build_fn= create_model, \n",
    "                        epochs= epochs, \n",
    "                        batch_size = batch_size, \n",
    "                        verbose=2)\n",
    "\n",
    "\n",
    "# Make dictionary of learning rate and momuntum grid\n",
    "\n",
    "param_grid = dict( batch_size = batch_size, lyrs=layers, neuron_size = neuron_size, opt=optimizer,act=activation, dr=drops, learning_rate = learning_rate,init_weights=init_weights, weight_constraint = weight_constraint)\n",
    "grid = RandomizedSearchCV(estimator= model, param_distributions=param_grid,n_iter=30, n_jobs=-1, cv=10, scoring='r2', verbose=2)\n",
    "\n",
    "grid_result = grid.fit(df_fs, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"Mean = %f (std=%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e45374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abae5e4",
   "metadata": {},
   "source": [
    "# Model Development without transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647d0816",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run functions.ipynb\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "# Run model with 90/10 datasets with optimized parameters\n",
    "input_dim = df_scc[top_30_features].shape[1]\n",
    "print(input_dim)\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "#from keras_adabound import AdaBound\n",
    "from keras.activations import elu\n",
    "from tensorflow.keras.activations import selu\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "from keras.constraints import maxnorm   # To setup weight constraints\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "max_epochs = 10\n",
    "np.random.seed(42)\n",
    "#callbacks = [EarlyStopping(monitor='val_mape', mode='min', restore_best_weights=True, patience=10, verbose =1),\n",
    "             #ModelCheckpoint(filepath='best_model.h5', monitor='val_mape', save_best_only=True)]\n",
    "callbacks = [ModelCheckpoint(filepath=path+'best_model_pcc_1.h5', monitor='val_root_mean_squared_error', save_best_only=True)]\n",
    "model_pcc_1 = create_model(lyrs=8, neuron_size=24, act='relu', opt='Nadam', dr=0.075, learning_rate=0.0002)\n",
    "\n",
    "# train model on full train set, with 80/20 CV split\n",
    "history_pcc_1=model_pcc_1.fit(df_scc[top_30_features], y_train,epochs=10,batch_size=4,validation_split=0.1, callbacks= [callbacks], verbose=2)\n",
    "#val_loss = np.mean(training.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf4a36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pcc_1.save(path+\"model_pcc_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81537ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = model_pcc_1.evaluate(X_test_fs, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea8f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "best_model_pcc_1 = load_model(path+'best_model_pcc_1.h5')\n",
    "best_model_pcc_1.evaluate(X_test_fs, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2039b489",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path = 'plots\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4766fed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run functions.ipynb\n",
    "matrics_plot(history_pcc_1,history_pcc_1.history['mae'],history_pcc_1.history['val_mae'],\"MAE\",\"PCC-1 Model\", \"Hardness\",plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a9cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run functions.ipynb\n",
    "r2_plot(model_pcc_1,X_test_fs,y_test,\"name\",\"model_name\",plot_path=\"plots\\\\_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afecbc4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f8a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29ec4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_gnn as tfgnn\n",
    "\n",
    "# Model hyper-parameters:\n",
    "h_dims = {'user': 256, 'movie': 64, 'genre': 128}\n",
    "\n",
    "# Model builder initialization:\n",
    "gnn = tfgnn.keras.ConvGNNBuilder(\n",
    "  lambda edge_set_name: WeightedSumConvolution(),\n",
    "  lambda node_set_name: tfgnn.keras.layers.NextStateFromConcat(\n",
    "     tf.keras.layers.Dense(h_dims[node_set_name]))\n",
    ")\n",
    "\n",
    "# Two rounds of message passing to target node sets:\n",
    "model = tf.keras.models.Sequential([\n",
    "    gnn.Convolve({'genre'}),  # sends messages from movie to genre\n",
    "    gnn.Convolve({'user'}),  # sends messages from movie and genre to users\n",
    "    tfgnn.keras.layers.Readout(node_set_name=\"user\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3a2e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae663180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "X = np.loadtxt('data.csv', delimiter=',', usecols=range(4))\n",
    "y = np.loadtxt('data.csv', delimiter=',', usecols=[4, 5, 6, 7, 8, 9]).reshape(-1, 2, 3) # reshape to mxn array\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(16, input_dim=4, activation='relu'))\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(4, activation='relu'))\n",
    "model.add(keras.layers.Dense(6, activation='relu')) # mxn nodes\n",
    "model.add(keras.layers.Reshape((2, 3))) # reshape to mxn array\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# Predict on new data\n",
    "X_new = np.array([[5.1, 3.5, 1.4, 0.2]])\n",
    "y_pred = model.predict(X_new)\n",
    "\n",
    "# Print predicted output\n",
    "print(\"Predicted output: \")\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24346ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18710766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Generate some sample data\n",
    "data = unfolded #np.random.rand(2520, 18)\n",
    "\n",
    "# Create a 2D grid for the x and y axes\n",
    "x, y = np.meshgrid(np.arange(18), np.arange(2520))\n",
    "\n",
    "# Create a 3D plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot the surface\n",
    "surf = ax.plot_surface(x, y, data, cmap='viridis',alpha=0.9,  linewidth=0)\n",
    "\n",
    "# Add a colorbar\n",
    "fig.colorbar(surf, shrink=0.5, aspect=15)\n",
    "\n",
    "# Set the axis labels and title\n",
    "\n",
    "ax.set_xlabel('j x n', labelpad=12, fontsize=20,color='r')\n",
    "ax.set_ylabel('Rows', labelpad=20, fontsize=20,color='r')\n",
    "ax.set_zlabel('Frequency', labelpad=8, fontsize=20,color='r')\n",
    "\n",
    "#ax.set_xlabel('X Axis')\n",
    "#ax.set_ylabel('Y Axis')\n",
    "#ax.set_zlabel('Z Axis')\n",
    "ax.set_title('Surface Plot')\n",
    "\n",
    "# set background color and grid\n",
    "ax.xaxis.pane.fill = False\n",
    "ax.yaxis.pane.fill = False\n",
    "ax.zaxis.pane.fill = False\n",
    "ax.xaxis.pane.set_edgecolor('white')\n",
    "ax.yaxis.pane.set_edgecolor('white')\n",
    "ax.zaxis.pane.set_edgecolor('white')\n",
    "ax.grid(False)\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a9fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run functions.ipynb\n",
    "\n",
    "n18_split(unfolded,title=\"123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e42dd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "data = unfolded #np.random.rand(2520, 18)\n",
    "\n",
    "plt.rc('font', size=18)\n",
    "\n",
    "# create plot\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# create colormap\n",
    "norm = plt.Normalize(np.array(data).min(), np.array(data).max())\n",
    "cmap = plt.cm.plasma\n",
    "\n",
    "# create ScalarMappable object based on frequency values\n",
    "sm = ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array(data)\n",
    "\n",
    "# plot surface with colors based on frequency values\n",
    "x, y = np.meshgrid(range(2520), range(18))\n",
    "z = data.T\n",
    "\n",
    "ax.plot_surface(x, y, z, cmap=cmap, alpha=0.9, linewidth=0, rstride=1, cstride=1)\n",
    "\n",
    "# add colorbar\n",
    "fig.colorbar(sm, ax=ax, shrink=0.4, pad=0.04)\n",
    "\n",
    "# set labels and title\n",
    "ax.set_xlabel('j x n', labelpad=12, fontsize=20, color='r')\n",
    "ax.set_ylabel('Rows', labelpad=20, fontsize=20, color='r')\n",
    "ax.set_zlabel('Frequency', labelpad=8, fontsize=20, color='r')\n",
    "ax.set_title('title')\n",
    "\n",
    "ax.yaxis.set_major_locator(MaxNLocator(6))\n",
    "ax.xaxis.set_major_locator(MaxNLocator(3))\n",
    "ax.zaxis.set_major_locator(MaxNLocator(3))\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), fontsize=18)\n",
    "ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "# set camera angle and distance\n",
    "ax.view_init(elev=15, azim=-30)\n",
    "\n",
    "# set background color and grid\n",
    "ax.xaxis.pane.fill = False\n",
    "ax.yaxis.pane.fill = False\n",
    "ax.zaxis.pane.fill = False\n",
    "ax.xaxis.pane.set_edgecolor('white')\n",
    "ax.yaxis.pane.set_edgecolor('white')\n",
    "ax.zaxis.pane.set_edgecolor('white')\n",
    "ax.grid(False)\n",
    "\n",
    "# set plot size and scale\n",
    "fig.set_size_inches(10, 8)\n",
    "x_scale = 1.2\n",
    "y_scale = 0.8\n",
    "z_scale = 0.8\n",
    "scale = np.diag([x_scale, y_scale, z_scale, 1.0])\n",
    "scale = scale * (1.0/scale.max())\n",
    "scale[3,3] = 1.0\n",
    "\n",
    "def short_proj():\n",
    "    return np.dot(Axes3D.get_proj(ax), scale)\n",
    "\n",
    "ax.get_proj = short_proj\n",
    "\n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b18dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this plot for data splitted\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import matplotlib.ticker as tck\n",
    "\n",
    "data = unfolded #np.random.rand(2520, 18)\n",
    "\n",
    "plt.rc('font', size=18)\n",
    "\n",
    "# create plot\n",
    "fig = plt.figure(figsize=(12, 14))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# create colormap\n",
    "norm = plt.Normalize(np.array(data).min(), np.array(data).max())\n",
    "cmap = plt.cm.Set1#nipy_spectral#prism#gist_rainbow#cubehelix #gist_stern #CMRmap # gnuplot#hsv #twilight#magma #cividis#inferno #ocean#prism #jet #nipy_spectral #CMRmap#gnuplot2 #plasma\n",
    "\n",
    "# create ScalarMappable object based on frequency values\n",
    "mappable = ScalarMappable(cmap=cmap, norm=norm)\n",
    "mappable.set_array(data)\n",
    "\n",
    "# plot surface with colors based on frequency values\n",
    "x, y = np.meshgrid(range(2520), range(18))\n",
    "z = data.T\n",
    "\n",
    "ax.plot_surface(x, y, z, cmap=cmap, alpha=0.9, linewidth=0.01, rstride=1, cstride=1)\n",
    "\n",
    "# add colorbar\n",
    "clb = fig.colorbar(mappable, ax=ax,  shrink=0.25, pad = -0.25)\n",
    "clb.ax.tick_params(labelsize=16) \n",
    "clb.set_label(r'$\\alpha$', rotation=0)\n",
    "# Label and coordinate\n",
    "#ax.text(5, 10,10 , r\"$\\alpha$\", color='red', fontsize=12)\n",
    "ax.text(4, max(ax.get_ybound()),1.1*max(ax.get_zbound()), r\"$\\alpha ^{  n}_{ij}$\", color='red', fontsize=18)\n",
    "\n",
    "\n",
    "# set labels and title\n",
    "ax.set_xlabel('Columns', labelpad=12, fontsize=20,color='r')\n",
    "ax.set_ylabel('Rows x N', labelpad=20, fontsize=20,color='r')\n",
    "#ax.set_zlabel('Frequency', labelpad=8, fontsize=20,color='r')\n",
    "ax.set_title('title')\n",
    "ax.margins(x=0, y=-0.25)\n",
    "\n",
    "#ax.set_xticks(np.linspace(0, 6, 4))\n",
    "#ax.set_xticklabels(['1', '2', '6','2'])\n",
    "\n",
    "ax.yaxis.set_major_locator(MaxNLocator(4))\n",
    "#ax.set_xticks(np.linspace(0, 18, 4))\n",
    "ax.set_yticklabels(['1','6','12','18'])\n",
    "\n",
    "ax.xaxis.set_major_locator(MaxNLocator(4))\n",
    "ax.xaxis.set_minor_locator(tck.AutoMinorLocator(2))\n",
    "\n",
    "\n",
    "ax.zaxis.set_major_locator(MaxNLocator(1))\n",
    "ax.get_zaxis().set_visible(False)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), fontsize=18)\n",
    "#ax.set_zticklabels(ax.get_zticklabels(), fontsize=18)\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "# set camera angle and distance\n",
    "#ax.view_init(elev=20, azim=-40)\n",
    "ax.view_init(elev=15, azim=-60)\n",
    "#ax.dist=12\n",
    "\n",
    "# set background color and grid\n",
    "ax.xaxis.pane.fill = False\n",
    "ax.yaxis.pane.fill = False\n",
    "ax.zaxis.pane.fill = False\n",
    "ax.xaxis.pane.set_edgecolor('white')\n",
    "ax.yaxis.pane.set_edgecolor('white')\n",
    "ax.zaxis.pane.set_edgecolor('white')\n",
    "ax.grid(False)\n",
    "\n",
    "ax.set_zticks([]) # Remove the tick labels on the z-axis\n",
    "ax.set_zticklabels([])\n",
    "\n",
    "ax.zaxis.line.set_color((1.0, 1.0, 1.0, 0.0)) # Set the z-axis line color to transparent\n",
    "\n",
    "\n",
    "\n",
    "ax.get_proj = short_proj\n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3363bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "surf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5834790",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ca1a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# generate some random input and output data\n",
    "X = X_train #np.random.random((1000, 5))\n",
    "y = y_train #np.random.random((1000, 6))  # 2x3 output array for each input\n",
    "\n",
    "X = np.random.random((1000, 5))\n",
    "y = np.random.random((1000, 6))  # 2x3 output array for each input\n",
    "\n",
    "#y= y.reshape(y_pred.shape[0], 2, 3)\n",
    "#print(\"x:\",X)\n",
    "#print(\"y:\",y)\n",
    "\n",
    "print(\"###############################\")\n",
    "print(np.shape(X), np.shape(y))\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(6, activation='linear'))  # output layer with 6 nodes, corresponding to 2x3 array\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# train the model\n",
    "model.fit(X, y, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# evaluate the model\n",
    "loss = model.evaluate(X, y, verbose=0)\n",
    "print(f'Test loss: {loss}')\n",
    "\n",
    "# make predictions on new data\n",
    "#X_new = X_test #np.random.random((5, 5))\n",
    "X_new = np.random.random((5, 5))\n",
    "\n",
    "y_pred = model.predict(X_new)\n",
    "\n",
    "# reshape the predicted output to match the shape of the true output\n",
    "y_pred = y_pred.reshape(y_pred.shape[0], 2, 3)\n",
    "\n",
    "# print the predicted output\n",
    "print(f'Predicted output shape: {y_pred.shape}')\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278a56bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# generate some random input and output data\n",
    "X = X_train #np.random.random((1000, 5))\n",
    "y = y_train #np.random.random((1000, 6))  # 2x3 output array for each input\n",
    "\n",
    "X = np.random.random((2520, 149))\n",
    "y = np.random.random((2520, 18))  # 2x3 output array for each input\n",
    "\n",
    "#y= y.reshape(y_pred.shape[0], 2, 3)\n",
    "#print(\"x:\",X)\n",
    "#print(\"y:\",y)\n",
    "\n",
    "print(\"###############################\")\n",
    "print(np.shape(X), np.shape(y))\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=149))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(18, activation='linear'))  # output layer with 6 nodes, corresponding to 2x3 array\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# train the model\n",
    "model.fit(X, y, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# evaluate the model\n",
    "loss = model.evaluate(X, y, verbose=0)\n",
    "print(f'Test loss: {loss}')\n",
    "\n",
    "# make predictions on new data\n",
    "#X_new = X_test #np.random.random((5, 5))\n",
    "X_new = np.random.random((5, 149))\n",
    "\n",
    "y_pred = model.predict(X_new)\n",
    "\n",
    "# reshape the predicted output to match the shape of the true output\n",
    "y_pred = y_pred.reshape(y_pred.shape[0], 3, 6)\n",
    "\n",
    "# print the predicted output\n",
    "print(f'Predicted output shape: {y_pred.shape}')\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808d7226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# generate some random input and output data\n",
    "X = X_fs #np.random.random((1000, 5))\n",
    "y = y_train #y_train #np.random.random((1000, 6))  # 2x3 output array for each input\n",
    "\n",
    "#X = np.random.random((1000, 5))\n",
    "#y = np.random.random((1000, 6))  # 2x3 output array for each input\n",
    "\n",
    "#y= y.reshape(y_pred.shape[0], 2, 3)\n",
    "#print(\"x:\",X)\n",
    "#print(\"y:\",y)\n",
    "print(np.shape(X)[1])\n",
    "print(\"###############################\")\n",
    "print(np.shape(X), np.shape(y))\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=np.shape(X)[1]))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "#model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(18, activation='sigmoid'))  # output layer with 18 nodes, corresponding to 3x6 array\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse','mae'])\n",
    "\n",
    "# train the model\n",
    "model.fit(X, y, epochs=20, batch_size=32, verbose=1)#, metrics=['mse','mae','accuracy'])\n",
    "\n",
    "# evaluate the model\n",
    "loss = model.evaluate(X, y, verbose=0)\n",
    "print(f'Test loss: {loss}')\n",
    "\n",
    "# make predictions on new data\n",
    "X_new = np.random.random((5, 30))\n",
    "#X_new = np.random.random((5, 5))\n",
    "\n",
    "y_pred = model.predict(X_new)\n",
    "\n",
    "# reshape the predicted output to match the shape of the true output\n",
    "y_pred = y_pred.reshape(y_pred.shape[0], 3, 6)\n",
    "\n",
    "# print the predicted output\n",
    "print(f'Predicted output shape: {y_pred.shape}')\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14b9152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# generate some random input and output data\n",
    "X = X_fs#np.random.random((1000, 5))\n",
    "y = y_train #y_train #np.random.random((1000, 6))  # 2x3 output array for each input\n",
    "\n",
    "#X = np.random.random((1000, 5))\n",
    "#y = np.random.random((1000, 6))  # 2x3 output array for each input\n",
    "\n",
    "#y= y.reshape(y_pred.shape[0], 2, 3)\n",
    "#print(\"x:\",X)\n",
    "#print(\"y:\",y)\n",
    "print(np.shape(X)[1])\n",
    "print(\"###############################\")\n",
    "print(np.shape(X), np.shape(y))\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=np.shape(X)[1]))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "#model.add(Dense(1, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "#model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(18, activation='tanh'))  # output layer with 18 nodes, corresponding to 3x6 array\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='RMSprop', loss='mse', metrics=['mse','mae'])\n",
    "\n",
    "# train the model\n",
    "model.fit(X, y, epochs=20, batch_size=32, verbose=1)#, metrics=['mse','mae','accuracy'])\n",
    "\n",
    "# evaluate the model\n",
    "loss = model.evaluate(X, y, verbose=0)\n",
    "print(f'Test loss: {loss}')\n",
    "\n",
    "# make predictions on new data\n",
    "X_new = np.random.random((5, 30))\n",
    "#X_new = np.random.random((5, 5))\n",
    "\n",
    "y_pred = model.predict(X_new)\n",
    "\n",
    "# reshape the predicted output to match the shape of the true output\n",
    "y_pred = y_pred.reshape(y_pred.shape[0], 3, 6)\n",
    "\n",
    "# print the predicted output\n",
    "print(f'Predicted output shape: {y_pred.shape}')\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945d856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run functions.ipynb\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "# Run model with 90/10 datasets with optimized parameters\n",
    "input_dim = np.shape(X)[1] #df_scc[top_30_features].shape[1]\n",
    "print(input_dim)\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "#from keras_adabound import AdaBound\n",
    "from keras.activations import elu\n",
    "from tensorflow.keras.activations import selu\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "from keras.constraints import maxnorm   # To setup weight constraints\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "max_epochs = 100 #10\n",
    "np.random.seed(42)\n",
    "#callbacks = [EarlyStopping(monitor='val_mape', mode='min', restore_best_weights=True, patience=10, verbose =1),\n",
    "             #ModelCheckpoint(filepath='best_model.h5', monitor='val_mape', save_best_only=True)]\n",
    "#callbacks = [ModelCheckpoint(filepath=path+'best_model_pcc_1.h5', monitor='val_root_mean_squared_error', save_best_only=True)]\n",
    "#model_pcc_1 = create_model(lyrs=8, neuron_size=24, act='relu', opt='Nadam', dr=0.075, learning_rate=0.0002)\n",
    "model_pcc_1 = create_model(lyrs=20, neuron_size=24, act='relu', opt='adam', dr=0.20000075, learning_rate=5.0E-03)\n",
    "\n",
    "# train model on full train set, with 80/20 CV split\n",
    "history_pcc_1=model_pcc_1.fit(X_fs, y_train,epochs=max_epochs,batch_size=50,validation_split=0.1, verbose=2)\n",
    "#val_loss = np.mean(training.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9455219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dca193",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4c6972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5cc368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fade7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fee8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_reshaped = y_train.reshape((2520, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0630e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the Pandas Series object to a NumPy array\n",
    "arr = y_train.values\n",
    "\n",
    "# reshape the NumPy array to shape (2520, 6)\n",
    "arr_reshaped = arr.reshape((2520, 6))\n",
    "\n",
    "# convert the NumPy array back to a Pandas DataFrame (optional)\n",
    "df = pd.DataFrame(arr_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7c3cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6433e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
